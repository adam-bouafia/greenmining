{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GreenMining Documentation","text":"<p> An empirical Python library for Mining Software Repositories (MSR) in Green IT research </p> <p> </p>"},{"location":"#what-is-greenmining","title":"What is GreenMining?","text":"<p>GreenMining is an empirical research tool for Mining Software Repositories (MSR) focused on Green IT and sustainable software practices. It provides researchers and practitioners with a comprehensive toolkit for:</p> <ul> <li>Mining repositories at scale to study green software evolution</li> <li>Classifying commits using the Green Software Foundation (GSF) pattern catalog</li> <li>Measuring energy consumption of software systems and analysis workloads</li> <li>Analyzing temporal trends and adoption patterns across projects</li> <li>Generating research-ready datasets with statistical analysis</li> </ul> <p>The library integrates with PyDriller for deep repository analysis and supports multiple energy measurement backends (RAPL, CodeCarbon) for empirical Green IT research.</p>"},{"location":"#key-capabilities","title":"Key Capabilities","text":"Feature Description 122 GSF Patterns Detect patterns across 15 categories (cloud, web, AI, caching, etc.) 321 Green Keywords Comprehensive keyword matching for green-aware commits GitHub Mining Fetch repositories by keywords, stars, language filters URL Analysis Analyze any GitHub repo directly via URL using PyDriller Statistical Analysis Pattern correlations, temporal trends, effect sizes Energy Measurement RAPL and CodeCarbon backends for power profiling Multiple Outputs JSON, CSV, and Markdown reports"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install greenmining\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Python API:</p> <pre><code>from greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\n\n# Check if a commit message is green-aware\nmessage = \"Optimize Redis caching to reduce energy consumption\"\nprint(is_green_aware(message))  # True\n\n# Get matched patterns\npatterns = get_pattern_by_keywords(message)\nprint(patterns)  # ['Cache Static Data']\n</code></pre> <p>Analyze a Repository:</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\"https://github.com/pallets/flask\")\n\nprint(f\"Green-aware: {result['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"#pattern-categories","title":"Pattern Categories","text":"<p>GreenMining detects 122 patterns across 15 categories:</p> Category Patterns Examples Cloud 40+ Caching, compression, auto-scaling, serverless Web 15+ Lazy loading, image optimization, minification AI/ML 10+ Model optimization, quantization, efficient training Caching 8 Redis, CDN, static data caching Async 6 Batch processing, queue-based architecture Database 8 Query optimization, connection pooling Network 6 Compression, CDN, efficient protocols Resource 5 Memory management, CPU optimization Code 4 Dead code removal, algorithm efficiency Infrastructure 4 Container optimization, IaC Microservices 4 Service decomposition, graceful shutdown Monitoring 3 Efficient logging, metrics collection General 8 Feature flags, incremental processing"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Install GreenMining via pip, source, or Docker</li> <li>Quick Start - Run your first analysis in 5 minutes</li> <li>Configuration - Configure via environment variables or config files</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Python API - Use GreenMining programmatically</li> <li>URL Analysis - Analyze repositories directly by URL</li> <li>Energy Measurement - Measure energy consumption with RAPL/CodeCarbon</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>GSF Patterns - All 122 patterns with categories and keywords</li> <li>Configuration Options - All configuration parameters</li> <li>Data Models - Repository, Commit, and AnalysisResult models</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Basic Usage - Simple pattern detection examples</li> <li>Complete Pipeline - Full research workflow example</li> </ul>"},{"location":"#project","title":"Project","text":"<ul> <li>Roadmap - Future features and development plans</li> <li>Contributing - How to contribute to GreenMining</li> <li>Changelog - Version history and release notes</li> </ul>"},{"location":"#example-output","title":"Example Output","text":"<p>When analyzing a repository, GreenMining produces reports like:</p> <pre><code>============================================================\nGREENMINING ANALYSIS RESULTS\n============================================================\n\nRepository: kubernetes/kubernetes\nCommits analyzed: 1000\nGreen-aware commits: 247 (24.7%)\n\nTop Patterns Detected:\n  1. Cache Static Data (89 commits)\n  2. Use Async Instead of Sync (67 commits)\n  3. Containerize Workload (45 commits)\n  4. Compress Transmitted Data (31 commits)\n\nCategories Distribution:\n  cloud: 45.2%\n  caching: 23.1%\n  async: 18.5%\n  infrastructure: 13.2%\n</code></pre>"},{"location":"#research-applications","title":"Research Applications","text":"<p>GreenMining is designed for empirical MSR research in Green IT:</p>"},{"location":"#mining-software-repositories-msr","title":"Mining Software Repositories (MSR)","text":"<ul> <li>Large-scale repository mining with GitHub API and GraphQL</li> <li>Configurable filters (stars, languages, dates, keywords)</li> <li>Batch processing with rate limit handling</li> </ul>"},{"location":"#green-it-pattern-analysis","title":"Green IT Pattern Analysis","text":"<ul> <li>122 GSF patterns across 15 sustainability categories</li> <li>Keyword-based commit classification with confidence scoring</li> <li>Pattern co-occurrence and correlation analysis</li> </ul>"},{"location":"#temporal-statistical-analysis","title":"Temporal &amp; Statistical Analysis","text":"<ul> <li>Trend analysis at configurable granularity (day/week/month/quarter/year)</li> <li>Effect size calculations (Cohen's d, Cliff's delta)</li> <li>Cross-repository comparative studies</li> </ul>"},{"location":"#energy-measurement","title":"Energy Measurement","text":"<ul> <li>RAPL backend for direct CPU/DRAM power measurement (Linux)</li> <li>CodeCarbon integration for cross-platform emissions tracking</li> <li>Energy profiling of analysis workloads</li> </ul>"},{"location":"#research-outputs","title":"Research Outputs","text":"<ul> <li>JSON, CSV, and Markdown report generation</li> <li>Publication-ready statistical summaries</li> <li>Reproducible analysis pipelines</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use GreenMining in your research, please cite:</p> <pre><code>@software{greenmining2024,\n  author = {Bouafia, Adam},\n  title = {GreenMining: Mining Green Software Patterns},\n  year = {2024},\n  url = {https://github.com/adam-bouafia/greenmining}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>GreenMining is released under the MIT License.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to GreenMining are documented here.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#104-2026-01-29","title":"[1.0.4] - 2026-01-29","text":""},{"location":"changelog/#major-performance-improvements","title":"\ud83d\ude80 Major Performance Improvements","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>GraphQL API Migration (5-10x faster repository fetching)</li> <li>Replaced REST API with GitHub GraphQL API v4</li> <li>Fetch 100 repos in 15 seconds (vs 2 minutes with REST)</li> <li>Single request for repos + commits (vs 100+ REST calls)</li> <li>Better rate limit efficiency: 5000 points/hour vs 5000 requests/hour</li> <li>New <code>GitHubGraphQLFetcher</code> class replaces <code>GitHubFetcher</code></li> <li> <p>Old REST implementation deadcoded for reference</p> </li> <li> <p>Enhanced Code Pattern Detection (13+ pattern types)</p> </li> <li>Expanded from 5 to 15 code-level detection patterns</li> <li>NEW patterns: Serverless computing, CDN/edge, Compression, ML optimization,     HTTP/2, gRPC, Container optimization, Green cloud regions, Auto-scaling,     Code splitting, Green ML training</li> <li>More accurate detection in code diffs</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>CLI Interface - Library is now Python API-only</li> <li>Removed <code>cli.py</code> and <code>main.py</code></li> <li>Removed CLI entry points</li> <li>Use Python API instead (simpler for researchers and developers)</li> </ul>"},{"location":"changelog/#performance-metrics","title":"Performance Metrics","text":"Metric Before (REST) After (GraphQL) Improvement Fetch 100 repos 2 minutes 15 seconds 8x faster API requests 100+ 1-2 50x fewer Rate limit cost ~100 requests ~2 points 50x better Pattern detection 5 types 15 types 3x more"},{"location":"changelog/#migration-from-103","title":"Migration from 1.0.3","text":"<p>No code changes needed! The GraphQL migration is transparent:</p> <pre><code># This still works exactly the same - just 10x faster now!\nfrom greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=\"token\",\n    max_repos=100\n)\n# Now uses GraphQL internally (10x faster)\n</code></pre> <p>If you were using CLI: - CLI has been removed - Use the Python API instead - See updated documentation for API examples</p> <p>New features available: <pre><code># Use GraphQL directly for more control\nfrom greenmining.services.github_graphql_fetcher import GitHubGraphQLFetcher\n\nfetcher = GitHubGraphQLFetcher(token=\"token\")\nrepos = fetcher.search_repositories(keywords=\"kubernetes\", max_repos=100)\n</code></pre></p>"},{"location":"changelog/#020-2024-xx-xx","title":"[0.2.0] - 2024-XX-XX","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>URL-based Analysis: Analyze repositories directly by URL using PyDriller</li> <li><code>greenmining analyze &lt;url&gt;</code> command (unified single/batch support)</li> <li><code>LocalRepoAnalyzer</code> service for Python API</li> <li>Energy Measurement: Track energy consumption during analysis</li> <li>RAPL backend for Linux (Intel CPUs)</li> <li>CodeCarbon backend for cross-platform carbon tracking</li> <li><code>--energy</code> and <code>--energy-backend</code> CLI options</li> <li>PyDriller Integration: Rich commit metrics</li> <li>Delta Maintainability Model (DMM) metrics</li> <li>Process metrics (code churn, change set, contributor count)</li> <li>Structural metrics (complexity, lines of code)</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Renamed \"Enhanced\" terminology to standard naming throughout codebase</li> <li>Simplified analyzer architecture</li> <li>Consolidated CLI: unified <code>analyze</code> command handles both local data and URL analysis</li> <li>Previous: <code>analyze</code>, <code>analyze-url</code>, <code>analyze-urls</code> (3 separate commands)</li> <li>Now: <code>analyze [SOURCES...]</code> - auto-detects URLs vs local mode</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>NLP-based semantic analysis (simplified to keyword matching)</li> <li>ML-based classification (not needed for pattern detection)</li> <li>External ML model dependencies</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Pattern detection accuracy improvements</li> <li>Configuration loading from .env files</li> <li>Report generation formatting</li> </ul>"},{"location":"changelog/#010-2024-xx-xx","title":"[0.1.0] - 2024-XX-XX","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial release</li> <li>GSF pattern detection (122 patterns, 15 categories)</li> <li>GitHub repository fetching</li> <li>Commit extraction and analysis</li> <li>Statistical analysis with effect sizes</li> <li>Temporal trend analysis</li> <li>Docker support</li> <li>Configuration via environment variables</li> </ul>"},{"location":"changelog/#dependencies","title":"Dependencies","text":"<ul> <li>Python 3.9+</li> <li>PyDriller for repository mining</li> <li>python-dotenv for configuration</li> </ul>"},{"location":"changelog/#migration-guide","title":"Migration Guide","text":""},{"location":"changelog/#from-01x-to-10x","title":"From 0.1.x to 1.0.x","text":""},{"location":"changelog/#api-changes","title":"API Changes","text":"<ol> <li> <p>Import paths unchanged:    <pre><code># Still works\nfrom greenmining import is_green_aware, GSF_PATTERNS\n</code></pre></p> </li> <li> <p>New services:    <pre><code># New in 0.2.0+\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom greenmining.energy.base import EnergyMeasurer\n</code></pre></p> </li> <li> <p>Removed NLP/ML features:    <pre><code># These no longer exist - remove from your code\n# from greenmining.analyzers import SemanticAnalyzer  # Removed\n# from greenmining.analyzers import MLClassifier  # Removed\n</code></pre></p> </li> </ol>"},{"location":"changelog/#from-103-to-104-cli-removal","title":"From 1.0.3 to 1.0.4 (CLI Removal)","text":"<p>CLI has been removed in 1.0.4. Use the Python API:</p> <pre><code># Old CLI: greenmining analyze https://github.com/org/repo\n# New Python API:\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\"https://github.com/org/repo\")\n</code></pre>"},{"location":"changelog/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>Documentation</li> <li>PyPI Package</li> </ul>"},{"location":"contributing/","title":"Contributing to GreenMining","text":"<p>Thank you for your interest in contributing to GreenMining! </p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code># Fork on GitHub, then:\ngit clone https://github.com/YOUR-USERNAME/greenmining.git\ncd greenmining\n</code></pre>"},{"location":"contributing/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<pre><code># Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\n# or: venv\\Scripts\\activate  # Windows\n\n# Install in development mode\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\npytest\n\n# With coverage\npytest --cov=greenmining --cov-report=html\n\n# Specific test file\npytest tests/test_gsf_patterns.py -v\n</code></pre>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We follow PEP 8 with these tools:</p> <pre><code># Format code\nblack greenmining/ tests/\n\n# Check types\nmypy greenmining/\n\n# Lint\nruff check greenmining/\n</code></pre>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Configuration (<code>.pre-commit-config.yaml</code>):</p> <pre><code>repos:\n  - repo: https://github.com/psf/black\n    rev: 23.12.1\n    hooks:\n      - id: black\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.1.9\n    hooks:\n      - id: ruff\n</code></pre>"},{"location":"contributing/#contribution-types","title":"Contribution Types","text":""},{"location":"contributing/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<ol> <li>Search existing issues first</li> <li>Create a new issue with:</li> <li>GreenMining version (<code>pip show greenmining</code>)</li> <li>Python version</li> <li>Operating system</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Error messages/tracebacks</li> </ol>"},{"location":"contributing/#feature-requests","title":"\u2728 Feature Requests","text":"<ol> <li>Check existing issues and roadmap</li> <li>Create an issue describing:</li> <li>Use case</li> <li>Proposed solution</li> <li>Alternatives considered</li> </ol>"},{"location":"contributing/#documentation","title":"\ud83d\udcdd Documentation","text":"<ul> <li>Fix typos and clarify explanations</li> <li>Add examples</li> <li>Improve docstrings</li> <li>Update README</li> </ul>"},{"location":"contributing/#code-contributions","title":"\ud83d\udd27 Code Contributions","text":"<ol> <li>Open an issue to discuss major changes</li> <li>Fork and create a feature branch</li> <li>Write tests for new functionality</li> <li>Ensure all tests pass</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#adding-new-gsf-patterns","title":"Adding New GSF Patterns","text":""},{"location":"contributing/#1-update-pattern-definition","title":"1. Update Pattern Definition","text":"<p>Edit <code>greenmining/gsf_patterns.py</code>:</p> <pre><code>GSF_PATTERNS = {\n    # ... existing patterns ...\n\n    \"new_pattern_key\": {\n        \"name\": \"New Pattern Name\",\n        \"category\": \"category_name\",  # cloud, web, ai, caching, etc.\n        \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n        \"description\": \"Brief description of the pattern\",\n        \"sci_impact\": \"How this pattern reduces software carbon intensity\"\n    },\n}\n</code></pre>"},{"location":"contributing/#2-add-keywords-to-green_keywords","title":"2. Add Keywords to GREEN_KEYWORDS","text":"<pre><code>GREEN_KEYWORDS = [\n    # ... existing keywords ...\n    \"keyword1\",\n    \"keyword2\", \n    \"keyword3\",\n]\n</code></pre>"},{"location":"contributing/#3-write-tests","title":"3. Write Tests","text":"<pre><code># tests/test_gsf_patterns.py\n\ndef test_new_pattern_detection():\n    \"\"\"Test that new pattern is detected correctly.\"\"\"\n    from greenmining import is_green_aware, get_pattern_by_keywords\n\n    # Should detect\n    assert is_green_aware(\"message with keyword1\")\n    patterns = get_pattern_by_keywords(\"message with keyword1\")\n    assert \"New Pattern Name\" in patterns\n\n    # Should not detect\n    assert not is_green_aware(\"unrelated message\")\n</code></pre>"},{"location":"contributing/#4-update-documentation","title":"4. Update Documentation","text":"<p>Add pattern to <code>docs/reference/patterns.md</code></p>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>greenmining/\n\u251c\u2500\u2500 __init__.py          # Package exports\n\u251c\u2500\u2500 __main__.py          # Entry point\n\u251c\u2500\u2500 config.py            # Configuration\n\u251c\u2500\u2500 gsf_patterns.py      # Pattern definitions\n\u251c\u2500\u2500 utils.py             # Utilities\n\u251c\u2500\u2500 analyzers/           # Analysis modules\n\u2502   \u251c\u2500\u2500 qualitative_analyzer.py\n\u2502   \u251c\u2500\u2500 statistical_analyzer.py\n\u2502   \u2514\u2500\u2500 temporal_analyzer.py\n\u251c\u2500\u2500 controllers/         # Controllers\n\u2502   \u2514\u2500\u2500 repository_controller.py\n\u251c\u2500\u2500 energy/              # Energy measurement\n\u2502   \u251c\u2500\u2500 base.py\n\u2502   \u251c\u2500\u2500 rapl_backend.py\n\u2502   \u2514\u2500\u2500 codecarbon_backend.py\n\u251c\u2500\u2500 models/              # Data models\n\u251c\u2500\u2500 presenters/          # Output formatting\n\u2502   \u2514\u2500\u2500 console_presenter.py\n\u2514\u2500\u2500 services/            # Core services\n    \u251c\u2500\u2500 commit_extractor.py\n    \u251c\u2500\u2500 data_aggregator.py\n    \u251c\u2500\u2500 data_analyzer.py\n    \u251c\u2500\u2500 github_fetcher.py\n    \u251c\u2500\u2500 local_repo_analyzer.py\n    \u2514\u2500\u2500 reports.py\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#1-create-branch","title":"1. Create Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/issue-description\n</code></pre>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write clean, documented code</li> <li>Add tests for new functionality</li> <li>Update documentation if needed</li> </ul>"},{"location":"contributing/#3-commit-with-clear-messages","title":"3. Commit with Clear Messages","text":"<pre><code>git add .\ngit commit -m \"feat: add new pattern detection for X\"\n# or\ngit commit -m \"fix: resolve issue with Y\"\n# or\ngit commit -m \"docs: update installation instructions\"\n</code></pre> <p>Follow Conventional Commits: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation - <code>test:</code> - Tests - <code>refactor:</code> - Code refactoring - <code>chore:</code> - Maintenance</p>"},{"location":"contributing/#4-push-and-create-pr","title":"4. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub with: - Clear title - Description of changes - Link to related issue - Screenshots if UI changes</p>"},{"location":"contributing/#5-address-review-feedback","title":"5. Address Review Feedback","text":"<p>Respond to reviewer comments and make requested changes.</p>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#test-structure","title":"Test Structure","text":"<pre><code># tests/test_module.py\n\nimport pytest\nfrom greenmining import function_to_test\n\nclass TestFunctionName:\n    \"\"\"Tests for function_to_test.\"\"\"\n\n    def test_basic_functionality(self):\n        \"\"\"Test basic happy path.\"\"\"\n        result = function_to_test(\"input\")\n        assert result == \"expected\"\n\n    def test_edge_case(self):\n        \"\"\"Test edge case handling.\"\"\"\n        result = function_to_test(\"\")\n        assert result is None\n\n    def test_error_handling(self):\n        \"\"\"Test error conditions.\"\"\"\n        with pytest.raises(ValueError):\n            function_to_test(None)\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest\n\n# Verbose output\npytest -v\n\n# Specific test\npytest tests/test_gsf_patterns.py::test_pattern_detection -v\n\n# With coverage\npytest --cov=greenmining --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help newcomers</li> <li>Acknowledge contributions</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b Issue Tracker</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"roadmap/","title":"Roadmap &amp; Future Work","text":"<p>This document outlines planned features and enhancements for GreenMining.</p>"},{"location":"roadmap/#current-status-v104","title":"Current Status (v1.0.4)","text":""},{"location":"roadmap/#implemented-features","title":"\u2705 Implemented Features","text":"Feature Status Description GSF Pattern Catalog \u2705 Complete 122 patterns across 15 categories GitHub Repository Mining \u2705 Complete GraphQL API with filters URL-Based Analysis \u2705 Complete Direct PyDriller analysis Commit Classification \u2705 Complete Keyword matching with confidence Energy Measurement \u2705 Complete RAPL + CodeCarbon backends Statistical Analysis \u2705 Complete Correlations, effect sizes, trends Temporal Analysis \u2705 Complete Configurable time granularity Report Generation \u2705 Complete JSON, CSV, Markdown outputs DMM Metrics \u2705 Complete Delta Maintainability Model Process Metrics \u26a0\ufe0f Partial Code churn, change set, contributors"},{"location":"roadmap/#phase-1-enhanced-repository-support-priority-high","title":"Phase 1: Enhanced Repository Support (Priority: High)","text":""},{"location":"roadmap/#11-configuration-based-url-input","title":"1.1 Configuration-Based URL Input","text":"<p>Add support for specifying repository URLs directly in configuration files.</p> <pre><code># greenmining.yaml\nsources:\n  urls:\n    - https://github.com/kubernetes/kubernetes\n    - https://github.com/istio/istio\n    - git@github.com:company/private-repo.git\n  search:\n    enabled: true\n    keywords: \"microservices cloud-native\"\n    min_stars: 500\n</code></pre> <p>Status: \ud83d\udd34 Not Started Effort: Low Impact: High</p>"},{"location":"roadmap/#12-batch-url-analysis-api","title":"1.2 Batch URL Analysis API","text":"<p>Dedicated function for analyzing multiple repositories.</p> <pre><code>from greenmining import analyze_repositories\n\nresults = analyze_repositories(\n    urls=[\n        \"https://github.com/kubernetes/kubernetes\",\n        \"https://github.com/istio/istio\",\n    ],\n    max_commits=100,\n    parallel_workers=4,\n    output_format=\"json\"\n)\n</code></pre> <p>Status: \ud83d\udd34 Not Started Effort: Medium Impact: High</p>"},{"location":"roadmap/#13-private-repository-support","title":"1.3 Private Repository Support","text":"<p>Authentication for private repositories via SSH keys or tokens.</p> <pre><code>from greenmining.services import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    ssh_key_path=\"~/.ssh/id_rsa\",\n    github_token=\"ghp_xxx\"  # For private GitHub repos\n)\nresult = analyzer.analyze_repository(\"git@github.com:company/private-repo.git\")\n</code></pre> <p>Status: \ud83d\udd34 Not Started Effort: Medium Impact: Medium</p>"},{"location":"roadmap/#phase-2-extended-energy-measurement-priority-high","title":"Phase 2: Extended Energy Measurement (Priority: High)","text":""},{"location":"roadmap/#21-cpu-energy-meter-backend","title":"2.1 CPU Energy Meter Backend","text":"<p>Cross-platform CPU energy measurement support.</p> <pre><code>from greenmining.energy import CPUEnergyMeter\n\nmeter = CPUEnergyMeter(\n    backend=\"auto\"  # auto-detect: RAPL, Windows EEE, Apple Silicon\n)\nwith meter.measure() as metrics:\n    result = analyzer.analyze_repository(url)\n</code></pre> <p>Platforms: - \u2705 Linux (RAPL) - Already implemented - \ud83d\udd34 Windows Energy Estimation Engine - \ud83d\udd34 Apple Silicon power metrics - \ud83d\udd34 AMD Energy Driver</p> <p>Status: \ud83d\udd34 Not Started Effort: High Impact: High</p>"},{"location":"roadmap/#22-integrated-energy-tracking","title":"2.2 Integrated Energy Tracking","text":"<p>Automatic energy tracking during analysis without manual setup.</p> <pre><code>from greenmining.services import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(energy_tracking=True, energy_backend=\"rapl\")\nresult = analyzer.analyze_repository(\"https://github.com/flask\")\n\nprint(result[\"energy_metrics\"][\"energy_joules\"])\nprint(result[\"energy_metrics\"][\"average_power_watts\"])\nprint(result[\"energy_metrics\"][\"duration_seconds\"])\n</code></pre> <p>Status: \ud83d\udd34 Not Started Effort: Medium Impact: High</p>"},{"location":"roadmap/#23-carbon-footprint-reporting","title":"2.3 Carbon Footprint Reporting","text":"<p>Generate carbon emissions reports based on energy measurements.</p> <pre><code>from greenmining.energy import CarbonReporter\n\nreporter = CarbonReporter(\n    country_iso=\"USA\",\n    cloud_provider=\"aws\",\n    region=\"us-east-1\"\n)\nreport = reporter.generate_report(analysis_results)\nprint(f\"CO2 emissions: {report.total_emissions_kg:.4f} kg\")\nprint(f\"Equivalent: {report.tree_months:.1f} tree-months\")\n</code></pre> <p>Status: \ud83d\udd34 Not Started Effort: Low Impact: Medium</p>"},{"location":"roadmap/#phase-3-full-pydriller-integration-priority-medium","title":"Phase 3: Full PyDriller Integration (Priority: Medium)","text":""},{"location":"roadmap/#31-complete-process-metrics","title":"3.1 Complete Process Metrics","text":"<p>Integrate all PyDriller process metrics.</p> <pre><code>from greenmining.services import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(process_metrics=\"full\")\nresult = analyzer.analyze_repository(url)\n\n# Available metrics:\n# - change_set: Files modified together\n# - code_churn: Lines added/removed over time\n# - commits_count: Per-file commit frequency\n# - contributors_count: Unique contributors per file\n# - contributors_experience: Developer expertise\n# - hunks_count: Contiguous change regions\n# - lines_count: Total lines in files\n</code></pre> <p>Status: \u26a0\ufe0f Partial Effort: Medium Impact: Medium</p>"},{"location":"roadmap/#32-method-level-analysis","title":"3.2 Method-Level Analysis","text":"<p>Extract per-method metrics using Lizard integration.</p> <pre><code>result = analyzer.analyze_repository(\n    url=\"https://github.com/flask\",\n    method_level_analysis=True\n)\n\nfor commit in result[\"commits\"]:\n    for method in commit[\"methods_modified\"]:\n        print(f\"{method['name']}: complexity={method['complexity']}\")\n        print(f\"  params={method['parameters']}, tokens={method['token_count']}\")\n</code></pre> <p>Status: \ud83d\udd34 Not Started Effort: High Impact: Medium</p>"},{"location":"roadmap/#33-source-code-access","title":"3.3 Source Code Access","text":"<p>Access full source code before/after for refactoring detection.</p> <pre><code>for commit in result[\"commits\"]:\n    for file in commit[\"modified_files\"]:\n        if file[\"source_code_before\"]:\n            # Analyze code transformations\n            detect_refactorings(\n                before=file[\"source_code_before\"],\n                after=file[\"source_code\"]\n            )\n</code></pre> <p>Status: \ud83d\udd34 Not Started Effort: Low Impact: Low</p>"},{"location":"roadmap/#phase-4-green-msr-techniques-priority-medium","title":"Phase 4: Green MSR Techniques (Priority: Medium)","text":""},{"location":"roadmap/#41-power-regression-detection","title":"4.1 Power Regression Detection","text":"<p>Identify commits that increased power consumption.</p> <pre><code>from greenmining.analyzers import PowerRegressionDetector\n\ndetector = PowerRegressionDetector(\n    test_command=\"pytest tests/ -x\",\n    energy_backend=\"rapl\",\n    threshold_percent=5.0,\n    iterations=5\n)\nregressions = detector.detect(\n    repo_path=\"/path/to/repo\",\n    baseline_commit=\"v1.0.0\",\n    target_commit=\"HEAD\"\n)\n\nfor regression in regressions:\n    print(f\"Commit {regression.sha[:8]}: +{regression.power_increase:.1f}%\")\n    print(f\"  Message: {regression.message}\")\n</code></pre> <p>Status: \ud83d\udd34 Not Started Effort: High Impact: High</p>"},{"location":"roadmap/#42-metrics-to-power-correlation","title":"4.2 Metrics-to-Power Correlation","text":"<p>Build models correlating code metrics with power consumption.</p> <pre><code>from greenmining.analyzers import MetricsPowerCorrelator\n\ncorrelator = MetricsPowerCorrelator()\ncorrelator.fit(\n    metrics=[\"complexity\", \"nloc\", \"code_churn\", \"method_count\"],\n    power_measurements=measured_power_data\n)\n\n# Results\nprint(f\"Pearson correlations: {correlator.pearson}\")\nprint(f\"Spearman correlations: {correlator.spearman}\")\nprint(f\"Feature importance: {correlator.feature_importance}\")\n</code></pre> <p>Status: \ud83d\udd34 Not Started Effort: High Impact: High</p>"},{"location":"roadmap/#43-version-by-version-power-analysis","title":"4.3 Version-by-Version Power Analysis","text":"<p>Measure power consumption across multiple software versions.</p> <pre><code>from greenmining.analyzers import VersionPowerAnalyzer\n\nanalyzer = VersionPowerAnalyzer(\n    test_command=\"pytest tests/\",\n    energy_backend=\"rapl\",\n    iterations=10\n)\nreport = analyzer.analyze_versions(\n    repo_path=\"/path/to/repo\",\n    versions=[\"v1.0\", \"v1.1\", \"v1.2\", \"v2.0\"],\n    warmup_iterations=2\n)\n\n# Visualize trends\nreport.plot_power_evolution(\"power_evolution.png\")\n</code></pre> <p>Status: \ud83d\udd34 Not Started Effort: High Impact: High</p>"},{"location":"roadmap/#phase-5-advanced-features-priority-low","title":"Phase 5: Advanced Features (Priority: Low)","text":""},{"location":"roadmap/#51-cicd-integration","title":"5.1 CI/CD Integration","text":"<p>GitHub Action for green energy gates.</p> <pre><code># .github/workflows/energy-check.yml\nname: Energy Check\non: [push, pull_request]\n\njobs:\n  energy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: adam-bouafia/greenmining-action@v1\n        with:\n          test_command: pytest tests/\n          energy_threshold: 5%  # Max increase allowed\n          fail_on_regression: true\n          baseline: main\n</code></pre> <p>Status: \ud83d\udd34 Not Started Effort: Medium Impact: High</p>"},{"location":"roadmap/#52-vs-code-extension","title":"5.2 VS Code Extension","text":"<p>Real-time green pattern suggestions in IDE.</p> <ul> <li>Highlight code that could benefit from green patterns</li> <li>Suggest optimizations based on GSF catalog</li> <li>Show energy impact estimates</li> <li>Quick-fix actions for common patterns</li> </ul> <p>Status: \ud83d\udd34 Not Started Effort: High Impact: Medium</p>"},{"location":"roadmap/#53-web-dashboard","title":"5.3 Web Dashboard","text":"<p>Interactive visualization of analysis results.</p> <ul> <li>Repository comparison charts</li> <li>Pattern distribution sunbursts</li> <li>Temporal trend graphs</li> <li>Energy consumption heatmaps</li> <li>Export to various formats</li> </ul> <p>Status: \ud83d\udd34 Not Started Effort: High Impact: Medium</p>"},{"location":"roadmap/#54-ml-based-pattern-detection","title":"5.4 ML-Based Pattern Detection","text":"<p>Machine learning for pattern detection without relying on commit messages.</p> <pre><code>from greenmining.ml import PatternPredictor\n\npredictor = PatternPredictor.load(\"pretrained-v1\")\npatterns = predictor.predict_from_diff(diff_content)\n# Returns patterns based on code structure, not just keywords\n</code></pre> <p>Status: \ud83d\udd34 Not Started Effort: Very High Impact: High</p>"},{"location":"roadmap/#implementation-priority-matrix","title":"Implementation Priority Matrix","text":"Phase Feature Effort Impact Priority 1 URL Config Support Low High \ud83d\udd34 Critical 1 Batch URL Analysis Medium High \ud83d\udd34 Critical 2 Integrated Energy Tracking Medium High \ud83d\udfe0 High 2 Carbon Reporting Low Medium \ud83d\udfe0 High 4 Power Regression Detection High High \ud83d\udfe1 Medium 3 Full Process Metrics Medium Medium \ud83d\udfe1 Medium 4 Metrics Correlation High High \ud83d\udfe1 Medium 3 Method-Level Analysis High Medium \ud83d\udfe1 Medium 5 CI/CD Integration Medium High \ud83d\udfe2 Low 5 VS Code Extension High Medium \ud83d\udfe2 Low 5 ML Pattern Detection Very High High \ud83d\udfe2 Low"},{"location":"roadmap/#contributing","title":"Contributing","text":"<p>We welcome contributions! If you'd like to help implement any of these features:</p> <ol> <li>Check the Contributing Guide</li> <li>Open an issue to discuss your approach</li> <li>Submit a pull request</li> </ol> <p>Priority areas for contribution: - Phase 1 features (URL support) - Phase 2 features (Energy measurement) - Additional GSF patterns - Documentation improvements</p>"},{"location":"roadmap/#version-history","title":"Version History","text":"<p>See Changelog for release notes.</p>"},{"location":"examples/basic/","title":"Basic Usage Examples","text":"<p>Simple examples to get started with GreenMining.</p>"},{"location":"examples/basic/#analyzing-a-single-repository","title":"Analyzing a Single Repository","text":""},{"location":"examples/basic/#by-url-recommended","title":"By URL (Recommended)","text":"<pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\"https://github.com/pallets/flask\")\n\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Total Commits: {result['total_commits']}\")\nprint(f\"Green-Aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\nprint(\"\\nTop Patterns:\")\nfor pattern, count in sorted(\n    result['pattern_distribution'].items(),\n    key=lambda x: x[1],\n    reverse=True\n)[:5]:\n    print(f\"  - {pattern}: {count}\")\n</code></pre> <p>Output:</p> <pre><code>Repository: flask\nTotal Commits: 200\nGreen-Aware: 47 (23.5%)\n\nTop Patterns:\n  - Cache Static Data: 15\n  - Use Async Instead of Sync: 12  \n  - Lazy Loading: 8\n</code></pre>"},{"location":"examples/basic/#with-energy-measurement-linux","title":"With Energy Measurement (Linux)","text":"<p><pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom greenmining.energy import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\nanalyzer = LocalRepoAnalyzer()\n\nmeter.start()\nresult = analyzer.analyze_repository(\"https://github.com/pallets/flask\")\nenergy = meter.stop()\n\nprint(f\"Energy consumed: {energy.energy_joules:.2f} J\")\nprint(f\"Green-aware: {result['green_aware_percentage']:.1f}%\")\n</code></pre> <pre><code>---\n\n## Detecting Green Patterns in Text\n\n### Single Message Detection\n\n```python\nfrom greenmining import is_green_aware, get_pattern_by_keywords\n\n# Check if a commit message is green-aware\nmessages = [\n    \"Implement Redis caching for user sessions\",\n    \"Enable gzip compression on API responses\",\n    \"Fix typo in README\",\n    \"Migrate to serverless Lambda functions\",\n    \"Update dependencies\",\n    \"Optimize database queries with indexing\",\n]\n\nfor msg in messages:\n    if is_green_aware(msg):\n        patterns = get_pattern_by_keywords(msg)\n        print(f\"\ud83c\udf31 {msg}\")\n        print(f\"   \u2192 Patterns: {patterns}\")\n    else:\n        print(f\"   {msg}\")\n</code></pre></p> <p>Output:</p> <pre><code>\ud83c\udf31 Implement Redis caching for user sessions\n   \u2192 Patterns: ['Cache Static Data']\n\ud83c\udf31 Enable gzip compression on API responses\n   \u2192 Patterns: ['Compress Transmitted Data', 'Enable Text Compression']\n   Fix typo in README\n\ud83c\udf31 Migrate to serverless Lambda functions\n   \u2192 Patterns: ['Use Serverless']\n   Update dependencies\n\ud83c\udf31 Optimize database queries with indexing\n   \u2192 Patterns: ['Optimize Database Queries', 'Index Optimization']\n</code></pre>"},{"location":"examples/basic/#batch-analysis","title":"Batch Analysis","text":"<pre><code>from greenmining import is_green_aware, get_pattern_by_keywords\n\ncommits = [\n    {\"sha\": \"abc123\", \"message\": \"Add lazy loading for images\"},\n    {\"sha\": \"def456\", \"message\": \"Refactor authentication module\"},\n    {\"sha\": \"ghi789\", \"message\": \"Enable HTTP/2 for API server\"},\n    {\"sha\": \"jkl012\", \"message\": \"Update changelog\"},\n    {\"sha\": \"mno345\", \"message\": \"Use WebSockets for real-time updates\"},\n]\n\nresults = []\nfor commit in commits:\n    green = is_green_aware(commit[\"message\"])\n    patterns = get_pattern_by_keywords(commit[\"message\"]) if green else []\n    results.append({\n        **commit,\n        \"green_aware\": green,\n        \"patterns\": patterns\n    })\n\n# Summary\ngreen_count = sum(1 for r in results if r[\"green_aware\"])\nprint(f\"Green-aware: {green_count}/{len(results)} ({100*green_count/len(results):.0f}%)\")\n</code></pre>"},{"location":"examples/basic/#exploring-gsf-patterns","title":"Exploring GSF Patterns","text":""},{"location":"examples/basic/#list-all-patterns","title":"List All Patterns","text":"<pre><code>from greenmining import GSF_PATTERNS\n\n# Count patterns\nprint(f\"Total patterns: {len(GSF_PATTERNS)}\")\n\n# List pattern names\nfor key, pattern in sorted(GSF_PATTERNS.items()):\n    print(f\"  - {pattern['name']} ({pattern['category']})\")\n</code></pre>"},{"location":"examples/basic/#filter-by-category","title":"Filter by Category","text":"<pre><code>from greenmining import GSF_PATTERNS\n\n# Get all categories\ncategories = sorted(set(p[\"category\"] for p in GSF_PATTERNS.values()))\nprint(f\"Categories: {categories}\")\n\n# Get patterns for a specific category\ncategory = \"caching\"\ncaching_patterns = [\n    p for p in GSF_PATTERNS.values() \n    if p[\"category\"] == category\n]\n\nprint(f\"\\n{category.upper()} Patterns:\")\nfor p in caching_patterns:\n    print(f\"  - {p['name']}: {p['keywords'][:3]}...\")\n</code></pre>"},{"location":"examples/basic/#search-patterns-by-keyword","title":"Search Patterns by Keyword","text":"<pre><code>from greenmining import GSF_PATTERNS\n\ndef search_patterns(keyword):\n    \"\"\"Find patterns that match a keyword.\"\"\"\n    matches = []\n    keyword = keyword.lower()\n    for pattern in GSF_PATTERNS.values():\n        if any(keyword in kw for kw in pattern[\"keywords\"]):\n            matches.append(pattern)\n    return matches\n\n# Search examples\nresults = search_patterns(\"redis\")\nprint(\"Patterns matching 'redis':\")\nfor p in results:\n    print(f\"  - {p['name']}\")\n\nresults = search_patterns(\"compress\")\nprint(\"\\nPatterns matching 'compress':\")\nfor p in results:\n    print(f\"  - {p['name']}\")\n</code></pre>"},{"location":"examples/basic/#working-with-configuration","title":"Working with Configuration","text":""},{"location":"examples/basic/#environment-variables","title":"Environment Variables","text":"<pre><code># Set token\nexport GITHUB_TOKEN=ghp_xxxxxxxxxxxx\n</code></pre> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\n# Run analysis\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\"https://github.com/org/repo\")\n</code></pre>"},{"location":"examples/basic/#env-file","title":".env File","text":"<pre><code># Create .env file\ncat &gt; .env &lt;&lt; EOF\nGITHUB_TOKEN=ghp_xxxxxxxxxxxx\nMAX_REPOS=50\nMIN_STARS=500\nENABLE_STATS=true\nEOF\n</code></pre> <pre><code># GreenMining automatically loads .env\nfrom greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=os.getenv(\"GITHUB_TOKEN\"),\n    max_repos=50\n)\n</code></pre>"},{"location":"examples/basic/#python-configuration","title":"Python Configuration","text":"<pre><code>from greenmining.config import Config\n\n# Initialize config\nconfig = Config()\n\n# Override settings\nconfig.MAX_REPOS = 25\nconfig.COMMITS_PER_REPO = 500\nconfig.ENABLE_TEMPORAL = True\n\n# View settings\nprint(f\"MAX_REPOS: {config.MAX_REPOS}\")\nprint(f\"COMMITS_PER_REPO: {config.COMMITS_PER_REPO}\")\nprint(f\"Languages: {config.SUPPORTED_LANGUAGES}\")\n</code></pre>"},{"location":"examples/basic/#reading-analysis-results","title":"Reading Analysis Results","text":""},{"location":"examples/basic/#load-previous-results","title":"Load Previous Results","text":"<pre><code>import json\n\n# Load analysis results\nwith open(\"data/analysis_results.json\") as f:\n    results = json.load(f)\n\n# Basic stats\ntotal = len(results)\ngreen = sum(1 for r in results if r.get(\"green_aware\", False))\nprint(f\"Total commits: {total}\")\nprint(f\"Green-aware: {green} ({100*green/total:.1f}%)\")\n\n# Pattern distribution\nfrom collections import Counter\npatterns = Counter()\nfor r in results:\n    for p in r.get(\"patterns\", []):\n        patterns[p] += 1\n\nprint(\"\\nTop 5 patterns:\")\nfor pattern, count in patterns.most_common(5):\n    print(f\"  {pattern}: {count}\")\n</code></pre>"},{"location":"examples/basic/#load-statistics","title":"Load Statistics","text":"<pre><code>import json\n\nwith open(\"data/aggregated_statistics.json\") as f:\n    stats = json.load(f)\n\n# Summary\nsummary = stats.get(\"summary\", {})\nprint(f\"Total commits: {summary.get('total_commits', 0)}\")\nprint(f\"Green awareness: {summary.get('green_aware_percentage', 0):.1f}%\")\n\n# Top patterns\nprint(\"\\nTop patterns:\")\nfor pattern, count in sorted(\n    stats.get(\"pattern_distribution\", {}).items(),\n    key=lambda x: x[1],\n    reverse=True\n)[:5]:\n    print(f\"  {pattern}: {count}\")\n</code></pre>"},{"location":"examples/basic/#quick-reference","title":"Quick Reference","text":"<pre><code>from greenmining import (\n    is_green_aware,           # Check if message is green-aware\n    get_pattern_by_keywords,  # Get matched patterns\n    GSF_PATTERNS,             # 122 GSF patterns dictionary\n    GREEN_KEYWORDS,           # 321 green keywords list\n    fetch_repositories        # Fetch repos from GitHub\n)\n\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom greenmining.energy import RAPLEnergyMeter, CodeCarbonMeter\nfrom greenmining.config import Config\n</code></pre>"},{"location":"examples/basic/#next-steps","title":"Next Steps","text":"<ul> <li>Complete Pipeline Example - Research workflow</li> <li>Python API - Programmatic usage</li> <li>Configuration - All settings</li> </ul>"},{"location":"examples/pipeline/","title":"Complete Pipeline Example","text":"<p>A comprehensive research workflow example using GreenMining.</p>"},{"location":"examples/pipeline/#research-scenario","title":"Research Scenario","text":"<p>We'll analyze green software practices in Python microservices projects:</p> <ol> <li>Fetch repositories from GitHub</li> <li>Extract and analyze commits</li> <li>Compute statistics</li> <li>Generate results</li> </ol>"},{"location":"examples/pipeline/#step-1-setup","title":"Step 1: Setup","text":""},{"location":"examples/pipeline/#install-greenmining","title":"Install GreenMining","text":"<pre><code>pip install greenmining\n</code></pre>"},{"location":"examples/pipeline/#configure-github-token","title":"Configure GitHub Token","text":"<pre><code>import os\nos.environ[\"GITHUB_TOKEN\"] = \"ghp_xxxxxxxxxxxxxxxxxxxx\"\n</code></pre>"},{"location":"examples/pipeline/#step-2-run-full-pipeline-python-api","title":"Step 2: Run Full Pipeline (Python API)","text":""},{"location":"examples/pipeline/#complete-script","title":"Complete Script","text":"<p><pre><code>#!/usr/bin/env python3\n\"\"\"\nGreen Software Research Pipeline\nAnalyzes green patterns in Python microservices projects\n\"\"\"\n\nimport os\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\nfrom greenmining.config import Config\nfrom greenmining.services import (\n    GitHubFetcher,\n    CommitExtractor,\n    DataAnalyzer,\n    DataAggregator,\n    ReportGenerator,\n)\n\n# Configuration\nGITHUB_TOKEN = os.environ.get(\"GITHUB_TOKEN\")\nMAX_REPOS = 20\nMIN_STARS = 500\nLANGUAGES = [\"Python\"]\nKEYWORDS = \"microservices\"\nMAX_COMMITS = 500\n\n# Output directory\noutput_dir = Path(\"data\")\noutput_dir.mkdir(exist_ok=True)\n\nprint(\"=\" * 60)\nprint(\"GREEN SOFTWARE RESEARCH PIPELINE\")\nprint(\"=\" * 60)\n\n# Step 1: Fetch repositories\nprint(\"\\n[1/5] Fetching repositories...\")\nfetcher = GitHubFetcher(\n    token=GITHUB_TOKEN,\n    max_repos=MAX_REPOS,\n    min_stars=MIN_STARS,\n    languages=LANGUAGES,\n)\nrepos = fetcher.search_repositories(keywords=KEYWORDS)\nprint(f\"    Found {len(repos)} repositories\")\n\n# Step 2: Extract commits\nprint(\"\\n[2/5] Extracting commits...\")\nextractor = CommitExtractor(max_commits=MAX_COMMITS)\ncommits = extractor.extract_from_repositories(repos)\nprint(f\"    Extracted {len(commits)} commits\")\n\n# Step 3: Analyze commits\nprint(\"\\n[3/5] Analyzing for green patterns...\")\nanalyzer = DataAnalyzer()\nresults = analyzer.analyze_commits(commits)\ngreen_count = sum(1 for r in results if r.get(\"green_aware\"))\nprint(f\"    Green-aware: {green_count} ({100*green_count/len(results):.1f}%)\")\n\n# Step 4: Aggregate statistics\nprint(\"\\n[4/5] Aggregating statistics...\")\naggregator = DataAggregator(\n    enable_stats=True,\n    enable_temporal=True,\n    temporal_granularity=\"quarter\"\n)\nstats = aggregator.aggregate(results, repos)\n\n# Step 5: Save results\nprint(\"\\n[5/5] Saving results...\")\nwith open(output_dir / \"aggregated_statistics.json\", \"w\") as f:\n    json.dump(stats, f, indent=2, default=str)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ANALYSIS COMPLETE\")\nprint(\"=\" * 60)\nprint(f\"Repositories: {len(repos)}\")\nprint(f\"Commits: {len(commits)}\")\nprint(f\"Green-aware: {green_count} ({100*green_count/len(results):.1f}%)\")\nprint(f\"\\nResults saved to: {output_dir.absolute()}\")\n</code></pre> from greenmining.services import (     GitHubFetcher,     CommitExtractor,     DataAnalyzer,     DataAggregator,     ReportGenerator )</p> <p>def main():     print(\"\ud83c\udf31 Green Software Research Pipeline\")     print(\"=\" * 50)</p> <pre><code># Configuration\nconfig = Config()\nconfig.MAX_REPOS = 20\nconfig.MIN_STARS = 500\nconfig.SUPPORTED_LANGUAGES = [\"Python\"]\nconfig.SEARCH_KEYWORDS = \"microservices\"\nconfig.COMMITS_PER_REPO = 500\nconfig.ENABLE_STATS = True\nconfig.ENABLE_TEMPORAL = True\nconfig.TEMPORAL_GRANULARITY = \"quarter\"\n\noutput_dir = Path(\"data\")\noutput_dir.mkdir(exist_ok=True)\n\n# Step 1: Fetch repositories\nprint(\"\\n\ud83d\udce5 Step 1: Fetching repositories...\")\nfetcher = GitHubFetcher(config)\nrepos = fetcher.fetch_repositories()\nprint(f\"   Found {len(repos)} repositories\")\n\nwith open(output_dir / \"repositories.json\", \"w\") as f:\n    json.dump(repos, f, indent=2, default=str)\n\n# Step 2: Extract commits\nprint(\"\\n\ud83d\udcdd Step 2: Extracting commits...\")\nextractor = CommitExtractor(config)\ncommits = extractor.extract_commits(repos)\nprint(f\"   Extracted {len(commits)} commits\")\n\nwith open(output_dir / \"commits.json\", \"w\") as f:\n    json.dump(commits, f, indent=2, default=str)\n\n# Step 3: Analyze commits\nprint(\"\\n\ud83d\udd0d Step 3: Analyzing commits...\")\nanalyzer = DataAnalyzer(config)\nresults = analyzer.analyze(commits)\n\ngreen_count = sum(1 for r in results if r.get(\"green_aware\", False))\nprint(f\"   Green-aware: {green_count}/{len(results)} ({100*green_count/len(results):.1f}%)\")\n\nwith open(output_dir / \"analysis_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2, default=str)\n\n# Step 4: Aggregate statistics\nprint(\"\\n\ud83d\udcca Step 4: Computing statistics...\")\naggregator = DataAggregator(config)\nstats = aggregator.aggregate(results)\n\nwith open(output_dir / \"aggregated_statistics.json\", \"w\") as f:\n    json.dump(stats, f, indent=2, default=str)\n\n# Step 5: Generate report\nprint(\"\\n\ud83d\udcc4 Step 5: Generating report...\")\nreporter = ReportGenerator(config)\nreport = reporter.generate(stats)\n\nwith open(output_dir / \"green_analysis.md\", \"w\") as f:\n    f.write(report)\n\n# Summary\nprint(\"\\n\" + \"=\" * 50)\nprint(\"\u2705 Pipeline Complete!\")\nprint(f\"\\nResults Summary:\")\nprint(f\"   Repositories: {len(repos)}\")\nprint(f\"   Total Commits: {len(results)}\")\nprint(f\"   Green-Aware: {green_count} ({100*green_count/len(results):.1f}%)\")\n\nif \"pattern_distribution\" in stats:\n    print(f\"\\nTop 5 Patterns:\")\n    for pattern, count in sorted(\n        stats[\"pattern_distribution\"].items(),\n        key=lambda x: x[1],\n        reverse=True\n    )[:5]:\n        print(f\"   - {pattern}: {count}\")\n\nprint(f\"\\nOutput files in: {output_dir.absolute()}\")\n</code></pre> <p>if name == \"main\":     main() <pre><code>### Run Script\n\n```bash\npython research_pipeline.py\n</code></pre></p>"},{"location":"examples/pipeline/#step-4-url-based-analysis","title":"Step 4: URL-Based Analysis","text":"<p>For analyzing specific repositories by URL:</p>"},{"location":"examples/pipeline/#analyze-single-repository","title":"Analyze Single Repository","text":"<pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nimport json\n\nanalyzer = LocalRepoAnalyzer()\n\n# Analyze Flask web framework\nresult = analyzer.analyze_repository(\"https://github.com/pallets/flask\")\n\n# Save results\nwith open(\"flask_analysis.json\", \"w\") as f:\n    json.dump(result, f, indent=2, default=str)\n\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Green-aware: {result['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"examples/pipeline/#analyze-multiple-repositories","title":"Analyze Multiple Repositories","text":"<pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nimport json\n\nrepos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n    \"https://github.com/fastapi/fastapi\",\n]\n\nanalyzer = LocalRepoAnalyzer()\nresults = []\n\nfor url in repos:\n    print(f\"Analyzing {url}...\")\n    result = analyzer.analyze_repository(url)\n    results.append(result)\n\n    print(f\"  Commits: {result['total_commits']}\")\n    print(f\"  Green: {result['green_aware_percentage']:.1f}%\")\n\n# Save combined results\nwith open(\"multi_repo_analysis.json\", \"w\") as f:\n    json.dump(results, f, indent=2, default=str)\n\n# Compare frameworks\nprint(\"\\nComparison:\")\nfor r in sorted(results, key=lambda x: x[\"green_aware_percentage\"], reverse=True):\n    name = r[\"repository\"][\"name\"]\n    pct = r[\"green_aware_percentage\"]\n    print(f\"  {name}: {pct:.1f}%\")\n</code></pre>"},{"location":"examples/pipeline/#step-5-analyze-results","title":"Step 5: Analyze Results","text":""},{"location":"examples/pipeline/#load-and-explore-data","title":"Load and Explore Data","text":"<pre><code>import json\nfrom collections import Counter\n\n# Load results\nwith open(\"data/analysis_results.json\") as f:\n    results = json.load(f)\n\nwith open(\"data/aggregated_statistics.json\") as f:\n    stats = json.load(f)\n\n# Basic statistics\ntotal = len(results)\ngreen = sum(1 for r in results if r.get(\"green_aware\", False))\n\nprint(f\"Total commits analyzed: {total}\")\nprint(f\"Green-aware commits: {green} ({100*green/total:.1f}%)\")\n</code></pre>"},{"location":"examples/pipeline/#pattern-analysis","title":"Pattern Analysis","text":"<pre><code># Pattern frequency\npatterns = Counter()\nfor result in results:\n    for pattern in result.get(\"patterns\", []):\n        patterns[pattern] += 1\n\nprint(\"\\nPattern Distribution:\")\nfor pattern, count in patterns.most_common(10):\n    pct = 100 * count / green\n    print(f\"  {pattern}: {count} ({pct:.1f}%)\")\n</code></pre>"},{"location":"examples/pipeline/#category-analysis","title":"Category Analysis","text":"<pre><code># Category distribution\ncategories = Counter()\nfor result in results:\n    if result.get(\"category\"):\n        categories[result[\"category\"]] += 1\n\nprint(\"\\nCategory Distribution:\")\nfor category, count in categories.most_common():\n    pct = 100 * count / green\n    print(f\"  {category}: {count} ({pct:.1f}%)\")\n</code></pre>"},{"location":"examples/pipeline/#temporal-trends","title":"Temporal Trends","text":"<pre><code>from datetime import datetime\nfrom collections import defaultdict\n\n# Group by month\nmonthly = defaultdict(lambda: {\"total\": 0, \"green\": 0})\n\nfor result in results:\n    date = datetime.fromisoformat(result[\"date\"].replace(\"Z\", \"+00:00\"))\n    month = date.strftime(\"%Y-%m\")\n    monthly[month][\"total\"] += 1\n    if result.get(\"green_aware\"):\n        monthly[month][\"green\"] += 1\n\nprint(\"\\nMonthly Trend:\")\nfor month in sorted(monthly.keys())[-12:]:  # Last 12 months\n    data = monthly[month]\n    pct = 100 * data[\"green\"] / data[\"total\"] if data[\"total\"] &gt; 0 else 0\n    bar = \"\u2588\" * int(pct / 5)\n    print(f\"  {month}: {pct:5.1f}% {bar}\")\n</code></pre>"},{"location":"examples/pipeline/#step-6-export-for-research","title":"Step 6: Export for Research","text":""},{"location":"examples/pipeline/#export-to-csv","title":"Export to CSV","text":"<pre><code>import csv\n\nwith open(\"green_commits.csv\", \"w\", newline=\"\") as f:\n    writer = csv.DictWriter(f, fieldnames=[\n        \"sha\", \"repository\", \"author\", \"date\",\n        \"green_aware\", \"patterns\", \"category\", \"message\"\n    ])\n    writer.writeheader()\n\n    for r in results:\n        writer.writerow({\n            \"sha\": r[\"sha\"][:8],\n            \"repository\": r.get(\"repository\", \"\"),\n            \"author\": r.get(\"author\", \"\"),\n            \"date\": r.get(\"date\", \"\"),\n            \"green_aware\": r.get(\"green_aware\", False),\n            \"patterns\": \"|\".join(r.get(\"patterns\", [])),\n            \"category\": r.get(\"category\", \"\"),\n            \"message\": r.get(\"message\", \"\")[:100]\n        })\n\nprint(\"Exported to green_commits.csv\")\n</code></pre>"},{"location":"examples/pipeline/#export-to-latex-table","title":"Export to LaTeX Table","text":"<pre><code>def to_latex_table(pattern_counts, top_n=10):\n    \"\"\"Generate LaTeX table from pattern counts.\"\"\"\n    lines = [\n        r\"\\begin{table}[h]\",\n        r\"\\centering\",\n        r\"\\caption{Top Green Software Patterns}\",\n        r\"\\begin{tabular}{lrr}\",\n        r\"\\toprule\",\n        r\"Pattern &amp; Count &amp; Percentage \\\\\",\n        r\"\\midrule\",\n    ]\n\n    total = sum(pattern_counts.values())\n    for pattern, count in pattern_counts.most_common(top_n):\n        pct = 100 * count / total\n        lines.append(f\"{pattern} &amp; {count} &amp; {pct:.1f}\\\\% \\\\\\\\\")\n\n    lines.extend([\n        r\"\\bottomrule\",\n        r\"\\end{tabular}\",\n        r\"\\end{table}\",\n    ])\n\n    return \"\\n\".join(lines)\n\nlatex = to_latex_table(patterns)\nprint(latex)\n</code></pre>"},{"location":"examples/pipeline/#output-files","title":"Output Files","text":"<p>After running the pipeline:</p> <pre><code>data/\n\u251c\u2500\u2500 repositories.json      # Fetched repository metadata\n\u251c\u2500\u2500 commits.json           # Extracted commits\n\u251c\u2500\u2500 analysis_results.json  # Per-commit analysis\n\u251c\u2500\u2500 aggregated_statistics.json  # Statistics &amp; trends\n\u2514\u2500\u2500 green_analysis.md      # Markdown report\n</code></pre>"},{"location":"examples/pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Examples - Simple usage patterns</li> <li>Python API - Full API reference</li> <li>Configuration - All options</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>GreenMining can be configured via environment variables, <code>.env</code> files, or programmatically.</p>"},{"location":"getting-started/configuration/#configuration-methods","title":"Configuration Methods","text":""},{"location":"getting-started/configuration/#method-1-environment-variables","title":"Method 1: Environment Variables","text":"<p>Set variables directly in your shell:</p> <pre><code>export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\nexport MAX_REPOS=50\nexport MIN_STARS=500\nexport ENABLE_TEMPORAL=true\n</code></pre>"},{"location":"getting-started/configuration/#method-2-env-file","title":"Method 2: .env File","text":"<p>Create a <code>.env</code> file in your project directory:</p> <pre><code># Required\nGITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n\n# Repository Fetching\nMAX_REPOS=100\nMIN_STARS=100\nSUPPORTED_LANGUAGES=Python,Java,Go,JavaScript,TypeScript\nSEARCH_KEYWORDS=microservices\n\n# Commit Extraction\nCOMMITS_PER_REPO=1000\nEXCLUDE_MERGE_COMMITS=true\nEXCLUDE_BOT_COMMITS=true\n\n# Analysis Features\nENABLE_DIFF_ANALYSIS=false\nBATCH_SIZE=10\n\n# Temporal Analysis\nENABLE_TEMPORAL=true\nTEMPORAL_GRANULARITY=quarter\nENABLE_STATS=true\n\n# Output\nOUTPUT_DIR=./data\nREPORT_FORMAT=markdown\n</code></pre>"},{"location":"getting-started/configuration/#method-3-python-config-object","title":"Method 3: Python Config Object","text":"<pre><code>from greenmining.config import Config\n\nconfig = Config()\n\n# Override settings\nconfig.MAX_REPOS = 50\nconfig.COMMITS_PER_REPO = 200\nconfig.MIN_STARS = 500\n</code></pre>"},{"location":"getting-started/configuration/#configuration-options-reference","title":"Configuration Options Reference","text":""},{"location":"getting-started/configuration/#github-api","title":"GitHub API","text":"Option Type Default Description <code>GITHUB_TOKEN</code> string (required) GitHub personal access token"},{"location":"getting-started/configuration/#repository-fetching","title":"Repository Fetching","text":"Option Type Default Description <code>MAX_REPOS</code> int 100 Maximum repositories to fetch <code>MIN_STARS</code> int 100 Minimum GitHub stars required <code>SUPPORTED_LANGUAGES</code> list Python,Java,Go,... Language filter (comma-separated) <code>SEARCH_KEYWORDS</code> string microservices Search keywords for GitHub API"},{"location":"getting-started/configuration/#url-analysis","title":"URL Analysis","text":"Option Type Default Description <code>REPOSITORY_URLS</code> list [] List of repository URLs to analyze <code>CLONE_PATH</code> string /tmp/greenmining_repos Directory for cloning repositories <code>CLEANUP_AFTER_ANALYSIS</code> bool true Delete cloned repos after analysis"},{"location":"getting-started/configuration/#commit-extraction","title":"Commit Extraction","text":"Option Type Default Description <code>COMMITS_PER_REPO</code> int 1000 Maximum commits per repository <code>EXCLUDE_MERGE_COMMITS</code> bool true Skip merge commits <code>EXCLUDE_BOT_COMMITS</code> bool true Skip bot commits <code>MIN_MESSAGE_LENGTH</code> int 10 Minimum commit message length"},{"location":"getting-started/configuration/#analysis-features","title":"Analysis Features","text":"Option Type Default Description <code>ENABLE_DIFF_ANALYSIS</code> bool false Analyze code diffs (slower) <code>BATCH_SIZE</code> int 10 Commits per batch"},{"location":"getting-started/configuration/#pydriller-options","title":"PyDriller Options","text":"Option Type Default Description <code>PROCESS_METRICS_ENABLED</code> bool true Compute process metrics <code>STRUCTURAL_METRICS_ENABLED</code> bool true Compute structural metrics <code>DMM_ENABLED</code> bool true Delta Maintainability Model"},{"location":"getting-started/configuration/#statistical-analysis","title":"Statistical Analysis","text":"Option Type Default Description <code>ENABLE_STATS</code> bool false Enable statistical analysis <code>ENABLE_TEMPORAL</code> bool false Enable temporal trend analysis <code>TEMPORAL_GRANULARITY</code> string quarter day/week/month/quarter/year"},{"location":"getting-started/configuration/#energy-measurement","title":"Energy Measurement","text":"Option Type Default Description <code>ENERGY_ENABLED</code> bool false Enable energy measurement <code>ENERGY_BACKEND</code> string rapl rapl, codecarbon, or cpu_meter <code>CARBON_TRACKING</code> bool false Track carbon emissions <code>COUNTRY_ISO</code> string USA Country for carbon calculations"},{"location":"getting-started/configuration/#output-configuration","title":"Output Configuration","text":"Option Type Default Description <code>OUTPUT_DIR</code> string ./data Output directory path <code>REPOS_FILE</code> string repositories.json Repository data filename <code>COMMITS_FILE</code> string commits.json Commits data filename <code>ANALYSIS_FILE</code> string analysis_results.json Analysis results filename <code>STATS_FILE</code> string aggregated_statistics.json Statistics filename <code>REPORT_FILE</code> string green_analysis.md Report filename"},{"location":"getting-started/configuration/#yaml-configuration","title":"YAML Configuration","text":"<p>For advanced configuration, use a YAML file:</p> <p>config/config.yaml:</p> <pre><code># GreenMining Configuration\nversion: \"2.0\"\n\nsearch:\n  keywords: [\"microservices\", \"kubernetes\"]\n  languages: [\"Python\", \"Go\", \"Java\"]\n  min_stars: 100\n  max_repos: 100\n\n  temporal:\n    created_after: \"2020-01-01\"\n    created_before: \"2025-12-31\"\n    pushed_after: \"2023-01-01\"\n\ndetection:\n  methods:\n    keyword:\n      enabled: true\n      confidence_weight: 1.0\n\n    diff_analysis:\n      enabled: true\n      confidence_weight: 0.8\n\nanalysis:\n  statistical:\n    enabled: true\n    methods:\n      - chi_square\n      - correlation_analysis\n      - temporal_trends\n      - effect_sizes\n</code></pre>"},{"location":"getting-started/configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>Configuration values are loaded in this order (later overrides earlier):</p> <ol> <li>Default values in <code>Config</code> class</li> <li>YAML config file if specified</li> <li><code>.env</code> file in current directory</li> <li>Environment variables</li> <li>Python Config overrides</li> </ol>"},{"location":"getting-started/configuration/#validating-configuration","title":"Validating Configuration","text":"<p>Check current configuration in Python:</p> <pre><code>from greenmining.config import Config\n\nconfig = Config()\n\n# Check settings\nprint(f\"GITHUB_TOKEN: {'***configured***' if config.GITHUB_TOKEN else 'Not set'}\")\nprint(f\"MAX_REPOS: {config.MAX_REPOS}\")\nprint(f\"COMMITS_PER_REPO: {config.COMMITS_PER_REPO}\")\nprint(f\"OUTPUT_DIR: {config.OUTPUT_DIR}\")\nprint(f\"ENABLE_TEMPORAL: {config.ENABLE_TEMPORAL}\")\nprint(f\"TEMPORAL_GRANULARITY: {config.TEMPORAL_GRANULARITY}\")\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Get started with examples</li> <li>Python API - Programmatic configuration</li> <li>Config Options Reference - Full reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers all methods to install GreenMining.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.9 or higher</li> <li>Operating System: Linux, macOS, or Windows</li> <li>GitHub Token: Required for repository fetching (optional for URL analysis)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-pip-recommended","title":"Method 1: pip (Recommended)","text":"<p>Install from PyPI:</p> <pre><code>pip install greenmining\n</code></pre> <p>Verify installation:</p> <pre><code>python -c \"import greenmining; print(f'greenmining v{greenmining.__version__}')\"\n# Output: greenmining v1.0.4\n</code></pre>"},{"location":"getting-started/installation/#method-2-from-source","title":"Method 2: From Source","text":"<p>Clone and install for development:</p> <pre><code># Clone repository\ngit clone https://github.com/adam-bouafia/greenmining.git\ncd greenmining\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#method-3-docker","title":"Method 3: Docker","text":"<p>Run using Docker:</p> <pre><code># Pull image\ndocker pull adambouafia/greenmining:latest\n\n# Interactive Python shell\ndocker run -it -v $(pwd)/data:/app/data \\\n           adambouafia/greenmining:latest python\n\n# Run a Python script\ndocker run -v $(pwd)/data:/app/data \\\n           -e GITHUB_TOKEN=your_token \\\n           adambouafia/greenmining:latest python your_script.py\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>GreenMining automatically installs these dependencies:</p> Package Purpose <code>PyGithub&gt;=2.1.1</code> GitHub API access <code>PyDriller&gt;=2.5</code> Repository mining <code>pandas&gt;=2.2.0</code> Data manipulation <code>scipy&gt;=1.10.0</code> Statistical analysis <code>numpy&gt;=1.24.0</code> Numerical operations <code>python-dotenv</code> Environment variable loading <code>tqdm</code> Progress bars"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For energy measurement features:</p> <pre><code># CodeCarbon support\npip install codecarbon\n\n# Full development environment\npip install greenmining[dev]\n</code></pre>"},{"location":"getting-started/installation/#github-token-setup","title":"GitHub Token Setup","text":"<p>A GitHub personal access token is required for the <code>fetch</code> and <code>pipeline</code> commands.</p>"},{"location":"getting-started/installation/#creating-a-token","title":"Creating a Token","text":"<ol> <li>Go to GitHub Settings \u2192 Developer settings \u2192 Personal access tokens</li> <li>Click \"Generate new token (classic)\"</li> <li>Select scopes:<ul> <li><code>repo</code> (for private repositories)</li> <li><code>public_repo</code> (for public repositories only)</li> </ul> </li> <li>Copy the generated token</li> </ol>"},{"location":"getting-started/installation/#configuring-the-token","title":"Configuring the Token","text":"<p>Option 1: Environment Variable</p> <pre><code>export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre> <p>Option 2: .env File</p> <p>Create a <code>.env</code> file in your project directory:</p> <pre><code>GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre> <p>Option 3: Pass to Functions</p> <p>Pass directly to API functions:</p> <pre><code>from greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=\"ghp_xxxxxxxxxxxxxxxxxxxx\",\n    max_repos=10\n)\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>Run the following to verify everything works:</p> <pre><code># Check version\nimport greenmining\nprint(f\"greenmining v{greenmining.__version__}\")\n\n# Check available exports\nfrom greenmining import GSF_PATTERNS, GREEN_KEYWORDS, is_green_aware\nprint(f\"{len(GSF_PATTERNS)} patterns loaded\")\nprint(f\"{len(GREEN_KEYWORDS)} keywords loaded\")\n\n# Test pattern detection\nprint(is_green_aware(\"Enable caching\"))  # True\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>Issue: <code>ModuleNotFoundError: No module named 'greenmining'</code></p> <p>Solution: Ensure you've activated your virtual environment and installed the package:</p> <pre><code>source venv/bin/activate\npip install greenmining\n</code></pre> <p>Issue: <code>GITHUB_TOKEN not set</code></p> <p>Solution: Set the environment variable or create a <code>.env</code> file:</p> <pre><code>export GITHUB_TOKEN=your_token_here\n</code></pre> <p>Issue: <code>Rate limit exceeded</code></p> <p>Solution: GitHub API has rate limits. Either:</p> <ul> <li>Wait for the rate limit to reset (1 hour)</li> <li>Use an authenticated token for higher limits</li> <li>Reduce <code>--max-repos</code> parameter</li> </ul> <p>Issue: <code>Permission denied</code> on RAPL energy measurement</p> <p>Solution: RAPL requires root access or specific permissions:</p> <pre><code># Grant read access to energy files\nsudo chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\n</code></pre> <p>Or use CodeCarbon instead (no root needed):</p> <pre><code>from greenmining.energy import CodeCarbonMeter\nmeter = CodeCarbonMeter()\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Run your first analysis</li> <li>Configuration - Customize GreenMining settings</li> <li>Python API - Complete API reference</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with GreenMining in 5 minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>GitHub token (for fetching repositories)</li> <li>GreenMining installed (<code>pip install greenmining</code>)</li> </ul>"},{"location":"getting-started/quickstart/#option-1-analyze-a-repository-by-url","title":"Option 1: Analyze a Repository by URL","text":"<p>Analyze any public GitHub repository directly:</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\n    \"https://github.com/pallets/flask\",\n    max_commits=100\n)\n\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Total commits: {result['total_commits']}\")\nprint(f\"Green-aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\nprint(\"\\nTop Patterns:\")\nfor pattern, count in sorted(\n    result['pattern_distribution'].items(),\n    key=lambda x: x[1],\n    reverse=True\n)[:5]:\n    print(f\"  - {pattern}: {count}\")\n</code></pre> <p>Output:</p> <pre><code>Repository: flask\nTotal commits: 100\nGreen-aware: 23 (23.0%)\n\nTop Patterns:\n  - Cache Static Data: 8\n  - Use Async Instead of Sync: 5\n  - Lazy Loading: 4\n</code></pre>"},{"location":"getting-started/quickstart/#option-2-full-pipeline-comprehensive","title":"Option 2: Full Pipeline (Comprehensive)","text":"<p>Run the complete analysis pipeline programmatically:</p> <pre><code>import os\nfrom greenmining.services import (\n    GitHubFetcher,\n    CommitExtractor,\n    DataAnalyzer,\n    DataAggregator,\n)\n\n# Set your GitHub token\ngithub_token = os.environ.get(\"GITHUB_TOKEN\")\n\n# 1. Fetch repositories\nfetcher = GitHubFetcher(token=github_token, max_repos=5, min_stars=500)\nrepos = fetcher.search_repositories()\n\n# 2. Extract commits\nextractor = CommitExtractor(max_commits=50)\ncommits = extractor.extract_from_repositories(repos)\n\n# 3. Analyze commits\nanalyzer = DataAnalyzer()\nresults = analyzer.analyze_commits(commits)\n\n# 4. Aggregate statistics\naggregator = DataAggregator(enable_stats=True, enable_temporal=True)\nstats = aggregator.aggregate(results, repos)\n\n# Print results\nprint(f\"Repositories analyzed: {len(repos)}\")\nprint(f\"Total commits: {len(commits)}\")\nprint(f\"Green-aware: {stats['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"getting-started/quickstart/#option-3-quick-pattern-detection","title":"Option 3: Quick Pattern Detection","text":"<p>Use GreenMining programmatically:</p> <pre><code>from greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\n\n# Check pattern count\nprint(f\"Loaded {len(GSF_PATTERNS)} GSF patterns\")\n# Output: Loaded 122 GSF patterns\n\n# Test green awareness detection\nmessages = [\n    \"Optimize Redis caching for better performance\",\n    \"Fix typo in README\",\n    \"Enable gzip compression for API responses\",\n    \"Update dependencies to latest versions\",\n]\n\nfor msg in messages:\n    is_green = is_green_aware(msg)\n    patterns = get_pattern_by_keywords(msg) if is_green else []\n    status = \"\ud83c\udf31\" if is_green else \"  \"\n    print(f\"{status} {msg[:50]}\")\n    if patterns:\n        print(f\"   \u2192 Patterns: {patterns}\")\n</code></pre> <p>Output:</p> <pre><code>\ud83c\udf31 Optimize Redis caching for better performance\n   \u2192 Patterns: ['Cache Static Data']\n   Fix typo in README\n\ud83c\udf31 Enable gzip compression for API responses\n   \u2192 Patterns: ['Compress Transmitted Data', 'Enable Text Compression']\n   Update dependencies to latest versions\n</code></pre>"},{"location":"getting-started/quickstart/#output-files","title":"Output Files","text":"<p>When running the pipeline, outputs are saved to the <code>data/</code> directory:</p> File Description <code>repositories.json</code> Fetched repository metadata <code>commits.json</code> Extracted commit data <code>analysis_results.json</code> Pattern detection results <code>aggregated_statistics.json</code> Summary statistics <code>aggregated_data.json</code> Full aggregated data"},{"location":"getting-started/quickstart/#quick-test-script","title":"Quick Test Script","text":"<p>Create <code>test_greenmining.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Quick test of GreenMining functionality.\"\"\"\n\nfrom greenmining import GSF_PATTERNS, GREEN_KEYWORDS, is_green_aware, get_pattern_by_keywords\n\n# Test 1: Check patterns loaded\nprint(f\"\u2713 Loaded {len(GSF_PATTERNS)} GSF patterns\")\nprint(f\"\u2713 Loaded {len(GREEN_KEYWORDS)} green keywords\")\n\n# Test 2: Get categories\ncategories = set(p[\"category\"] for p in GSF_PATTERNS.values())\nprint(f\"\u2713 Categories: {', '.join(sorted(categories))}\")\n\n# Test 3: Pattern detection\ntest_msg = \"Implement Redis caching to reduce database load\"\nis_green = is_green_aware(test_msg)\npatterns = get_pattern_by_keywords(test_msg)\n\nprint(f\"\u2713 Test message: '{test_msg}'\")\nprint(f\"  Green-aware: {is_green}\")\nprint(f\"  Patterns: {patterns}\")\n\nprint(\"\\n\u2705 All tests passed!\")\n</code></pre> <p>Run it:</p> <pre><code>python test_greenmining.py\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Customize settings</li> <li>Python API - Programmatic usage</li> <li>GSF Patterns - All 122 patterns</li> <li>URL Analysis - Analyze by URL</li> </ul>"},{"location":"reference/config-options/","title":"Configuration Options Reference","text":"<p>Complete reference for all GreenMining configuration options.</p>"},{"location":"reference/config-options/#configuration-methods","title":"Configuration Methods","text":"<p>GreenMining supports multiple configuration methods:</p> <ol> <li>Environment variables - <code>export OPTION=value</code></li> <li><code>.env</code> file - Key-value pairs in project root</li> <li>Python Config object - Programmatic configuration</li> </ol>"},{"location":"reference/config-options/#precedence-order","title":"Precedence Order","text":"<p>Later sources override earlier ones:</p> <ol> <li>Default values (lowest priority)</li> <li><code>.env</code> file</li> <li>Environment variables</li> <li>Python Config overrides (highest priority)</li> </ol>"},{"location":"reference/config-options/#github-api-options","title":"GitHub API Options","text":""},{"location":"reference/config-options/#github_token","title":"GITHUB_TOKEN","text":"<p>GitHub personal access token for API access.</p> Property Value Type string Default (none - required) Environment <code>GITHUB_TOKEN</code> Required Yes (for fetch operations) <pre><code>export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre>"},{"location":"reference/config-options/#repository-fetching-options","title":"Repository Fetching Options","text":""},{"location":"reference/config-options/#max_repos","title":"MAX_REPOS","text":"<p>Maximum number of repositories to fetch from GitHub.</p> Property Value Type integer Default 100 Environment <code>MAX_REPOS</code> <pre><code>from greenmining import fetch_repositories\n\nrepos = fetch_repositories(github_token=token, max_repos=50)\n</code></pre>"},{"location":"reference/config-options/#min_stars","title":"MIN_STARS","text":"<p>Minimum GitHub star count required for repositories.</p> Property Value Type integer Default 100 Environment <code>MIN_STARS</code> <pre><code>repos = fetch_repositories(github_token=token, min_stars=500)\n</code></pre>"},{"location":"reference/config-options/#supported_languages","title":"SUPPORTED_LANGUAGES","text":"<p>Programming languages to filter repositories.</p> Property Value Type list (comma-separated) Default Python,Java,Go,JavaScript,TypeScript,C#,Rust Environment <code>SUPPORTED_LANGUAGES</code> <pre><code>repos = fetch_repositories(github_token=token, languages=[\"Python\", \"Go\", \"Rust\"])\n</code></pre>"},{"location":"reference/config-options/#search_keywords","title":"SEARCH_KEYWORDS","text":"<p>Keywords for GitHub repository search.</p> Property Value Type string Default microservices Environment <code>SEARCH_KEYWORDS</code> <pre><code>repos = fetch_repositories(github_token=token, keywords=\"kubernetes cloud-native\")\n</code></pre>"},{"location":"reference/config-options/#url-analysis-options","title":"URL Analysis Options","text":""},{"location":"reference/config-options/#repository_urls","title":"REPOSITORY_URLS","text":"<p>List of repository URLs to analyze directly.</p> Property Value Type list Default [] Environment <code>REPOSITORY_URLS</code> (comma-separated) <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(max_commits=100)\nresult = analyzer.analyze_repository(\"https://github.com/org/repo\")\n</code></pre>"},{"location":"reference/config-options/#clone_path","title":"CLONE_PATH","text":"<p>Directory where repositories are cloned for analysis.</p> Property Value Type string (path) Default /tmp/greenmining_repos Environment <code>CLONE_PATH</code> <pre><code>export CLONE_PATH=/var/data/greenmining\n</code></pre>"},{"location":"reference/config-options/#cleanup_after_analysis","title":"CLEANUP_AFTER_ANALYSIS","text":"<p>Whether to delete cloned repositories after analysis.</p> Property Value Type boolean Default true Environment <code>CLEANUP_AFTER_ANALYSIS</code> <pre><code>export CLEANUP_AFTER_ANALYSIS=false\n</code></pre>"},{"location":"reference/config-options/#commit-extraction-options","title":"Commit Extraction Options","text":""},{"location":"reference/config-options/#commits_per_repo","title":"COMMITS_PER_REPO","text":"<p>Maximum commits to extract per repository.</p> Property Value Type integer Default 1000 Environment <code>COMMITS_PER_REPO</code> <pre><code>from greenmining.services.commit_extractor import CommitExtractor\n\nextractor = CommitExtractor()\nextractor.max_commits = 500\n</code></pre>"},{"location":"reference/config-options/#exclude_merge_commits","title":"EXCLUDE_MERGE_COMMITS","text":"<p>Skip merge commits during extraction.</p> Property Value Type boolean Default true Environment <code>EXCLUDE_MERGE_COMMITS</code> <pre><code>export EXCLUDE_MERGE_COMMITS=false\n</code></pre>"},{"location":"reference/config-options/#exclude_bot_commits","title":"EXCLUDE_BOT_COMMITS","text":"<p>Skip commits from bot accounts.</p> Property Value Type boolean Default true Environment <code>EXCLUDE_BOT_COMMITS</code> <pre><code>export EXCLUDE_BOT_COMMITS=false\n</code></pre>"},{"location":"reference/config-options/#min_message_length","title":"MIN_MESSAGE_LENGTH","text":"<p>Minimum commit message length to include.</p> Property Value Type integer Default 10 Environment <code>MIN_MESSAGE_LENGTH</code> <pre><code>export MIN_MESSAGE_LENGTH=20\n</code></pre>"},{"location":"reference/config-options/#analysis-options","title":"Analysis Options","text":""},{"location":"reference/config-options/#enable_diff_analysis","title":"ENABLE_DIFF_ANALYSIS","text":"<p>Enable code diff analysis for pattern detection.</p> Property Value Type boolean Default false Environment <code>ENABLE_DIFF_ANALYSIS</code> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(enable_diff_analysis=True)\n</code></pre>"},{"location":"reference/config-options/#batch_size","title":"BATCH_SIZE","text":"<p>Number of commits to process per batch.</p> Property Value Type integer Default 10 Environment <code>BATCH_SIZE</code> <pre><code>from greenmining.services.data_analyzer import DataAnalyzer\n\nanalyzer = DataAnalyzer()\nanalyzer.batch_size = 50\n</code></pre>"},{"location":"reference/config-options/#pydriller-options","title":"PyDriller Options","text":""},{"location":"reference/config-options/#process_metrics_enabled","title":"PROCESS_METRICS_ENABLED","text":"<p>Compute process metrics (code churn, change set, etc.).</p> Property Value Type boolean Default true Environment <code>PROCESS_METRICS_ENABLED</code> <pre><code>export PROCESS_METRICS_ENABLED=true\n</code></pre>"},{"location":"reference/config-options/#structural_metrics_enabled","title":"STRUCTURAL_METRICS_ENABLED","text":"<p>Compute structural metrics (complexity, lines of code).</p> Property Value Type boolean Default true Environment <code>STRUCTURAL_METRICS_ENABLED</code> <pre><code>export STRUCTURAL_METRICS_ENABLED=true\n</code></pre>"},{"location":"reference/config-options/#dmm_enabled","title":"DMM_ENABLED","text":"<p>Enable Delta Maintainability Model metrics.</p> Property Value Type boolean Default true Environment <code>DMM_ENABLED</code> <pre><code>export DMM_ENABLED=true\n</code></pre>"},{"location":"reference/config-options/#statistical-analysis-options","title":"Statistical Analysis Options","text":""},{"location":"reference/config-options/#enable_stats","title":"ENABLE_STATS","text":"<p>Enable statistical analysis (correlations, effect sizes).</p> Property Value Type boolean Default false Environment <code>ENABLE_STATS</code> <pre><code>from greenmining.services.data_aggregator import DataAggregator\n\naggregator = DataAggregator(enable_stats=True)\n</code></pre>"},{"location":"reference/config-options/#enable_temporal","title":"ENABLE_TEMPORAL","text":"<p>Enable temporal trend analysis.</p> Property Value Type boolean Default false Environment <code>ENABLE_TEMPORAL</code> <pre><code>aggregator = DataAggregator(enable_temporal=True)\n</code></pre>"},{"location":"reference/config-options/#temporal_granularity","title":"TEMPORAL_GRANULARITY","text":"<p>Time period granularity for temporal analysis.</p> Property Value Type string Default quarter Options day, week, month, quarter, year Environment <code>TEMPORAL_GRANULARITY</code> <pre><code>aggregator = DataAggregator(temporal_granularity=\"month\")\n</code></pre>"},{"location":"reference/config-options/#energy-measurement-options","title":"Energy Measurement Options","text":""},{"location":"reference/config-options/#energy_enabled","title":"ENERGY_ENABLED","text":"<p>Enable energy measurement during analysis.</p> Property Value Type boolean Default false Environment <code>ENERGY_ENABLED</code> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    energy_enabled=True,\n    energy_backend=\"rapl\"\n)\n</code></pre>"},{"location":"reference/config-options/#energy_backend","title":"ENERGY_BACKEND","text":"<p>Energy measurement backend to use.</p> Property Value Type string Default rapl Options rapl, codecarbon Environment <code>ENERGY_BACKEND</code> <pre><code>analyzer = LocalRepoAnalyzer(\n    energy_enabled=True,\n    energy_backend=\"codecarbon\"\n)\n</code></pre>"},{"location":"reference/config-options/#carbon_tracking","title":"CARBON_TRACKING","text":"<p>Enable carbon emissions tracking (CodeCarbon only).</p> Property Value Type boolean Default false Environment <code>CARBON_TRACKING</code> <pre><code>export CARBON_TRACKING=true\n</code></pre>"},{"location":"reference/config-options/#country_iso","title":"COUNTRY_ISO","text":"<p>ISO country code for carbon intensity calculations.</p> Property Value Type string Default USA Environment <code>COUNTRY_ISO</code> <pre><code>export COUNTRY_ISO=DEU  # Germany\n</code></pre>"},{"location":"reference/config-options/#output-options","title":"Output Options","text":""},{"location":"reference/config-options/#output_dir","title":"OUTPUT_DIR","text":"<p>Directory for output files.</p> Property Value Type string (path) Default ./data Environment <code>OUTPUT_DIR</code> <pre><code>export OUTPUT_DIR=/var/results/greenmining\n</code></pre>"},{"location":"reference/config-options/#repos_file","title":"REPOS_FILE","text":"<p>Filename for repository data.</p> Property Value Type string Default repositories.json Environment <code>REPOS_FILE</code>"},{"location":"reference/config-options/#commits_file","title":"COMMITS_FILE","text":"<p>Filename for commit data.</p> Property Value Type string Default commits.json Environment <code>COMMITS_FILE</code>"},{"location":"reference/config-options/#analysis_file","title":"ANALYSIS_FILE","text":"<p>Filename for analysis results.</p> Property Value Type string Default analysis_results.json Environment <code>ANALYSIS_FILE</code>"},{"location":"reference/config-options/#stats_file","title":"STATS_FILE","text":"<p>Filename for aggregated statistics.</p> Property Value Type string Default aggregated_statistics.json Environment <code>STATS_FILE</code>"},{"location":"reference/config-options/#report_file","title":"REPORT_FILE","text":"<p>Filename for Markdown report.</p> Property Value Type string Default green_analysis.md Environment <code>REPORT_FILE</code>"},{"location":"reference/config-options/#sample-env-file","title":"Sample .env File","text":"<p>Complete <code>.env</code> configuration:</p> <pre><code># Required\nGITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n\n# Repository Fetching\nMAX_REPOS=100\nMIN_STARS=100\nSUPPORTED_LANGUAGES=Python,Java,Go,JavaScript,TypeScript\nSEARCH_KEYWORDS=microservices\n\n# URL Analysis\nCLONE_PATH=/tmp/greenmining_repos\nCLEANUP_AFTER_ANALYSIS=true\n\n# Commit Extraction\nCOMMITS_PER_REPO=1000\nEXCLUDE_MERGE_COMMITS=true\nEXCLUDE_BOT_COMMITS=true\nMIN_MESSAGE_LENGTH=10\n\n# Analysis\nENABLE_DIFF_ANALYSIS=false\nBATCH_SIZE=10\n\n# PyDriller\nPROCESS_METRICS_ENABLED=true\nSTRUCTURAL_METRICS_ENABLED=true\nDMM_ENABLED=true\n\n# Statistical Analysis\nENABLE_STATS=true\nENABLE_TEMPORAL=true\nTEMPORAL_GRANULARITY=quarter\n\n# Energy Measurement\nENERGY_ENABLED=false\nENERGY_BACKEND=rapl\nCARBON_TRACKING=false\nCOUNTRY_ISO=USA\n\n# Output\nOUTPUT_DIR=./data\nREPORT_FILE=green_analysis.md\n</code></pre>"},{"location":"reference/config-options/#python-config-class","title":"Python Config Class","text":"<pre><code>from greenmining.config import Config\n\nconfig = Config()\n\n# Access all options\nprint(f\"MAX_REPOS: {config.MAX_REPOS}\")\nprint(f\"COMMITS_PER_REPO: {config.COMMITS_PER_REPO}\")\nprint(f\"SUPPORTED_LANGUAGES: {config.SUPPORTED_LANGUAGES}\")\nprint(f\"CLONE_PATH: {config.CLONE_PATH}\")\nprint(f\"ENERGY_ENABLED: {config.ENERGY_ENABLED}\")\nprint(f\"ENERGY_BACKEND: {config.ENERGY_BACKEND}\")\n\n# Override at runtime\nconfig.MAX_REPOS = 50\nconfig.ENABLE_TEMPORAL = True\n</code></pre>"},{"location":"reference/config-options/#next-steps","title":"Next Steps","text":"<ul> <li>GSF Patterns - Pattern reference</li> <li>Python API - Programmatic configuration</li> <li>Energy Measurement - Energy tracking guide</li> </ul>"},{"location":"reference/models/","title":"Data Models Reference","text":"<p>Reference for GreenMining data models and structures.</p>"},{"location":"reference/models/#core-models","title":"Core Models","text":""},{"location":"reference/models/#repository","title":"Repository","text":"<p>Represents a GitHub repository.</p> <pre><code>from greenmining.models import Repository\n\nrepo = Repository(\n    repo_id=12345,\n    name=\"flask\",\n    full_name=\"pallets/flask\",\n    owner=\"pallets\",\n    url=\"https://github.com/pallets/flask\",\n    clone_url=\"https://github.com/pallets/flask.git\",\n    stars=65000,\n    forks=16000,\n    watchers=2500,\n    open_issues=50,\n    language=\"Python\",\n    last_updated=\"2024-01-15T10:00:00Z\",\n    created_at=\"2010-04-06T12:00:00Z\",\n    description=\"The Python micro framework for building web applications.\",\n    main_branch=\"main\"\n)\n</code></pre>"},{"location":"reference/models/#fields","title":"Fields","text":"Field Type Description <code>repo_id</code> int GitHub repository ID <code>name</code> str Repository name <code>full_name</code> str Full name (owner/repo) <code>owner</code> str Repository owner <code>url</code> str GitHub URL <code>clone_url</code> str Git clone URL <code>stars</code> int Star count <code>forks</code> int Fork count <code>watchers</code> int Watcher count <code>open_issues</code> int Open issue count <code>language</code> str Primary language <code>last_updated</code> str Last update timestamp <code>created_at</code> str Creation timestamp <code>description</code> str Repository description <code>main_branch</code> str Default branch name"},{"location":"reference/models/#commit","title":"Commit","text":"<p>Represents a Git commit.</p> <pre><code>from greenmining.models import Commit\n\ncommit = Commit(\n    sha=\"abc123def456789\",\n    message=\"Implement Redis caching for user sessions\",\n    author=\"developer\",\n    date=\"2024-01-15T10:30:00Z\",\n    repository=\"pallets/flask\"\n)\n</code></pre>"},{"location":"reference/models/#fields_1","title":"Fields","text":"Field Type Description <code>sha</code> str Commit SHA hash <code>message</code> str Commit message <code>author</code> str Author username <code>date</code> str Commit timestamp (ISO 8601) <code>repository</code> str Repository full name"},{"location":"reference/models/#analysisresult","title":"AnalysisResult","text":"<p>Represents the analysis result for a commit.</p> <pre><code>from greenmining.models import AnalysisResult\n\nresult = AnalysisResult(\n    commit=commit,\n    green_aware=True,\n    patterns=[\"Cache Static Data\"],\n    confidence=0.85\n)\n</code></pre>"},{"location":"reference/models/#fields_2","title":"Fields","text":"Field Type Description <code>commit</code> Commit The analyzed commit <code>green_aware</code> bool Whether commit is green-aware <code>patterns</code> list[str] Matched pattern names <code>confidence</code> float Detection confidence (0-1)"},{"location":"reference/models/#analysis-output-structures","title":"Analysis Output Structures","text":""},{"location":"reference/models/#commit-analysis-dictionary","title":"Commit Analysis Dictionary","text":"<p>When commits are analyzed, they're returned as dictionaries:</p> <pre><code>{\n    \"sha\": \"abc123def456789\",\n    \"message\": \"Implement Redis caching for user sessions\",\n    \"author\": \"developer\",\n    \"date\": \"2024-01-15T10:30:00Z\",\n    \"repository\": \"pallets/flask\",\n    \"green_aware\": True,\n    \"patterns\": [\"Cache Static Data\"],\n    \"category\": \"caching\",\n    \"keywords_matched\": [\"redis\", \"caching\"],\n    \"confidence\": 0.85,\n\n    # If diff analysis enabled\n    \"diff_patterns\": [\"caching\"],\n    \"files_modified\": 3,\n\n    # If PyDriller metrics enabled\n    \"insertions\": 45,\n    \"deletions\": 12,\n    \"dmm_unit_size\": 0.85,\n    \"dmm_unit_complexity\": 0.72,\n    \"dmm_unit_interfacing\": 0.90\n}\n</code></pre>"},{"location":"reference/models/#aggregated-statistics-structure","title":"Aggregated Statistics Structure","text":"<p>Output from <code>DataAggregator.aggregate()</code>:</p> <pre><code>{\n    \"summary\": {\n        \"total_commits\": 5000,\n        \"green_aware_count\": 1250,\n        \"green_aware_percentage\": 25.0,\n        \"total_repos\": 50,\n        \"repos_with_green_commits\": 42,\n        \"analysis_date\": \"2024-01-15T10:00:00Z\"\n    },\n\n    \"pattern_distribution\": {\n        \"Cache Static Data\": 320,\n        \"Use Async Instead of Sync\": 180,\n        \"Compress Transmitted Data\": 150,\n        \"Lazy Loading\": 120,\n        \"Optimize Database Queries\": 95\n    },\n\n    \"category_distribution\": {\n        \"cloud\": 450,\n        \"caching\": 320,\n        \"async\": 210,\n        \"web\": 180,\n        \"database\": 95\n    },\n\n    \"per_repo_stats\": [\n        {\n            \"repository\": \"pallets/flask\",\n            \"total_commits\": 200,\n            \"green_aware_count\": 47,\n            \"green_aware_percentage\": 23.5,\n            \"top_patterns\": [\"Cache Static Data\", \"Lazy Loading\"]\n        }\n    ],\n\n    \"per_language_stats\": {\n        \"Python\": {\n            \"total_commits\": 2000,\n            \"green_aware_count\": 520,\n            \"green_aware_percentage\": 26.0\n        }\n    },\n\n    # If enable_temporal=True\n    \"temporal_analysis\": {\n        \"periods\": [\n            {\n                \"period\": \"2024-Q1\",\n                \"commit_count\": 500,\n                \"green_count\": 125,\n                \"green_awareness_rate\": 0.25\n            }\n        ],\n        \"overall_trend\": {\n            \"direction\": \"increasing\",\n            \"significant\": True\n        }\n    },\n\n    # If enable_stats=True\n    \"statistics\": {\n        \"pattern_correlations\": {\n            \"top_positive_correlations\": [\n                {\n                    \"pattern1\": \"caching\",\n                    \"pattern2\": \"performance\",\n                    \"correlation\": 0.75\n                }\n            ]\n        },\n        \"effect_size\": {\n            \"green_vs_nongreen_patterns\": {\n                \"cohens_d\": 0.65,\n                \"magnitude\": \"medium\"\n            }\n        },\n        \"descriptive\": {\n            \"patterns_per_commit\": {\n                \"mean\": 2.3,\n                \"median\": 2.0,\n                \"std\": 1.1\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"reference/models/#url-analysis-result-structure","title":"URL Analysis Result Structure","text":"<p>Output from <code>LocalRepoAnalyzer.analyze_repository()</code>:</p> <pre><code>{\n    \"repository\": {\n        \"name\": \"flask\",\n        \"url\": \"https://github.com/pallets/flask\",\n        \"owner\": \"pallets\",\n        \"clone_path\": \"/tmp/greenmining_repos/flask\"\n    },\n\n    \"total_commits\": 200,\n    \"green_aware_count\": 47,\n    \"green_aware_percentage\": 23.5,\n\n    \"commits\": [\n        {\n            \"sha\": \"abc123...\",\n            \"message\": \"Optimize template caching\",\n            \"author\": \"developer\",\n            \"date\": \"2024-03-15T10:30:00\",\n            \"green_aware\": True,\n            \"patterns\": [\"Cache Static Data\"],\n\n            # PyDriller metrics\n            \"modified_files\": 3,\n            \"insertions\": 45,\n            \"deletions\": 12,\n            \"files\": [\"app.py\", \"cache.py\", \"config.py\"],\n\n            # DMM metrics\n            \"dmm_unit_size\": 0.85,\n            \"dmm_unit_complexity\": 0.72,\n            \"dmm_unit_interfacing\": 0.90\n        }\n    ],\n\n    \"pattern_distribution\": {\n        \"Cache Static Data\": 15,\n        \"Use Async Instead of Sync\": 12,\n        \"Lazy Loading\": 8\n    },\n\n    \"process_metrics\": {\n        \"change_set\": {\n            \"max\": 25,\n            \"avg\": 5.2\n        },\n        \"code_churn\": {\n            \"added\": 5000,\n            \"removed\": 2000\n        },\n        \"contributors_count\": 45,\n        \"commits_count\": 200\n    }\n}\n</code></pre>"},{"location":"reference/models/#energy-metrics-structures","title":"Energy Metrics Structures","text":""},{"location":"reference/models/#energymetrics","title":"EnergyMetrics","text":"<pre><code>from greenmining.energy.base import EnergyMetrics\n\n@dataclass\nclass EnergyMetrics:\n    energy_joules: float       # Total energy consumed\n    duration_seconds: float    # Measurement duration\n    average_power_watts: float # Average power draw\n    start_time: datetime       # Start timestamp\n    end_time: datetime         # End timestamp\n\n    # CodeCarbon specific\n    energy_kwh: float = 0.0    # Energy in kWh\n    emissions_kg: float = 0.0  # CO2 emissions\n</code></pre>"},{"location":"reference/models/#commitenergyprofile","title":"CommitEnergyProfile","text":"<pre><code>from greenmining.energy.base import CommitEnergyProfile\n\n@dataclass\nclass CommitEnergyProfile:\n    commit_sha: str            # Commit identifier\n    energy_joules: float       # Energy for this commit\n    duration_seconds: float    # Analysis duration\n    patterns_detected: list    # Patterns found\n    files_analyzed: int        # Files in commit\n</code></pre>"},{"location":"reference/models/#pattern-structure","title":"Pattern Structure","text":"<p>GSF patterns are stored as dictionaries:</p> <pre><code>{\n    \"cache_static_data\": {\n        \"name\": \"Cache Static Data\",\n        \"category\": \"cloud\",\n        \"keywords\": [\"cache\", \"caching\", \"static\", \"cdn\", \"redis\", \"memcache\"],\n        \"description\": \"Cache static content to reduce server load and network transfers\",\n        \"sci_impact\": \"Reduces energy by minimizing redundant compute and network operations\"\n    }\n}\n</code></pre>"},{"location":"reference/models/#fields_3","title":"Fields","text":"Field Type Description <code>name</code> str Human-readable pattern name <code>category</code> str Pattern category <code>keywords</code> list[str] Detection keywords <code>description</code> str Pattern description <code>sci_impact</code> str Impact on software carbon intensity"},{"location":"reference/models/#working-with-models","title":"Working with Models","text":""},{"location":"reference/models/#serialization","title":"Serialization","text":"<pre><code>import json\n\n# Convert to JSON\nresult_json = json.dumps(analysis_result, default=str)\n\n# Load from JSON\nwith open(\"results.json\") as f:\n    results = json.load(f)\n</code></pre>"},{"location":"reference/models/#filtering-results","title":"Filtering Results","text":"<pre><code># Get only green-aware commits\ngreen_commits = [c for c in commits if c[\"green_aware\"]]\n\n# Group by pattern\nfrom collections import defaultdict\nby_pattern = defaultdict(list)\nfor commit in green_commits:\n    for pattern in commit[\"patterns\"]:\n        by_pattern[pattern].append(commit)\n\n# Get top patterns\ntop_patterns = sorted(\n    by_pattern.items(), \n    key=lambda x: len(x[1]), \n    reverse=True\n)[:10]\n</code></pre>"},{"location":"reference/models/#aggregating-custom-metrics","title":"Aggregating Custom Metrics","text":"<pre><code>import statistics\n\n# Calculate custom statistics\ngreen_counts = [r[\"green_aware_count\"] for r in per_repo_stats]\navg_green = statistics.mean(green_counts)\nmedian_green = statistics.median(green_counts)\nstd_green = statistics.stdev(green_counts) if len(green_counts) &gt; 1 else 0\n</code></pre>"},{"location":"reference/models/#next-steps","title":"Next Steps","text":"<ul> <li>Python API - Working with models</li> <li>GSF Patterns - Pattern reference</li> <li>Configuration - All options</li> </ul>"},{"location":"reference/patterns/","title":"GSF Patterns Reference","text":"<p>Complete reference for all 122 Green Software Foundation patterns supported by GreenMining.</p>"},{"location":"reference/patterns/#overview","title":"Overview","text":"<p>GreenMining detects patterns from the Green Software Foundation catalog, organized into 15 categories.</p> Statistic Value Total Patterns 122 Categories 15 Keywords 321"},{"location":"reference/patterns/#pattern-categories","title":"Pattern Categories","text":""},{"location":"reference/patterns/#cloud-patterns-40","title":"Cloud Patterns (40+)","text":"<p>Patterns for cloud-native and infrastructure optimization.</p> Pattern Keywords Description Cache Static Data cache, caching, static, cdn, redis, memcache Cache static content to reduce server load Choose Region Closest region, closest, proximity, latency Deploy in regions closest to users Compress Stored Data compress, storage, gzip, zstd Compress data at rest Compress Transmitted Data compress, transmission, gzip, brotli Compress data before network transfer Containerize Workload container, docker, kubernetes, pod Use containers for resource efficiency Delete Unused Storage delete, remove, unused, cleanup Remove unused storage resources Encrypt What Is Necessary encrypt, tls, ssl, crypto Only encrypt data that needs protection Evaluate CPU Architectures cpu, arm, graviton, processor Consider ARM and efficient CPUs Use Service Mesh service mesh, istio, linkerd, envoy Optimize service-to-service communication TLS Termination tls termination, ssl offload Terminate TLS at edge Implement Stateless Design stateless, session, horizontal Design without server-side state Match SLO Requirements slo, sla, service level Don't over-engineer beyond SLO Match VM Utilization vm, instance, size, utilization Right-size VMs to workload Move to Cloud cloud, migrate, migration Leverage cloud efficiency Optimize Average Utilization utilization, optimize, average Increase resource utilization Optimize High Utilization high utilization, maximize Target high utilization levels Optimize Network Traffic network, traffic, optimize Reduce unnecessary network traffic Scale Down Idle Resources scale down, idle, shutdown Reduce resources when idle Scale Infrastructure scaling, autoscaling, scale Scale based on demand Terminate Unused Resources terminate, unused, cleanup Remove unused resources Use Reserved Instances reserved, spot, savings Use cost-efficient instance types Use Serverless serverless, lambda, functions Use serverless for variable workloads Use Spot Instances spot, preemptible, interruption Use spot instances for cost savings"},{"location":"reference/patterns/#web-patterns-15","title":"Web Patterns (15+)","text":"<p>Patterns for web application optimization.</p> Pattern Keywords Description Enable Text Compression gzip, brotli, deflate Compress text responses Lazy Loading lazy, defer, on-demand Load content only when needed Minify CSS/JS minify, minification, uglify Reduce asset file sizes Optimize Images image, webp, avif, srcset Use modern image formats Use CDN cdn, content delivery, edge Serve from edge locations Cache HTTP Responses cache-control, etag, expires Enable browser caching Reduce DOM Size dom, elements, optimize Minimize DOM complexity Use Service Workers service worker, pwa, offline Enable offline caching Preconnect Resources preconnect, preload, prefetch Hint browser to connect early Remove Unused CSS unused, purge, tree-shake Remove dead CSS code Optimize Fonts font, woff2, subset Use efficient font loading Reduce JavaScript javascript, bundle, split Minimize JS payload Use HTTP/2 http2, multiplexing Use modern HTTP protocol Enable Keep-Alive keep-alive, persistent Reuse HTTP connections"},{"location":"reference/patterns/#aiml-patterns-10","title":"AI/ML Patterns (10+)","text":"<p>Patterns for machine learning optimization.</p> Pattern Keywords Description Model Optimization model, optimize, prune Optimize model architecture Quantization quantize, int8, fp16 Reduce model precision Knowledge Distillation distillation, student, teacher Train smaller models Efficient Training training, efficient, epoch Optimize training process Batch Inference batch, inference, throughput Process predictions in batches Model Caching model cache, warm, preload Cache loaded models Feature Selection feature, select, reduce Use fewer features Early Stopping early stop, convergence Stop training when converged Mixed Precision mixed precision, amp Use mixed precision training Gradient Checkpointing checkpoint, gradient, memory Trade compute for memory"},{"location":"reference/patterns/#caching-patterns-8","title":"Caching Patterns (8)","text":"<p>Patterns for caching strategies.</p> Pattern Keywords Description Redis Caching redis, cache, memory Use Redis for caching CDN Caching cdn, edge, cache Cache at CDN edge Database Query Cache query cache, mysql, postgres Cache database queries Application Cache app cache, memory, local In-memory application cache Distributed Cache distributed, memcached Multi-node caching Cache Invalidation invalidate, ttl, expire Proper cache expiration Write-Through Cache write-through, consistency Consistent caching Cache Warming warm, preload, prefetch Pre-populate caches"},{"location":"reference/patterns/#async-patterns-6","title":"Async Patterns (6)","text":"<p>Patterns for asynchronous processing.</p> Pattern Keywords Description Queue Non-Urgent Requests queue, async, defer Queue non-critical work Use Async Instead of Sync async, await, non-blocking Prefer async operations Batch Processing batch, bulk, aggregate Process in batches Event-Driven Architecture event, pub-sub, message Use event-driven design Background Jobs background, worker, job Process in background Stream Processing stream, reactive, flow Use streaming for large data"},{"location":"reference/patterns/#database-patterns-8","title":"Database Patterns (8)","text":"<p>Patterns for database optimization.</p> Pattern Keywords Description Optimize Database Queries query, optimize, explain Improve query performance Use Connection Pooling pool, connection, reuse Reuse database connections Index Optimization index, btree, covering Optimize database indexes Read Replicas replica, read, slave Scale reads with replicas Denormalization denormalize, join, embed Reduce join operations Partition Tables partition, shard, split Split large tables Use NoSQL nosql, document, key-value Use appropriate database type Lazy Loading Relations lazy, eager, n+1 Avoid N+1 query problems"},{"location":"reference/patterns/#network-patterns-6","title":"Network Patterns (6)","text":"<p>Patterns for network optimization.</p> Pattern Keywords Description HTTP Compression gzip, compress, transfer Compress HTTP responses Reduce API Calls batch, aggregate, graphql Minimize API requests Use WebSockets websocket, socket, realtime Use persistent connections Protocol Optimization http3, quic, protocol Use efficient protocols Edge Computing edge, close, proximity Process at the edge Connection Reuse keep-alive, persist, reuse Reuse network connections"},{"location":"reference/patterns/#resource-patterns-5","title":"Resource Patterns (5)","text":"<p>Patterns for resource management.</p> Pattern Keywords Description Memory Optimization memory, heap, gc Optimize memory usage CPU Optimization cpu, thread, parallel Optimize CPU usage I/O Optimization io, disk, buffer Optimize I/O operations Resource Pooling pool, reuse, recycle Pool expensive resources Garbage Collection Tuning gc, tuning, generational Tune GC parameters"},{"location":"reference/patterns/#code-patterns-4","title":"Code Patterns (4)","text":"<p>Patterns for code-level optimization.</p> Pattern Keywords Description Remove Dead Code dead code, unused, remove Eliminate unused code Algorithm Optimization algorithm, complexity, O(n) Use efficient algorithms Loop Optimization loop, iteration, vectorize Optimize loops Avoid Premature Optimization premature, profile, measure Profile before optimizing"},{"location":"reference/patterns/#infrastructure-patterns-4","title":"Infrastructure Patterns (4)","text":"<p>Patterns for infrastructure optimization.</p> Pattern Keywords Description Alpine Containers alpine, minimal, scratch Use minimal base images Infrastructure as Code iac, terraform, ansible Manage infrastructure as code Renewable Energy Regions renewable, green, carbon Use green energy regions Container Optimization container, layer, cache Optimize container builds"},{"location":"reference/patterns/#microservices-patterns-4","title":"Microservices Patterns (4)","text":"<p>Patterns for microservices architecture.</p> Pattern Keywords Description Service Decomposition decompose, microservice, split Right-size services Colocation Strategies colocate, affinity, proximity Place related services together Graceful Shutdown graceful, shutdown, sigterm Handle shutdown properly Service Mesh Optimization mesh, sidecar, istio Optimize service mesh overhead"},{"location":"reference/patterns/#monitoring-patterns-3","title":"Monitoring Patterns (3)","text":"<p>Patterns for observability optimization.</p> Pattern Keywords Description Efficient Logging logging, log level, structured Optimize log volume Metrics Aggregation metrics, aggregate, rollup Aggregate metrics efficiently Trace Sampling sampling, trace, opentelemetry Sample traces appropriately"},{"location":"reference/patterns/#general-patterns-8","title":"General Patterns (8)","text":"<p>General optimization patterns.</p> Pattern Keywords Description Feature Flags feature flag, toggle, switch Use feature flags Incremental Processing incremental, delta, diff Process only changes Precomputation precompute, materialize, cache Precompute expensive results Background Jobs background, async, worker Process in background Rate Limiting rate limit, throttle, backoff Limit request rates Circuit Breaker circuit, breaker, fallback Fail fast with fallbacks Retry with Backoff retry, backoff, exponential Retry with exponential backoff Timeout Configuration timeout, deadline, cancel Set appropriate timeouts"},{"location":"reference/patterns/#accessing-patterns-programmatically","title":"Accessing Patterns Programmatically","text":"<pre><code>from greenmining import GSF_PATTERNS\n\n# Count patterns\nprint(f\"Total patterns: {len(GSF_PATTERNS)}\")  # 122\n\n# Get all categories\ncategories = set(p[\"category\"] for p in GSF_PATTERNS.values())\nprint(f\"Categories: {sorted(categories)}\")\n\n# Find patterns by category\ncloud_patterns = [\n    p for p in GSF_PATTERNS.values() \n    if p[\"category\"] == \"cloud\"\n]\nprint(f\"Cloud patterns: {len(cloud_patterns)}\")\n\n# Get pattern details\npattern = GSF_PATTERNS[\"cache_static_data\"]\nprint(f\"Name: {pattern['name']}\")\nprint(f\"Category: {pattern['category']}\")\nprint(f\"Keywords: {pattern['keywords']}\")\nprint(f\"Description: {pattern['description']}\")\nprint(f\"SCI Impact: {pattern['sci_impact']}\")\n</code></pre>"},{"location":"reference/patterns/#green-keywords","title":"Green Keywords","text":"<p>The 321 green keywords used for detection:</p> <pre><code>from greenmining import GREEN_KEYWORDS\n\n# Categories of keywords\nkeyword_categories = {\n    \"energy\": [\"energy\", \"power\", \"watt\", \"joule\", \"consumption\"],\n    \"carbon\": [\"carbon\", \"emission\", \"co2\", \"greenhouse\", \"footprint\"],\n    \"efficiency\": [\"efficient\", \"efficiency\", \"optimize\", \"reduce\", \"minimize\"],\n    \"sustainability\": [\"sustainable\", \"green\", \"eco\", \"environmental\"],\n    \"performance\": [\"performance\", \"fast\", \"speed\", \"latency\", \"throughput\"],\n    \"resource\": [\"resource\", \"memory\", \"cpu\", \"disk\", \"network\"],\n    \"caching\": [\"cache\", \"cached\", \"caching\", \"redis\", \"memcache\"],\n    \"compression\": [\"compress\", \"gzip\", \"brotli\", \"minify\", \"compact\"],\n}\n\n# Sample keywords\nprint(GREEN_KEYWORDS[:20])\n# ['energy', 'power', 'carbon', 'emission', 'footprint', 'sustainability', ...]\n</code></pre>"},{"location":"reference/patterns/#pattern-detection-example","title":"Pattern Detection Example","text":"<pre><code>from greenmining import is_green_aware, get_pattern_by_keywords\n\n# Test messages\nmessages = [\n    \"Implement Redis caching for user sessions\",\n    \"Enable gzip compression on API responses\",\n    \"Migrate to serverless Lambda functions\",\n    \"Optimize database queries with proper indexing\",\n    \"Add lazy loading for images\",\n    \"Fix typo in documentation\",\n]\n\nfor msg in messages:\n    is_green = is_green_aware(msg)\n    patterns = get_pattern_by_keywords(msg) if is_green else []\n\n    if is_green:\n        print(f\"\ud83c\udf31 {msg}\")\n        print(f\"   Patterns: {patterns}\")\n    else:\n        print(f\"   {msg}\")\n</code></pre> <p>Output:</p> <pre><code>\ud83c\udf31 Implement Redis caching for user sessions\n   Patterns: ['Cache Static Data']\n\ud83c\udf31 Enable gzip compression on API responses\n   Patterns: ['Compress Transmitted Data', 'Enable Text Compression']\n\ud83c\udf31 Migrate to serverless Lambda functions\n   Patterns: ['Use Serverless']\n\ud83c\udf31 Optimize database queries with proper indexing\n   Patterns: ['Optimize Database Queries', 'Index Optimization']\n\ud83c\udf31 Add lazy loading for images\n   Patterns: ['Lazy Loading', 'Optimize Images']\n   Fix typo in documentation\n</code></pre>"},{"location":"reference/patterns/#contributing-patterns","title":"Contributing Patterns","text":"<p>To suggest new patterns or improvements:</p> <ol> <li>Check the GSF Patterns Catalog</li> <li>Open an issue on GitHub</li> <li>Submit a pull request with pattern additions</li> </ol>"},{"location":"reference/patterns/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Options - All configuration parameters</li> <li>Python API - Programmatic usage</li> <li>Data Models - Repository and Commit models</li> </ul>"},{"location":"user-guide/api/","title":"Python API Reference","text":"<p>Use GreenMining programmatically in your Python scripts.</p>"},{"location":"user-guide/api/#quick-import","title":"Quick Import","text":"<pre><code>from greenmining import (\n    GSF_PATTERNS,        # Dict of 122 GSF patterns\n    GREEN_KEYWORDS,      # List of 321 green keywords\n    is_green_aware,      # Check if message is green-aware\n    get_pattern_by_keywords,  # Get matched patterns\n    fetch_repositories,  # Fetch repos from GitHub\n    Config,              # Configuration class\n)\n</code></pre>"},{"location":"user-guide/api/#core-functions","title":"Core Functions","text":""},{"location":"user-guide/api/#is_green_aware","title":"is_green_aware()","text":"<p>Check if a commit message indicates green software awareness.</p> <pre><code>def is_green_aware(commit_message: str) -&gt; bool\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>commit_message</code> str The commit message to analyze <p>Returns: <code>bool</code> - True if message contains green keywords</p> <p>Example:</p> <pre><code>from greenmining import is_green_aware\n\n# Returns True\nis_green_aware(\"Optimize Redis caching for better performance\")\nis_green_aware(\"Enable gzip compression on API responses\")\nis_green_aware(\"Implement async batch processing\")\n\n# Returns False\nis_green_aware(\"Fix typo in README\")\nis_green_aware(\"Update dependencies\")\nis_green_aware(\"Refactor variable names\")\n</code></pre>"},{"location":"user-guide/api/#get_pattern_by_keywords","title":"get_pattern_by_keywords()","text":"<p>Find GSF patterns that match a commit message.</p> <pre><code>def get_pattern_by_keywords(commit_message: str) -&gt; list[str]\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>commit_message</code> str The commit message to analyze <p>Returns: <code>list[str]</code> - List of matched pattern names</p> <p>Example:</p> <pre><code>from greenmining import get_pattern_by_keywords\n\npatterns = get_pattern_by_keywords(\"Implement Redis caching layer\")\nprint(patterns)\n# Output: ['Cache Static Data']\n\npatterns = get_pattern_by_keywords(\"Enable gzip compression for API responses\")\nprint(patterns)\n# Output: ['Compress Transmitted Data', 'Enable Text Compression']\n\npatterns = get_pattern_by_keywords(\"Fix typo\")\nprint(patterns)\n# Output: []\n</code></pre>"},{"location":"user-guide/api/#fetch_repositories","title":"fetch_repositories()","text":"<p>Fetch repositories from GitHub matching search criteria.</p> <pre><code>def fetch_repositories(\n    github_token: str,\n    max_repos: int = 100,\n    min_stars: int = 100,\n    languages: list = None,\n    keywords: str = \"microservices\"\n) -&gt; list\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>github_token</code> str (required) GitHub personal access token <code>max_repos</code> int 100 Maximum repositories to fetch <code>min_stars</code> int 100 Minimum star count <code>languages</code> list None Filter by languages <code>keywords</code> str \"microservices\" Search keywords <p>Returns: <code>list</code> - List of Repository objects</p> <p>Example:</p> <pre><code>from greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=\"ghp_xxxx\",\n    max_repos=10,\n    min_stars=500,\n    keywords=\"kubernetes\",\n    languages=[\"Python\", \"Go\"]\n)\n\nfor repo in repos:\n    print(f\"{repo.full_name}: {repo.stars} stars\")\n</code></pre>"},{"location":"user-guide/api/#data-structures","title":"Data Structures","text":""},{"location":"user-guide/api/#gsf_patterns","title":"GSF_PATTERNS","text":"<p>Dictionary containing all 122 Green Software Foundation patterns.</p> <pre><code>from greenmining import GSF_PATTERNS\n\n# Structure\nGSF_PATTERNS = {\n    \"pattern_id\": {\n        \"name\": \"Pattern Name\",\n        \"category\": \"category_name\",\n        \"keywords\": [\"keyword1\", \"keyword2\"],\n        \"description\": \"Pattern description\",\n        \"sci_impact\": \"Impact on software carbon intensity\"\n    },\n    ...\n}\n</code></pre> <p>Example Usage:</p> <pre><code>from greenmining import GSF_PATTERNS\n\n# Get pattern count\nprint(f\"Total patterns: {len(GSF_PATTERNS)}\")  # 122\n\n# Get all categories\ncategories = set(p[\"category\"] for p in GSF_PATTERNS.values())\nprint(f\"Categories: {categories}\")\n# {'cloud', 'web', 'ai', 'caching', 'async', 'database', ...}\n\n# Find patterns by category\ncloud_patterns = [\n    p[\"name\"] for p in GSF_PATTERNS.values() \n    if p[\"category\"] == \"cloud\"\n]\nprint(f\"Cloud patterns: {len(cloud_patterns)}\")  # 40+\n\n# Get pattern details\ncache_pattern = GSF_PATTERNS[\"cache_static_data\"]\nprint(f\"Name: {cache_pattern['name']}\")\nprint(f\"Keywords: {cache_pattern['keywords']}\")\n</code></pre>"},{"location":"user-guide/api/#green_keywords","title":"GREEN_KEYWORDS","text":"<p>List of 321 keywords indicating green software practices.</p> <pre><code>from greenmining import GREEN_KEYWORDS\n\nprint(f\"Total keywords: {len(GREEN_KEYWORDS)}\")  # 321\n\n# Sample keywords\nprint(GREEN_KEYWORDS[:10])\n# ['energy', 'power', 'carbon', 'emission', 'footprint', \n#  'sustainability', 'sustainable', 'green', 'efficient', 'efficiency']\n</code></pre>"},{"location":"user-guide/api/#service-classes","title":"Service Classes","text":""},{"location":"user-guide/api/#dataanalyzer","title":"DataAnalyzer","text":"<p>Analyze commits for green software patterns.</p> <pre><code>from greenmining.services.data_analyzer import DataAnalyzer\n\nanalyzer = DataAnalyzer(\n    enable_diff_analysis=False,  # Analyze code diffs\n    patterns=None,               # Custom patterns (default: GSF_PATTERNS)\n    batch_size=10                # Commits per batch\n)\n</code></pre> <p>Methods:</p> <pre><code># Analyze a single commit\nresult = analyzer.analyze_commit(commit_dict)\n\n# Analyze multiple commits\nresults = analyzer.analyze_commits(commits_list)\n\n# Save results to file\nanalyzer.save_results(results, \"output.json\")\n</code></pre> <p>Example:</p> <pre><code>from greenmining.services.data_analyzer import DataAnalyzer\n\nanalyzer = DataAnalyzer(enable_diff_analysis=True)\n\ncommit = {\n    \"sha\": \"abc123\",\n    \"message\": \"Implement Redis caching for user sessions\",\n    \"author\": \"developer\",\n    \"date\": \"2024-01-15T10:00:00Z\"\n}\n\nresult = analyzer.analyze_commit(commit)\nprint(f\"Green-aware: {result['green_aware']}\")\nprint(f\"Patterns: {result['patterns']}\")\n</code></pre>"},{"location":"user-guide/api/#dataaggregator","title":"DataAggregator","text":"<p>Aggregate analysis results with statistics.</p> <pre><code>from greenmining.services.data_aggregator import DataAggregator\n\naggregator = DataAggregator(\n    config=None,                  # Config object\n    enable_stats=True,            # Statistical analysis\n    enable_temporal=True,         # Temporal trends\n    temporal_granularity=\"quarter\"  # day/week/month/quarter/year\n)\n</code></pre> <p>Methods:</p> <pre><code># Aggregate results\naggregated = aggregator.aggregate(analysis_results, repositories)\n\n# Save to files\naggregator.save_results(aggregated, \"stats.json\", \"stats.csv\", analysis_results)\n\n# Print summary\naggregator.print_summary(aggregated)\n</code></pre> <p>Example:</p> <pre><code>from greenmining.services.data_aggregator import DataAggregator\n\naggregator = DataAggregator(\n    enable_stats=True,\n    enable_temporal=True,\n    temporal_granularity=\"month\"\n)\n\n# Assuming analysis_results and repositories are already populated\naggregated = aggregator.aggregate(analysis_results, repositories)\n\nprint(f\"Total commits: {aggregated['summary']['total_commits']}\")\nprint(f\"Green-aware: {aggregated['summary']['green_aware_percentage']}%\")\n</code></pre>"},{"location":"user-guide/api/#localrepoanalyzer","title":"LocalRepoAnalyzer","text":"<p>Analyze repositories directly from GitHub URLs using PyDriller.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/greenmining_repos\",  # Clone directory\n    cleanup_after=True                     # Delete after analysis\n)\n</code></pre> <p>Methods:</p> <pre><code># Analyze single repository\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/owner/repo\",\n    max_commits=1000,\n    since_date=datetime(2024, 1, 1),\n    to_date=datetime(2024, 12, 31)\n)\n</code></pre> <p>Example:</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom datetime import datetime\n\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/analysis\",\n    cleanup_after=True\n)\n\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=100\n)\n\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Commits analyzed: {result['total_commits']}\")\nprint(f\"Green-aware: {result['green_aware_count']}\")\n\n# Access individual commits\nfor commit in result['commits'][:5]:\n    if commit['green_aware']:\n        print(f\"  \ud83c\udf31 {commit['message'][:50]}...\")\n</code></pre>"},{"location":"user-guide/api/#reportgenerator","title":"ReportGenerator","text":"<p>Generate Markdown reports from analysis results.</p> <pre><code>from greenmining.services.reports import ReportGenerator\n\ngenerator = ReportGenerator()\n</code></pre> <p>Methods:</p> <pre><code># Generate full report\nreport = generator.generate_report(aggregated_data)\n\n# Save to file\ngenerator.save_report(report, \"report.md\")\n</code></pre>"},{"location":"user-guide/api/#analyzer-classes","title":"Analyzer Classes","text":""},{"location":"user-guide/api/#statisticalanalyzer","title":"StatisticalAnalyzer","text":"<p>Compute statistical metrics on analysis results.</p> <pre><code>from greenmining.analyzers.statistical_analyzer import StatisticalAnalyzer\n\nanalyzer = StatisticalAnalyzer()\n\n# Pattern correlations\ncorrelations = analyzer.analyze_pattern_correlations(analysis_results)\n\n# Effect sizes\neffect_sizes = analyzer.analyze_effect_sizes(analysis_results)\n\n# Descriptive statistics\ndescriptive = analyzer.get_descriptive_statistics(analysis_results)\n</code></pre>"},{"location":"user-guide/api/#temporalanalyzer","title":"TemporalAnalyzer","text":"<p>Analyze patterns over time.</p> <pre><code>from greenmining.analyzers.temporal_analyzer import TemporalAnalyzer\n\nanalyzer = TemporalAnalyzer(granularity=\"quarter\")\n\n# Group commits by period\nperiods = analyzer.group_commits_by_period(commits)\n\n# Analyze trends\ntrends = analyzer.analyze_trends(periods)\n\n# Pattern evolution\nevolution = analyzer.analyze_pattern_evolution(commits)\n</code></pre>"},{"location":"user-guide/api/#qualitativeanalyzer","title":"QualitativeAnalyzer","text":"<p>Generate validation samples for manual review.</p> <pre><code>from greenmining.analyzers.qualitative_analyzer import QualitativeAnalyzer\n\nanalyzer = QualitativeAnalyzer(\n    sample_size=30,\n    stratify_by=\"pattern\"  # pattern/repository/time/random\n)\n\n# Generate samples\nsamples = analyzer.generate_validation_samples(analysis_results)\n\n# Export for review\nanalyzer.export_samples_for_review(samples, \"validation_samples.csv\")\n</code></pre>"},{"location":"user-guide/api/#codediffanalyzer","title":"CodeDiffAnalyzer","text":"<p>Analyze code changes for green patterns.</p> <pre><code>from greenmining.analyzers.code_diff_analyzer import CodeDiffAnalyzer\n\nanalyzer = CodeDiffAnalyzer()\n\n# Check if file is analyzable\nis_code = analyzer.is_code_file(\"app.py\")  # True\n\n# Analyze diff content\npatterns = analyzer.detect_patterns_in_diff(diff_text)\n</code></pre>"},{"location":"user-guide/api/#configuration-class","title":"Configuration Class","text":"<pre><code>from greenmining.config import Config\n\nconfig = Config()\n\n# Access configuration values\nprint(config.MAX_REPOS)           # 100\nprint(config.COMMITS_PER_REPO)    # 1000\nprint(config.SUPPORTED_LANGUAGES) # ['Python', 'Java', ...]\nprint(config.OUTPUT_DIR)          # 'data'\n\n# URL Analysis options\nprint(config.REPOSITORY_URLS)     # []\nprint(config.CLONE_PATH)          # '/tmp/greenmining_repos'\n\n# Energy options\nprint(config.ENERGY_ENABLED)      # False\nprint(config.ENERGY_BACKEND)      # 'rapl'\n\n# PyDriller options\nprint(config.PROCESS_METRICS_ENABLED)  # True\nprint(config.DMM_ENABLED)              # True\n</code></pre>"},{"location":"user-guide/api/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Complete GreenMining analysis workflow.\"\"\"\n\nfrom greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom greenmining.services.data_aggregator import DataAggregator\nfrom greenmining.services.reports import ReportGenerator\n\n# 1. Analyze repository\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=200\n)\n\nprint(f\"Analyzed {result['total_commits']} commits\")\nprint(f\"Green-aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\n# 2. Aggregate results\naggregator = DataAggregator(enable_stats=True, enable_temporal=True)\naggregated = aggregator.aggregate(\n    analysis_results=result['commits'],\n    repositories=[result['repository']]\n)\n\n# 3. Generate report\ngenerator = ReportGenerator()\nreport = generator.generate_report(aggregated)\ngenerator.save_report(report, \"flask_analysis.md\")\n\nprint(\"Report saved to flask_analysis.md\")\n</code></pre>"},{"location":"user-guide/api/#next-steps","title":"Next Steps","text":"<ul> <li>URL Analysis - Deep dive into URL-based analysis</li> <li>Energy Measurement - Power profiling with RAPL/CodeCarbon</li> <li>GSF Patterns Reference - All 122 patterns</li> </ul>"},{"location":"user-guide/energy/","title":"Energy Measurement","text":"<p>Measure energy consumption of your analysis workloads with RAPL and CodeCarbon.</p>"},{"location":"user-guide/energy/#overview","title":"Overview","text":"<p>GreenMining includes energy measurement capabilities to profile the power consumption of analysis operations. This is useful for:</p> <ul> <li>Research - Quantify energy cost of mining operations</li> <li>Optimization - Identify energy-intensive analysis steps</li> <li>Reporting - Include energy metrics in analysis reports</li> </ul>"},{"location":"user-guide/energy/#supported-backends","title":"Supported Backends","text":"Backend Platform Features RAPL Linux (Intel/AMD) Direct CPU/DRAM power reading CodeCarbon Cross-platform Emissions tracking, cloud support CPU Energy Meter Linux Alternative to RAPL (future)"},{"location":"user-guide/energy/#rapl-backend","title":"RAPL Backend","text":"<p>Intel's Running Average Power Limit (RAPL) provides direct power measurements on Linux systems with Intel or AMD processors.</p>"},{"location":"user-guide/energy/#requirements","title":"Requirements","text":"<ul> <li>Linux operating system</li> <li>Intel Core 2nd generation+ or AMD Ryzen</li> <li>Read access to <code>/sys/class/powercap/intel-rapl/</code></li> </ul>"},{"location":"user-guide/energy/#checking-availability","title":"Checking Availability","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\nif meter.is_available():\n    print(\"RAPL is available on this system\")\nelse:\n    print(\"RAPL not available - try running as root\")\n</code></pre>"},{"location":"user-guide/energy/#basic-usage","title":"Basic Usage","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\n\n# Start measurement\nmeter.start()\n\n# Your workload here\nresult = expensive_computation()\n\n# Stop and get metrics\nmetrics = meter.stop()\n\nprint(f\"Energy consumed: {metrics.energy_joules:.2f} J\")\nprint(f\"Duration: {metrics.duration_seconds:.2f} s\")\nprint(f\"Average power: {metrics.average_power_watts:.2f} W\")\n</code></pre>"},{"location":"user-guide/energy/#with-context-manager","title":"With Context Manager","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\n\nwith meter.measure() as measurement:\n    # Your analysis code\n    analyzer.analyze_repository(repo_url)\n\nprint(f\"Analysis consumed {measurement.energy_joules:.2f} J\")\n</code></pre>"},{"location":"user-guide/energy/#permission-setup","title":"Permission Setup","text":"<p>RAPL typically requires root access. To allow non-root users:</p> <pre><code># Grant read access to RAPL files\nsudo chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\nsudo chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/intel-rapl:0:*/energy_uj\n\n# Or create a udev rule for persistent access\necho 'SUBSYSTEM==\"powercap\", ACTION==\"add\", RUN+=\"/bin/chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\"' | sudo tee /etc/udev/rules.d/99-rapl.rules\n</code></pre>"},{"location":"user-guide/energy/#codecarbon-backend","title":"CodeCarbon Backend","text":"<p>CodeCarbon tracks energy consumption and CO2 emissions across platforms.</p>"},{"location":"user-guide/energy/#installation","title":"Installation","text":"<pre><code>pip install codecarbon\n</code></pre>"},{"location":"user-guide/energy/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from greenmining.energy.codecarbon_meter import CodeCarbonMeter\n\nmeter = CodeCarbonMeter(\n    country_iso_code=\"USA\",  # For carbon intensity\n    project_name=\"greenmining-analysis\",\n    tracking_mode=\"process\"  # or \"machine\"\n)\n\n# Start tracking\nmeter.start()\n\n# Your workload\nresult = analyzer.analyze_repository(repo_url)\n\n# Stop and get metrics\nmetrics = meter.stop()\n\nprint(f\"Energy: {metrics.energy_kwh:.6f} kWh\")\nprint(f\"CO2 emissions: {metrics.emissions_kg:.6f} kg\")\nprint(f\"Duration: {metrics.duration_seconds:.2f} s\")\n</code></pre>"},{"location":"user-guide/energy/#configuration-options","title":"Configuration Options","text":"<pre><code>meter = CodeCarbonMeter(\n    country_iso_code=\"FRA\",      # France\n    region=\"ile-de-france\",       # Optional region\n    project_name=\"my-analysis\",\n    output_dir=\"./energy_logs\",   # Where to save logs\n    save_to_file=True,            # Save detailed logs\n    tracking_mode=\"process\"       # process or machine\n)\n</code></pre>"},{"location":"user-guide/energy/#carbon-tracking","title":"Carbon Tracking","text":"<pre><code>from greenmining.energy.codecarbon_meter import CodeCarbonMeter\n\nmeter = CodeCarbonMeter(project_name=\"greenmining\")\n\nmeter.start()\n# ... analysis ...\nmetrics = meter.stop()\n\nprint(f\"Energy: {metrics.energy_joules:.2f} J\")\nprint(f\"Carbon footprint: {metrics.carbon_grams:.4f} g CO2\")\n</code></pre>"},{"location":"user-guide/energy/#energy-metrics","title":"Energy Metrics","text":""},{"location":"user-guide/energy/#energyresult-class","title":"EnergyResult Class","text":"<pre><code>from greenmining.energy.base import EnergyResult\n\n@dataclass\nclass EnergyResult:\n    energy_joules: float       # Total energy in Joules\n    duration_seconds: float    # Measurement duration\n    average_power_watts: float # Average power draw\n    start_time: datetime       # Measurement start\n    end_time: datetime         # Measurement end\n\n    # CodeCarbon specific\n    energy_kwh: float = 0.0    # Energy in kilowatt-hours\n    emissions_kg: float = 0.0  # CO2 emissions in kg\n</code></pre>"},{"location":"user-guide/energy/#commitenergyprofile","title":"CommitEnergyProfile","text":"<p>Track energy per commit analysis:</p> <pre><code>from greenmining.energy.base import CommitEnergyProfile\n\n@dataclass\nclass CommitEnergyProfile:\n    commit_sha: str\n    energy_joules: float\n    duration_seconds: float\n    patterns_detected: list\n    files_analyzed: int\n</code></pre>"},{"location":"user-guide/energy/#research-applications","title":"Research Applications","text":""},{"location":"user-guide/energy/#measuring-analysis-efficiency","title":"Measuring Analysis Efficiency","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nmeter = RAPLEnergyMeter()\nanalyzer = LocalRepoAnalyzer()\n\nrepos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n]\n\nresults = []\nfor url in repos:\n    meter.start()\n    analysis = analyzer.analyze_repository(url, max_commits=100)\n    energy = meter.stop()\n\n    results.append({\n        \"repo\": url,\n        \"commits\": analysis[\"total_commits\"],\n        \"energy_joules\": energy.energy_joules,\n        \"joules_per_commit\": energy.energy_joules / analysis[\"total_commits\"]\n    })\n\n# Compare efficiency\nfor r in results:\n    print(f\"{r['repo']}: {r['joules_per_commit']:.3f} J/commit\")\n</code></pre>"},{"location":"user-guide/energy/#energy-aware-batch-processing","title":"Energy-Aware Batch Processing","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\nenergy_budget_joules = 1000  # Set energy budget\n\ntotal_energy = 0\nanalyzed_repos = []\n\nfor repo in repositories:\n    meter.start()\n    result = analyze(repo)\n    metrics = meter.stop()\n\n    total_energy += metrics.energy_joules\n    analyzed_repos.append(repo)\n\n    if total_energy &gt;= energy_budget_joules:\n        print(f\"Energy budget reached after {len(analyzed_repos)} repos\")\n        break\n</code></pre>"},{"location":"user-guide/energy/#comparing-backends","title":"Comparing Backends","text":"Feature RAPL CodeCarbon Accuracy High (hardware) Medium (estimation) Platform Linux only Cross-platform Granularity Microseconds Seconds CO2 tracking No Yes Cloud support No Yes Setup May need root pip install"},{"location":"user-guide/energy/#recommendation","title":"Recommendation","text":"<ul> <li>Use RAPL for precise measurements on Linux</li> <li>Use CodeCarbon for cross-platform and carbon tracking</li> </ul>"},{"location":"user-guide/energy/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/energy/#rapl-not-available","title":"RAPL Not Available","text":"<pre><code># Check if RAPL files exist\nimport os\nrapl_path = \"/sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\"\nprint(f\"RAPL exists: {os.path.exists(rapl_path)}\")\n\n# Check permissions\nif os.path.exists(rapl_path):\n    print(f\"Readable: {os.access(rapl_path, os.R_OK)}\")\n</code></pre>"},{"location":"user-guide/energy/#codecarbon-import-error","title":"CodeCarbon Import Error","text":"<pre><code># Install with all dependencies\npip install codecarbon[viz]\n\n# Or minimal install\npip install codecarbon\n</code></pre>"},{"location":"user-guide/energy/#virtual-machine-limitations","title":"Virtual Machine Limitations","text":"<p>RAPL typically doesn't work in VMs. Use CodeCarbon instead:</p> <pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\nfrom greenmining.energy.codecarbon_meter import CodeCarbonMeter\n\n# Try RAPL first, fall back to CodeCarbon\nrapl = RAPLEnergyMeter()\nif rapl.is_available():\n    meter = rapl\nelse:\n    print(\"RAPL not available, using CodeCarbon\")\n    meter = CodeCarbonMeter()\n</code></pre>"},{"location":"user-guide/energy/#next-steps","title":"Next Steps","text":"<ul> <li>Python API - Full API reference</li> <li>URL Analysis - Analyze repositories by URL</li> <li>Configuration - Energy settings</li> </ul>"},{"location":"user-guide/url-analysis/","title":"URL Analysis","text":"<p>Analyze GitHub repositories directly by URL using PyDriller.</p>"},{"location":"user-guide/url-analysis/#overview","title":"Overview","text":"<p>URL analysis allows you to analyze any GitHub repository without using the GitHub API rate limits. It uses PyDriller to clone repositories locally and extract commit data with full diff information.</p>"},{"location":"user-guide/url-analysis/#benefits","title":"Benefits","text":"<ul> <li>No GitHub API limits - Clone and analyze directly</li> <li>Full commit data - Access diffs, modified files, metrics</li> <li>Process metrics - Code churn, change set size, contributor count</li> <li>DMM metrics - Delta Maintainability Model scores</li> <li>Historical analysis - Analyze any date range</li> </ul>"},{"location":"user-guide/url-analysis/#python-api","title":"Python API","text":""},{"location":"user-guide/url-analysis/#localrepoanalyzer","title":"LocalRepoAnalyzer","text":"<p>The main class for URL-based analysis.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/greenmining_repos\",  # Where to clone\n    cleanup_after=True                     # Delete after analysis\n)\n</code></pre>"},{"location":"user-guide/url-analysis/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>clone_path</code> str /tmp/greenmining_repos Directory for cloning <code>cleanup_after</code> bool True Delete cloned repo after analysis"},{"location":"user-guide/url-analysis/#single-repository-analysis","title":"Single Repository Analysis","text":"<p>Analyze a single repository.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom datetime import datetime\n\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=200,\n    since_date=datetime(2024, 1, 1),\n    to_date=datetime(2024, 12, 31)\n)\n\nprint(f\"Total commits: {result['total_commits']}\")\nprint(f\"Green-aware: {result['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"user-guide/url-analysis/#multiple-repositories","title":"Multiple Repositories","text":"<pre><code>repos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n    \"https://github.com/fastapi/fastapi\"\n]\n\nfor repo_url in repos:\n    result = analyzer.analyze_repository(repo_url, max_commits=100)\n    print(f\"{result['repository']['name']}: {result['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"user-guide/url-analysis/#analyze_repository-parameters","title":"analyze_repository() Parameters","text":"Parameter Type Default Description <code>repo_url</code> str (required) GitHub repository URL <code>max_commits</code> int 1000 Maximum commits to analyze <code>since_date</code> datetime None Start date filter <code>to_date</code> datetime None End date filter"},{"location":"user-guide/url-analysis/#return-value","title":"Return Value","text":"<pre><code>{\n    \"repository\": {\n        \"name\": \"flask\",\n        \"url\": \"https://github.com/pallets/flask\",\n        \"owner\": \"pallets\",\n        \"clone_path\": \"/tmp/greenmining_repos/flask\"\n    },\n    \"total_commits\": 200,\n    \"green_aware_count\": 47,\n    \"green_aware_percentage\": 23.5,\n    \"commits\": [\n        {\n            \"sha\": \"abc123...\",\n            \"message\": \"Optimize template caching\",\n            \"author\": \"developer\",\n            \"date\": \"2024-03-15T10:30:00\",\n            \"green_aware\": True,\n            \"patterns\": [\"Cache Static Data\"],\n            \"modified_files\": 3,\n            \"insertions\": 45,\n            \"deletions\": 12,\n            \"dmm_unit_size\": 0.85,\n            \"dmm_unit_complexity\": 0.72,\n            \"dmm_unit_interfacing\": 0.90\n        },\n        ...\n    ],\n    \"pattern_distribution\": {\n        \"Cache Static Data\": 15,\n        \"Use Async Instead of Sync\": 12,\n        ...\n    },\n    \"process_metrics\": {\n        \"change_set\": {\"max\": 25, \"avg\": 5.2},\n        \"code_churn\": {\"added\": 5000, \"removed\": 2000},\n        \"contributors_count\": 45\n    }\n}\n</code></pre>"},{"location":"user-guide/url-analysis/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Analyze Flask repository for green patterns.\"\"\"\n\nfrom datetime import datetime\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\n# Initialize analyzer\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/flask_analysis\",\n    cleanup_after=True\n)\n\n# Analyze repository\nprint(\"Analyzing Flask repository...\")\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=100,\n    since_date=datetime(2024, 1, 1)\n)\n\n# Print summary\nprint(f\"\\n{'='*60}\")\nprint(\"ANALYSIS RESULTS\")\nprint(f\"{'='*60}\")\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Total commits: {result['total_commits']}\")\nprint(f\"Green-aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\n# Top patterns\nprint(f\"\\nTop Patterns:\")\nfor pattern, count in sorted(\n    result['pattern_distribution'].items(), \n    key=lambda x: x[1], \n    reverse=True\n)[:5]:\n    print(f\"  {pattern}: {count}\")\n\n# Sample green commits\nprint(f\"\\nSample Green Commits:\")\ngreen_commits = [c for c in result['commits'] if c['green_aware']]\nfor commit in green_commits[:5]:\n    print(f\"  \ud83c\udf31 {commit['message'][:60]}...\")\n    print(f\"     Patterns: {commit['patterns']}\")\n</code></pre> <p>Output:</p> <pre><code>Analyzing Flask repository...\n\n============================================================\nANALYSIS RESULTS\n============================================================\nRepository: flask\nTotal commits: 100\nGreen-aware: 23 (23.0%)\n\nTop Patterns:\n  Cache Static Data: 8\n  Use Async Instead of Sync: 5\n  Lazy Loading: 4\n  Compress Transmitted Data: 3\n  Optimize Database Queries: 3\n\nSample Green Commits:\n  \ud83c\udf31 Implement response caching for static assets...\n     Patterns: ['Cache Static Data']\n  \ud83c\udf31 Add async support for request handling...\n     Patterns: ['Use Async Instead of Sync']\n</code></pre>"},{"location":"user-guide/url-analysis/#supported-url-formats","title":"Supported URL Formats","text":"<pre><code># HTTPS (recommended)\n\"https://github.com/owner/repo\"\n\"https://github.com/owner/repo.git\"\n\n# SSH\n\"git@github.com:owner/repo.git\"\n\n# With branch (coming soon)\n\"https://github.com/owner/repo/tree/branch-name\"\n</code></pre>"},{"location":"user-guide/url-analysis/#pydriller-integration","title":"PyDriller Integration","text":"<p>GreenMining uses PyDriller for commit extraction with these metrics:</p>"},{"location":"user-guide/url-analysis/#commit-metrics","title":"Commit Metrics","text":"Metric Description <code>modified_files</code> Number of files changed <code>insertions</code> Lines added <code>deletions</code> Lines removed <code>files</code> List of modified file paths"},{"location":"user-guide/url-analysis/#dmm-metrics-delta-maintainability-model","title":"DMM Metrics (Delta Maintainability Model)","text":"Metric Range Description <code>dmm_unit_size</code> 0-1 Unit size maintainability <code>dmm_unit_complexity</code> 0-1 Cyclomatic complexity impact <code>dmm_unit_interfacing</code> 0-1 Interface complexity"},{"location":"user-guide/url-analysis/#process-metrics","title":"Process Metrics","text":"Metric Description <code>change_set</code> Files changed per commit (max, avg) <code>code_churn</code> Lines added/removed over time <code>contributors_count</code> Unique contributors <code>commits_count</code> Total commits in period"},{"location":"user-guide/url-analysis/#configuration-options","title":"Configuration Options","text":"<p>Configure URL analysis via environment variables or Config:</p> <pre><code># Environment variables\nexport CLONE_PATH=/custom/path\nexport CLEANUP_AFTER_ANALYSIS=false\nexport PROCESS_METRICS_ENABLED=true\nexport DMM_ENABLED=true\n</code></pre> <pre><code># Python configuration\nfrom greenmining.config import Config\n\nconfig = Config()\nprint(config.CLONE_PATH)               # /tmp/greenmining_repos\nprint(config.CLEANUP_AFTER_ANALYSIS)   # True\nprint(config.PROCESS_METRICS_ENABLED)  # True\nprint(config.DMM_ENABLED)              # True\n</code></pre>"},{"location":"user-guide/url-analysis/#batch-analysis","title":"Batch Analysis","text":"<p>Analyze multiple repositories efficiently:</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nimport json\n\nrepos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n    \"https://github.com/fastapi/fastapi\",\n]\n\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\nall_results = []\n\nfor url in repos:\n    print(f\"Analyzing {url}...\")\n    result = analyzer.analyze_repository(url, max_commits=100)\n    all_results.append(result)\n    print(f\"  \u2713 {result['green_aware_count']}/{result['total_commits']} green-aware\")\n\n# Save combined results\nwith open(\"batch_results.json\", \"w\") as f:\n    json.dump(all_results, f, indent=2, default=str)\n\n# Summary\nprint(f\"\\nTotal repositories: {len(all_results)}\")\ntotal_commits = sum(r['total_commits'] for r in all_results)\ntotal_green = sum(r['green_aware_count'] for r in all_results)\nprint(f\"Total commits: {total_commits}\")\nprint(f\"Total green-aware: {total_green} ({total_green/total_commits*100:.1f}%)\")\n</code></pre>"},{"location":"user-guide/url-analysis/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/url-analysis/#clone-failures","title":"Clone Failures","text":"<pre><code># Increase timeout\nanalyzer = LocalRepoAnalyzer(clone_timeout=300)  # 5 minutes\n\n# Use SSH for private repos\nresult = analyzer.analyze_repository(\"git@github.com:org/private-repo.git\")\n</code></pre>"},{"location":"user-guide/url-analysis/#large-repositories","title":"Large Repositories","text":"<pre><code># Limit commits for large repos\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/kubernetes/kubernetes\",\n    max_commits=500  # Limit for faster analysis\n)\n</code></pre>"},{"location":"user-guide/url-analysis/#disk-space","title":"Disk Space","text":"<pre><code># Always cleanup\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\n\n# Or manual cleanup\nimport shutil\nshutil.rmtree(\"/tmp/greenmining_repos\")\n</code></pre>"},{"location":"user-guide/url-analysis/#next-steps","title":"Next Steps","text":"<ul> <li>Energy Measurement - Measure energy during analysis</li> <li>Python API - Full API reference</li> <li>Configuration Options - All settings</li> </ul>"}]}