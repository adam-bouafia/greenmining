{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GreenMining Documentation","text":"<p> A Python library for mining and analyzing green software engineering patterns in GitHub repositories </p> <p> </p>"},{"location":"#what-is-greenmining","title":"What is GreenMining?","text":"<p>GreenMining is a research-grade tool for analyzing software repositories to detect green software engineering practices. It identifies commits that implement sustainability patterns based on the Green Software Foundation (GSF) catalog.</p>"},{"location":"#key-capabilities","title":"Key Capabilities","text":"Feature Description 122 GSF Patterns Detect patterns across 15 categories (cloud, web, AI, caching, etc.) 321 Green Keywords Comprehensive keyword matching for green-aware commits GitHub Mining Fetch repositories by keywords, stars, language filters URL Analysis Analyze any GitHub repo directly via URL using PyDriller Statistical Analysis Pattern correlations, temporal trends, effect sizes Energy Measurement RAPL and CodeCarbon backends for power profiling Multiple Outputs JSON, CSV, and Markdown reports"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install greenmining\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Command Line:</p> <pre><code># Set your GitHub token\nexport GITHUB_TOKEN=your_token_here\n\n# Run full pipeline\ngreenmining pipeline --max-repos 10 --max-commits 50\n\n# Or analyze a specific repository\ngreenmining analyze-url https://github.com/user/repo\n</code></pre> <p>Python API:</p> <pre><code>from greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\n\n# Check if a commit message is green-aware\nmessage = \"Optimize Redis caching to reduce energy consumption\"\nprint(is_green_aware(message))  # True\n\n# Get matched patterns\npatterns = get_pattern_by_keywords(message)\nprint(patterns)  # ['Cache Static Data']\n</code></pre>"},{"location":"#pattern-categories","title":"Pattern Categories","text":"<p>GreenMining detects 122 patterns across 15 categories:</p> Category Patterns Examples Cloud 40+ Caching, compression, auto-scaling, serverless Web 15+ Lazy loading, image optimization, minification AI/ML 10+ Model optimization, quantization, efficient training Caching 8 Redis, CDN, static data caching Async 6 Batch processing, queue-based architecture Database 8 Query optimization, connection pooling Network 6 Compression, CDN, efficient protocols Resource 5 Memory management, CPU optimization Code 4 Dead code removal, algorithm efficiency Infrastructure 4 Container optimization, IaC Microservices 4 Service decomposition, graceful shutdown Monitoring 3 Efficient logging, metrics collection General 8 Feature flags, incremental processing"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Install GreenMining via pip, source, or Docker</li> <li>Quick Start - Run your first analysis in 5 minutes</li> <li>Configuration - Configure via environment variables or config files</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>CLI Commands - Complete reference for all CLI commands</li> <li>Python API - Use GreenMining programmatically</li> <li>URL Analysis - Analyze repositories directly by URL</li> <li>Energy Measurement - Measure energy consumption with RAPL/CodeCarbon</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>GSF Patterns - All 122 patterns with categories and keywords</li> <li>Configuration Options - All configuration parameters</li> <li>Data Models - Repository, Commit, and AnalysisResult models</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Basic Usage - Simple pattern detection examples</li> <li>Complete Pipeline - Full research workflow example</li> </ul>"},{"location":"#example-output","title":"Example Output","text":"<p>When analyzing a repository, GreenMining produces reports like:</p> <pre><code>============================================================\nGREENMINING ANALYSIS RESULTS\n============================================================\n\nRepository: kubernetes/kubernetes\nCommits analyzed: 1000\nGreen-aware commits: 247 (24.7%)\n\nTop Patterns Detected:\n  1. Cache Static Data (89 commits)\n  2. Use Async Instead of Sync (67 commits)\n  3. Containerize Workload (45 commits)\n  4. Compress Transmitted Data (31 commits)\n\nCategories Distribution:\n  cloud: 45.2%\n  caching: 23.1%\n  async: 18.5%\n  infrastructure: 13.2%\n</code></pre>"},{"location":"#research-applications","title":"Research Applications","text":"<p>GreenMining is designed for software engineering research:</p> <ul> <li>Mining Software Repositories (MSR) studies on green practices</li> <li>Green Software Foundation pattern adoption analysis</li> <li>Temporal trend analysis of sustainability practices</li> <li>Cross-repository comparison of green awareness</li> <li>Energy measurement of analysis workloads</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use GreenMining in your research, please cite:</p> <pre><code>@software{greenmining2024,\n  author = {Bouafia, Adam},\n  title = {GreenMining: Mining Green Software Patterns},\n  year = {2024},\n  url = {https://github.com/adam-bouafia/greenmining}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>GreenMining is released under the MIT License.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to GreenMining are documented here.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Comprehensive MkDocs documentation</li> <li>Read the Docs integration</li> </ul>"},{"location":"changelog/#020-2024-xx-xx","title":"[0.2.0] - 2024-XX-XX","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>URL-based Analysis: Analyze repositories directly by URL using PyDriller</li> <li><code>greenmining analyze-url &lt;url&gt;</code> command</li> <li><code>greenmining analyze-urls &lt;file&gt;</code> for batch processing</li> <li><code>LocalRepoAnalyzer</code> service for Python API</li> <li>Energy Measurement: Track energy consumption during analysis</li> <li>RAPL backend for Linux (Intel CPUs)</li> <li>CodeCarbon backend for cross-platform carbon tracking</li> <li><code>--energy</code> and <code>--energy-backend</code> CLI options</li> <li>PyDriller Integration: Rich commit metrics</li> <li>Delta Maintainability Model (DMM) metrics</li> <li>Process metrics (code churn, change set, contributor count)</li> <li>Structural metrics (complexity, lines of code)</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Renamed \"Enhanced\" terminology to standard naming throughout codebase</li> <li>Simplified analyzer architecture</li> <li>Improved CLI command structure</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>NLP-based semantic analysis (simplified to keyword matching)</li> <li>ML-based classification (not needed for pattern detection)</li> <li>External ML model dependencies</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Pattern detection accuracy improvements</li> <li>Configuration loading from .env files</li> <li>Report generation formatting</li> </ul>"},{"location":"changelog/#010-2024-xx-xx","title":"[0.1.0] - 2024-XX-XX","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Initial release</li> <li>GSF pattern detection (122 patterns, 15 categories)</li> <li>GitHub repository fetching</li> <li>Commit extraction and analysis</li> <li>Statistical analysis with effect sizes</li> <li>Temporal trend analysis</li> <li>CLI interface with 10 commands</li> <li>Docker support</li> <li>Configuration via environment variables</li> </ul>"},{"location":"changelog/#core-features","title":"Core Features","text":"<ul> <li><code>greenmining fetch</code> - Fetch repositories from GitHub</li> <li><code>greenmining extract</code> - Extract commits from repositories</li> <li><code>greenmining analyze</code> - Analyze commits for green patterns</li> <li><code>greenmining aggregate</code> - Compute statistics</li> <li><code>greenmining report</code> - Generate Markdown reports</li> <li><code>greenmining pipeline</code> - Run complete workflow</li> <li><code>greenmining status</code> - Check system status</li> </ul>"},{"location":"changelog/#dependencies","title":"Dependencies","text":"<ul> <li>Python 3.9+</li> <li>PyDriller for repository mining</li> <li>Click for CLI</li> <li>python-dotenv for configuration</li> </ul>"},{"location":"changelog/#migration-guide","title":"Migration Guide","text":""},{"location":"changelog/#from-01x-to-020","title":"From 0.1.x to 0.2.0","text":""},{"location":"changelog/#api-changes","title":"API Changes","text":"<ol> <li> <p>Import paths unchanged:    <pre><code># Still works\nfrom greenmining import is_green_aware, GSF_PATTERNS\n</code></pre></p> </li> <li> <p>New services:    <pre><code># New in 0.2.0\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom greenmining.energy.base import EnergyMeasurer\n</code></pre></p> </li> <li> <p>Removed NLP/ML features:    <pre><code># These no longer exist - remove from your code\n# from greenmining.analyzers import SemanticAnalyzer  # Removed\n# from greenmining.analyzers import MLClassifier  # Removed\n</code></pre></p> </li> </ol>"},{"location":"changelog/#cli-changes","title":"CLI Changes","text":"<ol> <li> <p>New commands:    <pre><code>greenmining analyze-url https://github.com/org/repo\ngreenmining analyze-urls urls.txt\n</code></pre></p> </li> <li> <p>New options:    <pre><code>greenmining analyze-url &lt;url&gt; --energy --energy-backend rapl\n</code></pre></p> </li> </ol>"},{"location":"changelog/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>Documentation</li> <li>PyPI Package</li> </ul>"},{"location":"contributing/","title":"Contributing to GreenMining","text":"<p>Thank you for your interest in contributing to GreenMining! </p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code># Fork on GitHub, then:\ngit clone https://github.com/YOUR-USERNAME/greenmining.git\ncd greenmining\n</code></pre>"},{"location":"contributing/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<pre><code># Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\n# or: venv\\Scripts\\activate  # Windows\n\n# Install in development mode\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\npytest\n\n# With coverage\npytest --cov=greenmining --cov-report=html\n\n# Specific test file\npytest tests/test_gsf_patterns.py -v\n</code></pre>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We follow PEP 8 with these tools:</p> <pre><code># Format code\nblack greenmining/ tests/\n\n# Check types\nmypy greenmining/\n\n# Lint\nruff check greenmining/\n</code></pre>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Configuration (<code>.pre-commit-config.yaml</code>):</p> <pre><code>repos:\n  - repo: https://github.com/psf/black\n    rev: 23.12.1\n    hooks:\n      - id: black\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.1.9\n    hooks:\n      - id: ruff\n</code></pre>"},{"location":"contributing/#contribution-types","title":"Contribution Types","text":""},{"location":"contributing/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<ol> <li>Search existing issues first</li> <li>Create a new issue with:</li> <li>GreenMining version (<code>greenmining --version</code>)</li> <li>Python version</li> <li>Operating system</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Error messages/tracebacks</li> </ol>"},{"location":"contributing/#feature-requests","title":"\u2728 Feature Requests","text":"<ol> <li>Check existing issues and roadmap</li> <li>Create an issue describing:</li> <li>Use case</li> <li>Proposed solution</li> <li>Alternatives considered</li> </ol>"},{"location":"contributing/#documentation","title":"\ud83d\udcdd Documentation","text":"<ul> <li>Fix typos and clarify explanations</li> <li>Add examples</li> <li>Improve docstrings</li> <li>Update README</li> </ul>"},{"location":"contributing/#code-contributions","title":"\ud83d\udd27 Code Contributions","text":"<ol> <li>Open an issue to discuss major changes</li> <li>Fork and create a feature branch</li> <li>Write tests for new functionality</li> <li>Ensure all tests pass</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#adding-new-gsf-patterns","title":"Adding New GSF Patterns","text":""},{"location":"contributing/#1-update-pattern-definition","title":"1. Update Pattern Definition","text":"<p>Edit <code>greenmining/gsf_patterns.py</code>:</p> <pre><code>GSF_PATTERNS = {\n    # ... existing patterns ...\n\n    \"new_pattern_key\": {\n        \"name\": \"New Pattern Name\",\n        \"category\": \"category_name\",  # cloud, web, ai, caching, etc.\n        \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n        \"description\": \"Brief description of the pattern\",\n        \"sci_impact\": \"How this pattern reduces software carbon intensity\"\n    },\n}\n</code></pre>"},{"location":"contributing/#2-add-keywords-to-green_keywords","title":"2. Add Keywords to GREEN_KEYWORDS","text":"<pre><code>GREEN_KEYWORDS = [\n    # ... existing keywords ...\n    \"keyword1\",\n    \"keyword2\", \n    \"keyword3\",\n]\n</code></pre>"},{"location":"contributing/#3-write-tests","title":"3. Write Tests","text":"<pre><code># tests/test_gsf_patterns.py\n\ndef test_new_pattern_detection():\n    \"\"\"Test that new pattern is detected correctly.\"\"\"\n    from greenmining import is_green_aware, get_pattern_by_keywords\n\n    # Should detect\n    assert is_green_aware(\"message with keyword1\")\n    patterns = get_pattern_by_keywords(\"message with keyword1\")\n    assert \"New Pattern Name\" in patterns\n\n    # Should not detect\n    assert not is_green_aware(\"unrelated message\")\n</code></pre>"},{"location":"contributing/#4-update-documentation","title":"4. Update Documentation","text":"<p>Add pattern to <code>docs/reference/patterns.md</code></p>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>greenmining/\n\u251c\u2500\u2500 __init__.py          # Package exports\n\u251c\u2500\u2500 __main__.py          # Entry point\n\u251c\u2500\u2500 cli.py               # CLI commands\n\u251c\u2500\u2500 config.py            # Configuration\n\u251c\u2500\u2500 gsf_patterns.py      # Pattern definitions\n\u251c\u2500\u2500 main.py              # Core functions\n\u251c\u2500\u2500 utils.py             # Utilities\n\u251c\u2500\u2500 analyzers/           # Analysis modules\n\u2502   \u251c\u2500\u2500 qualitative_analyzer.py\n\u2502   \u251c\u2500\u2500 statistical_analyzer.py\n\u2502   \u2514\u2500\u2500 temporal_analyzer.py\n\u251c\u2500\u2500 controllers/         # Controllers\n\u2502   \u2514\u2500\u2500 repository_controller.py\n\u251c\u2500\u2500 energy/              # Energy measurement\n\u2502   \u251c\u2500\u2500 base.py\n\u2502   \u251c\u2500\u2500 rapl_backend.py\n\u2502   \u2514\u2500\u2500 codecarbon_backend.py\n\u251c\u2500\u2500 models/              # Data models\n\u251c\u2500\u2500 presenters/          # Output formatting\n\u2502   \u2514\u2500\u2500 console_presenter.py\n\u2514\u2500\u2500 services/            # Core services\n    \u251c\u2500\u2500 commit_extractor.py\n    \u251c\u2500\u2500 data_aggregator.py\n    \u251c\u2500\u2500 data_analyzer.py\n    \u251c\u2500\u2500 github_fetcher.py\n    \u251c\u2500\u2500 local_repo_analyzer.py\n    \u2514\u2500\u2500 reports.py\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#1-create-branch","title":"1. Create Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/issue-description\n</code></pre>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write clean, documented code</li> <li>Add tests for new functionality</li> <li>Update documentation if needed</li> </ul>"},{"location":"contributing/#3-commit-with-clear-messages","title":"3. Commit with Clear Messages","text":"<pre><code>git add .\ngit commit -m \"feat: add new pattern detection for X\"\n# or\ngit commit -m \"fix: resolve issue with Y\"\n# or\ngit commit -m \"docs: update installation instructions\"\n</code></pre> <p>Follow Conventional Commits: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation - <code>test:</code> - Tests - <code>refactor:</code> - Code refactoring - <code>chore:</code> - Maintenance</p>"},{"location":"contributing/#4-push-and-create-pr","title":"4. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub with: - Clear title - Description of changes - Link to related issue - Screenshots if UI changes</p>"},{"location":"contributing/#5-address-review-feedback","title":"5. Address Review Feedback","text":"<p>Respond to reviewer comments and make requested changes.</p>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#test-structure","title":"Test Structure","text":"<pre><code># tests/test_module.py\n\nimport pytest\nfrom greenmining import function_to_test\n\nclass TestFunctionName:\n    \"\"\"Tests for function_to_test.\"\"\"\n\n    def test_basic_functionality(self):\n        \"\"\"Test basic happy path.\"\"\"\n        result = function_to_test(\"input\")\n        assert result == \"expected\"\n\n    def test_edge_case(self):\n        \"\"\"Test edge case handling.\"\"\"\n        result = function_to_test(\"\")\n        assert result is None\n\n    def test_error_handling(self):\n        \"\"\"Test error conditions.\"\"\"\n        with pytest.raises(ValueError):\n            function_to_test(None)\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest\n\n# Verbose output\npytest -v\n\n# Specific test\npytest tests/test_cli.py::test_analyze_command -v\n\n# With coverage\npytest --cov=greenmining --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help newcomers</li> <li>Acknowledge contributions</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b Issue Tracker</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"examples/basic/","title":"Basic Usage Examples","text":"<p>Simple examples to get started with GreenMining.</p>"},{"location":"examples/basic/#analyzing-a-single-repository","title":"Analyzing a Single Repository","text":""},{"location":"examples/basic/#by-url-recommended","title":"By URL (Recommended)","text":"<pre><code># Analyze any GitHub repository by URL\ngreenmining analyze-url https://github.com/pallets/flask\n\n# With energy measurement (Linux only)\ngreenmining analyze-url https://github.com/pallets/flask --energy\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udcca GreenMining Analysis Complete\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nRepository: pallets/flask\nTotal Commits: 200\nGreen-Aware Commits: 47 (23.5%)\n\nTop Patterns:\n  - Cache Static Data: 15\n  - Use Async Instead of Sync: 12  \n  - Lazy Loading: 8\n\nResults saved to: data/url_analysis_flask.json\n</code></pre>"},{"location":"examples/basic/#using-python-api","title":"Using Python API","text":"<pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\n# Initialize analyzer\nanalyzer = LocalRepoAnalyzer()\n\n# Analyze repository\nurl = \"https://github.com/pallets/flask\"\nresult = analyzer.analyze_repository(url)\n\n# Print results\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Total Commits: {result['total_commits']}\")\nprint(f\"Green-Aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\nprint(\"\\nTop Patterns:\")\nfor pattern, count in sorted(\n    result['pattern_distribution'].items(),\n    key=lambda x: x[1],\n    reverse=True\n)[:5]:\n    print(f\"  - {pattern}: {count}\")\n</code></pre>"},{"location":"examples/basic/#detecting-green-patterns-in-text","title":"Detecting Green Patterns in Text","text":""},{"location":"examples/basic/#single-message-detection","title":"Single Message Detection","text":"<pre><code>from greenmining import is_green_aware, get_pattern_by_keywords\n\n# Check if a commit message is green-aware\nmessages = [\n    \"Implement Redis caching for user sessions\",\n    \"Enable gzip compression on API responses\",\n    \"Fix typo in README\",\n    \"Migrate to serverless Lambda functions\",\n    \"Update dependencies\",\n    \"Optimize database queries with indexing\",\n]\n\nfor msg in messages:\n    if is_green_aware(msg):\n        patterns = get_pattern_by_keywords(msg)\n        print(f\"\ud83c\udf31 {msg}\")\n        print(f\"   \u2192 Patterns: {patterns}\")\n    else:\n        print(f\"   {msg}\")\n</code></pre> <p>Output:</p> <pre><code>\ud83c\udf31 Implement Redis caching for user sessions\n   \u2192 Patterns: ['Cache Static Data']\n\ud83c\udf31 Enable gzip compression on API responses\n   \u2192 Patterns: ['Compress Transmitted Data', 'Enable Text Compression']\n   Fix typo in README\n\ud83c\udf31 Migrate to serverless Lambda functions\n   \u2192 Patterns: ['Use Serverless']\n   Update dependencies\n\ud83c\udf31 Optimize database queries with indexing\n   \u2192 Patterns: ['Optimize Database Queries', 'Index Optimization']\n</code></pre>"},{"location":"examples/basic/#batch-analysis","title":"Batch Analysis","text":"<pre><code>from greenmining import is_green_aware, get_pattern_by_keywords\n\ncommits = [\n    {\"sha\": \"abc123\", \"message\": \"Add lazy loading for images\"},\n    {\"sha\": \"def456\", \"message\": \"Refactor authentication module\"},\n    {\"sha\": \"ghi789\", \"message\": \"Enable HTTP/2 for API server\"},\n    {\"sha\": \"jkl012\", \"message\": \"Update changelog\"},\n    {\"sha\": \"mno345\", \"message\": \"Use WebSockets for real-time updates\"},\n]\n\nresults = []\nfor commit in commits:\n    green = is_green_aware(commit[\"message\"])\n    patterns = get_pattern_by_keywords(commit[\"message\"]) if green else []\n    results.append({\n        **commit,\n        \"green_aware\": green,\n        \"patterns\": patterns\n    })\n\n# Summary\ngreen_count = sum(1 for r in results if r[\"green_aware\"])\nprint(f\"Green-aware: {green_count}/{len(results)} ({100*green_count/len(results):.0f}%)\")\n</code></pre>"},{"location":"examples/basic/#exploring-gsf-patterns","title":"Exploring GSF Patterns","text":""},{"location":"examples/basic/#list-all-patterns","title":"List All Patterns","text":"<pre><code>from greenmining import GSF_PATTERNS\n\n# Count patterns\nprint(f\"Total patterns: {len(GSF_PATTERNS)}\")\n\n# List pattern names\nfor key, pattern in sorted(GSF_PATTERNS.items()):\n    print(f\"  - {pattern['name']} ({pattern['category']})\")\n</code></pre>"},{"location":"examples/basic/#filter-by-category","title":"Filter by Category","text":"<pre><code>from greenmining import GSF_PATTERNS\n\n# Get all categories\ncategories = sorted(set(p[\"category\"] for p in GSF_PATTERNS.values()))\nprint(f\"Categories: {categories}\")\n\n# Get patterns for a specific category\ncategory = \"caching\"\ncaching_patterns = [\n    p for p in GSF_PATTERNS.values() \n    if p[\"category\"] == category\n]\n\nprint(f\"\\n{category.upper()} Patterns:\")\nfor p in caching_patterns:\n    print(f\"  - {p['name']}: {p['keywords'][:3]}...\")\n</code></pre>"},{"location":"examples/basic/#search-patterns-by-keyword","title":"Search Patterns by Keyword","text":"<pre><code>from greenmining import GSF_PATTERNS\n\ndef search_patterns(keyword):\n    \"\"\"Find patterns that match a keyword.\"\"\"\n    matches = []\n    keyword = keyword.lower()\n    for pattern in GSF_PATTERNS.values():\n        if any(keyword in kw for kw in pattern[\"keywords\"]):\n            matches.append(pattern)\n    return matches\n\n# Search examples\nresults = search_patterns(\"redis\")\nprint(\"Patterns matching 'redis':\")\nfor p in results:\n    print(f\"  - {p['name']}\")\n\nresults = search_patterns(\"compress\")\nprint(\"\\nPatterns matching 'compress':\")\nfor p in results:\n    print(f\"  - {p['name']}\")\n</code></pre>"},{"location":"examples/basic/#working-with-configuration","title":"Working with Configuration","text":""},{"location":"examples/basic/#environment-variables","title":"Environment Variables","text":"<pre><code># Set token\nexport GITHUB_TOKEN=ghp_xxxxxxxxxxxx\n\n# Run analysis\ngreenmining analyze-url https://github.com/org/repo\n</code></pre>"},{"location":"examples/basic/#env-file","title":".env File","text":"<pre><code># Create .env file\ncat &gt; .env &lt;&lt; EOF\nGITHUB_TOKEN=ghp_xxxxxxxxxxxx\nMAX_REPOS=50\nMIN_STARS=500\nENABLE_STATS=true\nEOF\n\n# GreenMining automatically loads .env\ngreenmining pipeline\n</code></pre>"},{"location":"examples/basic/#python-configuration","title":"Python Configuration","text":"<pre><code>from greenmining.config import Config\n\n# Initialize config\nconfig = Config()\n\n# Override settings\nconfig.MAX_REPOS = 25\nconfig.COMMITS_PER_REPO = 500\nconfig.ENABLE_TEMPORAL = True\n\n# View settings\nprint(f\"MAX_REPOS: {config.MAX_REPOS}\")\nprint(f\"COMMITS_PER_REPO: {config.COMMITS_PER_REPO}\")\nprint(f\"Languages: {config.SUPPORTED_LANGUAGES}\")\n</code></pre>"},{"location":"examples/basic/#reading-analysis-results","title":"Reading Analysis Results","text":""},{"location":"examples/basic/#load-previous-results","title":"Load Previous Results","text":"<pre><code>import json\n\n# Load analysis results\nwith open(\"data/analysis_results.json\") as f:\n    results = json.load(f)\n\n# Basic stats\ntotal = len(results)\ngreen = sum(1 for r in results if r.get(\"green_aware\", False))\nprint(f\"Total commits: {total}\")\nprint(f\"Green-aware: {green} ({100*green/total:.1f}%)\")\n\n# Pattern distribution\nfrom collections import Counter\npatterns = Counter()\nfor r in results:\n    for p in r.get(\"patterns\", []):\n        patterns[p] += 1\n\nprint(\"\\nTop 5 patterns:\")\nfor pattern, count in patterns.most_common(5):\n    print(f\"  {pattern}: {count}\")\n</code></pre>"},{"location":"examples/basic/#load-statistics","title":"Load Statistics","text":"<pre><code>import json\n\nwith open(\"data/aggregated_statistics.json\") as f:\n    stats = json.load(f)\n\n# Summary\nsummary = stats.get(\"summary\", {})\nprint(f\"Total commits: {summary.get('total_commits', 0)}\")\nprint(f\"Green awareness: {summary.get('green_aware_percentage', 0):.1f}%\")\n\n# Top patterns\nprint(\"\\nTop patterns:\")\nfor pattern, count in sorted(\n    stats.get(\"pattern_distribution\", {}).items(),\n    key=lambda x: x[1],\n    reverse=True\n)[:5]:\n    print(f\"  {pattern}: {count}\")\n</code></pre>"},{"location":"examples/basic/#cli-quick-reference","title":"CLI Quick Reference","text":"<pre><code># Show all commands\ngreenmining --help\n\n# Check status\ngreenmining status\n\n# Analyze URL (most common)\ngreenmining analyze-url https://github.com/owner/repo\n\n# Analyze multiple URLs\ngreenmining analyze-urls urls.txt --output batch_results.json\n\n# Full pipeline (fetch \u2192 extract \u2192 analyze \u2192 aggregate \u2192 report)\ngreenmining pipeline --max-repos 10\n\n# Generate report from existing data\ngreenmining report --format markdown\n</code></pre>"},{"location":"examples/basic/#next-steps","title":"Next Steps","text":"<ul> <li>Complete Pipeline Example - Research workflow</li> <li>CLI Reference - All commands</li> <li>Python API - Programmatic usage</li> </ul>"},{"location":"examples/pipeline/","title":"Complete Pipeline Example","text":"<p>A comprehensive research workflow example using GreenMining.</p>"},{"location":"examples/pipeline/#research-scenario","title":"Research Scenario","text":"<p>We'll analyze green software practices in Python microservices projects:</p> <ol> <li>Fetch repositories from GitHub</li> <li>Extract and analyze commits</li> <li>Compute statistics</li> <li>Generate a research report</li> </ol>"},{"location":"examples/pipeline/#step-1-setup","title":"Step 1: Setup","text":""},{"location":"examples/pipeline/#install-greenmining","title":"Install GreenMining","text":"<pre><code>pip install greenmining\n</code></pre>"},{"location":"examples/pipeline/#configure-github-token","title":"Configure GitHub Token","text":"<pre><code>export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre>"},{"location":"examples/pipeline/#create-project-directory","title":"Create Project Directory","text":"<pre><code>mkdir green-research\ncd green-research\n</code></pre>"},{"location":"examples/pipeline/#step-2-run-full-pipeline-cli","title":"Step 2: Run Full Pipeline (CLI)","text":""},{"location":"examples/pipeline/#option-a-single-command","title":"Option A: Single Command","text":"<pre><code>greenmining pipeline \\\n    --max-repos 20 \\\n    --min-stars 500 \\\n    --languages Python \\\n    --keywords microservices \\\n    --max-commits 500 \\\n    --enable-stats \\\n    --enable-temporal \\\n    --temporal-granularity quarter\n</code></pre>"},{"location":"examples/pipeline/#option-b-step-by-step","title":"Option B: Step by Step","text":"<pre><code># 1. Fetch repositories\ngreenmining fetch \\\n    --max-repos 20 \\\n    --min-stars 500 \\\n    --languages Python \\\n    --keywords microservices\n\n# 2. Extract commits\ngreenmining extract --max-commits 500\n\n# 3. Analyze commits\ngreenmining analyze --enable-diff-analysis\n\n# 4. Aggregate statistics\ngreenmining aggregate \\\n    --enable-stats \\\n    --enable-temporal \\\n    --temporal-granularity quarter\n\n# 5. Generate report\ngreenmining report --format markdown\n</code></pre>"},{"location":"examples/pipeline/#step-3-run-pipeline-python","title":"Step 3: Run Pipeline (Python)","text":""},{"location":"examples/pipeline/#complete-script","title":"Complete Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nGreen Software Research Pipeline\nAnalyzes green patterns in Python microservices projects\n\"\"\"\n\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\nfrom greenmining.config import Config\nfrom greenmining.services import (\n    GitHubFetcher,\n    CommitExtractor,\n    DataAnalyzer,\n    DataAggregator,\n    ReportGenerator\n)\n\ndef main():\n    print(\"\ud83c\udf31 Green Software Research Pipeline\")\n    print(\"=\" * 50)\n\n    # Configuration\n    config = Config()\n    config.MAX_REPOS = 20\n    config.MIN_STARS = 500\n    config.SUPPORTED_LANGUAGES = [\"Python\"]\n    config.SEARCH_KEYWORDS = \"microservices\"\n    config.COMMITS_PER_REPO = 500\n    config.ENABLE_STATS = True\n    config.ENABLE_TEMPORAL = True\n    config.TEMPORAL_GRANULARITY = \"quarter\"\n\n    output_dir = Path(\"data\")\n    output_dir.mkdir(exist_ok=True)\n\n    # Step 1: Fetch repositories\n    print(\"\\n\ud83d\udce5 Step 1: Fetching repositories...\")\n    fetcher = GitHubFetcher(config)\n    repos = fetcher.fetch_repositories()\n    print(f\"   Found {len(repos)} repositories\")\n\n    with open(output_dir / \"repositories.json\", \"w\") as f:\n        json.dump(repos, f, indent=2, default=str)\n\n    # Step 2: Extract commits\n    print(\"\\n\ud83d\udcdd Step 2: Extracting commits...\")\n    extractor = CommitExtractor(config)\n    commits = extractor.extract_commits(repos)\n    print(f\"   Extracted {len(commits)} commits\")\n\n    with open(output_dir / \"commits.json\", \"w\") as f:\n        json.dump(commits, f, indent=2, default=str)\n\n    # Step 3: Analyze commits\n    print(\"\\n\ud83d\udd0d Step 3: Analyzing commits...\")\n    analyzer = DataAnalyzer(config)\n    results = analyzer.analyze(commits)\n\n    green_count = sum(1 for r in results if r.get(\"green_aware\", False))\n    print(f\"   Green-aware: {green_count}/{len(results)} ({100*green_count/len(results):.1f}%)\")\n\n    with open(output_dir / \"analysis_results.json\", \"w\") as f:\n        json.dump(results, f, indent=2, default=str)\n\n    # Step 4: Aggregate statistics\n    print(\"\\n\ud83d\udcca Step 4: Computing statistics...\")\n    aggregator = DataAggregator(config)\n    stats = aggregator.aggregate(results)\n\n    with open(output_dir / \"aggregated_statistics.json\", \"w\") as f:\n        json.dump(stats, f, indent=2, default=str)\n\n    # Step 5: Generate report\n    print(\"\\n\ud83d\udcc4 Step 5: Generating report...\")\n    reporter = ReportGenerator(config)\n    report = reporter.generate(stats)\n\n    with open(output_dir / \"green_analysis.md\", \"w\") as f:\n        f.write(report)\n\n    # Summary\n    print(\"\\n\" + \"=\" * 50)\n    print(\"\u2705 Pipeline Complete!\")\n    print(f\"\\nResults Summary:\")\n    print(f\"   Repositories: {len(repos)}\")\n    print(f\"   Total Commits: {len(results)}\")\n    print(f\"   Green-Aware: {green_count} ({100*green_count/len(results):.1f}%)\")\n\n    if \"pattern_distribution\" in stats:\n        print(f\"\\nTop 5 Patterns:\")\n        for pattern, count in sorted(\n            stats[\"pattern_distribution\"].items(),\n            key=lambda x: x[1],\n            reverse=True\n        )[:5]:\n            print(f\"   - {pattern}: {count}\")\n\n    print(f\"\\nOutput files in: {output_dir.absolute()}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/pipeline/#run-script","title":"Run Script","text":"<pre><code>python research_pipeline.py\n</code></pre>"},{"location":"examples/pipeline/#step-4-url-based-analysis","title":"Step 4: URL-Based Analysis","text":"<p>For analyzing specific repositories by URL:</p>"},{"location":"examples/pipeline/#analyze-single-repository","title":"Analyze Single Repository","text":"<pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nimport json\n\nanalyzer = LocalRepoAnalyzer()\n\n# Analyze Flask web framework\nresult = analyzer.analyze_repository(\"https://github.com/pallets/flask\")\n\n# Save results\nwith open(\"flask_analysis.json\", \"w\") as f:\n    json.dump(result, f, indent=2, default=str)\n\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Green-aware: {result['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"examples/pipeline/#analyze-multiple-repositories","title":"Analyze Multiple Repositories","text":"<pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nimport json\n\nrepos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n    \"https://github.com/fastapi/fastapi\",\n]\n\nanalyzer = LocalRepoAnalyzer()\nresults = []\n\nfor url in repos:\n    print(f\"Analyzing {url}...\")\n    result = analyzer.analyze_repository(url)\n    results.append(result)\n\n    print(f\"  Commits: {result['total_commits']}\")\n    print(f\"  Green: {result['green_aware_percentage']:.1f}%\")\n\n# Save combined results\nwith open(\"multi_repo_analysis.json\", \"w\") as f:\n    json.dump(results, f, indent=2, default=str)\n\n# Compare frameworks\nprint(\"\\nComparison:\")\nfor r in sorted(results, key=lambda x: x[\"green_aware_percentage\"], reverse=True):\n    name = r[\"repository\"][\"name\"]\n    pct = r[\"green_aware_percentage\"]\n    print(f\"  {name}: {pct:.1f}%\")\n</code></pre>"},{"location":"examples/pipeline/#step-5-analyze-results","title":"Step 5: Analyze Results","text":""},{"location":"examples/pipeline/#load-and-explore-data","title":"Load and Explore Data","text":"<pre><code>import json\nfrom collections import Counter\n\n# Load results\nwith open(\"data/analysis_results.json\") as f:\n    results = json.load(f)\n\nwith open(\"data/aggregated_statistics.json\") as f:\n    stats = json.load(f)\n\n# Basic statistics\ntotal = len(results)\ngreen = sum(1 for r in results if r.get(\"green_aware\", False))\n\nprint(f\"Total commits analyzed: {total}\")\nprint(f\"Green-aware commits: {green} ({100*green/total:.1f}%)\")\n</code></pre>"},{"location":"examples/pipeline/#pattern-analysis","title":"Pattern Analysis","text":"<pre><code># Pattern frequency\npatterns = Counter()\nfor result in results:\n    for pattern in result.get(\"patterns\", []):\n        patterns[pattern] += 1\n\nprint(\"\\nPattern Distribution:\")\nfor pattern, count in patterns.most_common(10):\n    pct = 100 * count / green\n    print(f\"  {pattern}: {count} ({pct:.1f}%)\")\n</code></pre>"},{"location":"examples/pipeline/#category-analysis","title":"Category Analysis","text":"<pre><code># Category distribution\ncategories = Counter()\nfor result in results:\n    if result.get(\"category\"):\n        categories[result[\"category\"]] += 1\n\nprint(\"\\nCategory Distribution:\")\nfor category, count in categories.most_common():\n    pct = 100 * count / green\n    print(f\"  {category}: {count} ({pct:.1f}%)\")\n</code></pre>"},{"location":"examples/pipeline/#temporal-trends","title":"Temporal Trends","text":"<pre><code>from datetime import datetime\nfrom collections import defaultdict\n\n# Group by month\nmonthly = defaultdict(lambda: {\"total\": 0, \"green\": 0})\n\nfor result in results:\n    date = datetime.fromisoformat(result[\"date\"].replace(\"Z\", \"+00:00\"))\n    month = date.strftime(\"%Y-%m\")\n    monthly[month][\"total\"] += 1\n    if result.get(\"green_aware\"):\n        monthly[month][\"green\"] += 1\n\nprint(\"\\nMonthly Trend:\")\nfor month in sorted(monthly.keys())[-12:]:  # Last 12 months\n    data = monthly[month]\n    pct = 100 * data[\"green\"] / data[\"total\"] if data[\"total\"] &gt; 0 else 0\n    bar = \"\u2588\" * int(pct / 5)\n    print(f\"  {month}: {pct:5.1f}% {bar}\")\n</code></pre>"},{"location":"examples/pipeline/#step-6-export-for-research","title":"Step 6: Export for Research","text":""},{"location":"examples/pipeline/#export-to-csv","title":"Export to CSV","text":"<pre><code>import csv\n\nwith open(\"green_commits.csv\", \"w\", newline=\"\") as f:\n    writer = csv.DictWriter(f, fieldnames=[\n        \"sha\", \"repository\", \"author\", \"date\",\n        \"green_aware\", \"patterns\", \"category\", \"message\"\n    ])\n    writer.writeheader()\n\n    for r in results:\n        writer.writerow({\n            \"sha\": r[\"sha\"][:8],\n            \"repository\": r.get(\"repository\", \"\"),\n            \"author\": r.get(\"author\", \"\"),\n            \"date\": r.get(\"date\", \"\"),\n            \"green_aware\": r.get(\"green_aware\", False),\n            \"patterns\": \"|\".join(r.get(\"patterns\", [])),\n            \"category\": r.get(\"category\", \"\"),\n            \"message\": r.get(\"message\", \"\")[:100]\n        })\n\nprint(\"Exported to green_commits.csv\")\n</code></pre>"},{"location":"examples/pipeline/#export-to-latex-table","title":"Export to LaTeX Table","text":"<pre><code>def to_latex_table(pattern_counts, top_n=10):\n    \"\"\"Generate LaTeX table from pattern counts.\"\"\"\n    lines = [\n        r\"\\begin{table}[h]\",\n        r\"\\centering\",\n        r\"\\caption{Top Green Software Patterns}\",\n        r\"\\begin{tabular}{lrr}\",\n        r\"\\toprule\",\n        r\"Pattern &amp; Count &amp; Percentage \\\\\",\n        r\"\\midrule\",\n    ]\n\n    total = sum(pattern_counts.values())\n    for pattern, count in pattern_counts.most_common(top_n):\n        pct = 100 * count / total\n        lines.append(f\"{pattern} &amp; {count} &amp; {pct:.1f}\\\\% \\\\\\\\\")\n\n    lines.extend([\n        r\"\\bottomrule\",\n        r\"\\end{tabular}\",\n        r\"\\end{table}\",\n    ])\n\n    return \"\\n\".join(lines)\n\nlatex = to_latex_table(patterns)\nprint(latex)\n</code></pre>"},{"location":"examples/pipeline/#output-files","title":"Output Files","text":"<p>After running the pipeline:</p> <pre><code>data/\n\u251c\u2500\u2500 repositories.json      # Fetched repository metadata\n\u251c\u2500\u2500 commits.json           # Extracted commits\n\u251c\u2500\u2500 analysis_results.json  # Per-commit analysis\n\u251c\u2500\u2500 aggregated_statistics.json  # Statistics &amp; trends\n\u2514\u2500\u2500 green_analysis.md      # Markdown report\n</code></pre>"},{"location":"examples/pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Examples - Simple usage patterns</li> <li>CLI Reference - All commands</li> <li>Configuration - All options</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>GreenMining can be configured via environment variables, <code>.env</code> files, or programmatically.</p>"},{"location":"getting-started/configuration/#configuration-methods","title":"Configuration Methods","text":""},{"location":"getting-started/configuration/#method-1-environment-variables","title":"Method 1: Environment Variables","text":"<p>Set variables directly in your shell:</p> <pre><code>export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\nexport MAX_REPOS=50\nexport MIN_STARS=500\nexport ENABLE_TEMPORAL=true\n</code></pre>"},{"location":"getting-started/configuration/#method-2-env-file","title":"Method 2: .env File","text":"<p>Create a <code>.env</code> file in your project directory:</p> <pre><code># Required\nGITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n\n# Repository Fetching\nMAX_REPOS=100\nMIN_STARS=100\nSUPPORTED_LANGUAGES=Python,Java,Go,JavaScript,TypeScript\nSEARCH_KEYWORDS=microservices\n\n# Commit Extraction\nCOMMITS_PER_REPO=1000\nEXCLUDE_MERGE_COMMITS=true\nEXCLUDE_BOT_COMMITS=true\n\n# Analysis Features\nENABLE_DIFF_ANALYSIS=false\nBATCH_SIZE=10\n\n# Temporal Analysis\nENABLE_TEMPORAL=true\nTEMPORAL_GRANULARITY=quarter\nENABLE_STATS=true\n\n# Output\nOUTPUT_DIR=./data\nREPORT_FORMAT=markdown\n</code></pre>"},{"location":"getting-started/configuration/#method-3-python-config-object","title":"Method 3: Python Config Object","text":"<pre><code>from greenmining.config import Config\n\nconfig = Config()\n\n# Override settings\nconfig.MAX_REPOS = 50\nconfig.COMMITS_PER_REPO = 200\nconfig.MIN_STARS = 500\n</code></pre>"},{"location":"getting-started/configuration/#configuration-options-reference","title":"Configuration Options Reference","text":""},{"location":"getting-started/configuration/#github-api","title":"GitHub API","text":"Option Type Default Description <code>GITHUB_TOKEN</code> string (required) GitHub personal access token"},{"location":"getting-started/configuration/#repository-fetching","title":"Repository Fetching","text":"Option Type Default Description <code>MAX_REPOS</code> int 100 Maximum repositories to fetch <code>MIN_STARS</code> int 100 Minimum GitHub stars required <code>SUPPORTED_LANGUAGES</code> list Python,Java,Go,... Language filter (comma-separated) <code>SEARCH_KEYWORDS</code> string microservices Search keywords for GitHub API"},{"location":"getting-started/configuration/#url-analysis","title":"URL Analysis","text":"Option Type Default Description <code>REPOSITORY_URLS</code> list [] List of repository URLs to analyze <code>CLONE_PATH</code> string /tmp/greenmining_repos Directory for cloning repositories <code>CLEANUP_AFTER_ANALYSIS</code> bool true Delete cloned repos after analysis"},{"location":"getting-started/configuration/#commit-extraction","title":"Commit Extraction","text":"Option Type Default Description <code>COMMITS_PER_REPO</code> int 1000 Maximum commits per repository <code>EXCLUDE_MERGE_COMMITS</code> bool true Skip merge commits <code>EXCLUDE_BOT_COMMITS</code> bool true Skip bot commits <code>MIN_MESSAGE_LENGTH</code> int 10 Minimum commit message length"},{"location":"getting-started/configuration/#analysis-features","title":"Analysis Features","text":"Option Type Default Description <code>ENABLE_DIFF_ANALYSIS</code> bool false Analyze code diffs (slower) <code>BATCH_SIZE</code> int 10 Commits per batch"},{"location":"getting-started/configuration/#pydriller-options","title":"PyDriller Options","text":"Option Type Default Description <code>PROCESS_METRICS_ENABLED</code> bool true Compute process metrics <code>STRUCTURAL_METRICS_ENABLED</code> bool true Compute structural metrics <code>DMM_ENABLED</code> bool true Delta Maintainability Model"},{"location":"getting-started/configuration/#statistical-analysis","title":"Statistical Analysis","text":"Option Type Default Description <code>ENABLE_STATS</code> bool false Enable statistical analysis <code>ENABLE_TEMPORAL</code> bool false Enable temporal trend analysis <code>TEMPORAL_GRANULARITY</code> string quarter day/week/month/quarter/year"},{"location":"getting-started/configuration/#energy-measurement","title":"Energy Measurement","text":"Option Type Default Description <code>ENERGY_ENABLED</code> bool false Enable energy measurement <code>ENERGY_BACKEND</code> string rapl rapl, codecarbon, or cpu_meter <code>CARBON_TRACKING</code> bool false Track carbon emissions <code>COUNTRY_ISO</code> string USA Country for carbon calculations"},{"location":"getting-started/configuration/#output-configuration","title":"Output Configuration","text":"Option Type Default Description <code>OUTPUT_DIR</code> string ./data Output directory path <code>REPOS_FILE</code> string repositories.json Repository data filename <code>COMMITS_FILE</code> string commits.json Commits data filename <code>ANALYSIS_FILE</code> string analysis_results.json Analysis results filename <code>STATS_FILE</code> string aggregated_statistics.json Statistics filename <code>REPORT_FILE</code> string green_analysis.md Report filename"},{"location":"getting-started/configuration/#yaml-configuration","title":"YAML Configuration","text":"<p>For advanced configuration, use a YAML file:</p> <p>config/config.yaml:</p> <pre><code># GreenMining Configuration\nversion: \"2.0\"\n\nsearch:\n  keywords: [\"microservices\", \"kubernetes\"]\n  languages: [\"Python\", \"Go\", \"Java\"]\n  min_stars: 100\n  max_repos: 100\n\n  temporal:\n    created_after: \"2020-01-01\"\n    created_before: \"2025-12-31\"\n    pushed_after: \"2023-01-01\"\n\ndetection:\n  methods:\n    keyword:\n      enabled: true\n      confidence_weight: 1.0\n\n    diff_analysis:\n      enabled: true\n      confidence_weight: 0.8\n\nanalysis:\n  statistical:\n    enabled: true\n    methods:\n      - chi_square\n      - correlation_analysis\n      - temporal_trends\n      - effect_sizes\n</code></pre>"},{"location":"getting-started/configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>Configuration values are loaded in this order (later overrides earlier):</p> <ol> <li>Default values in <code>Config</code> class</li> <li>YAML config file if specified</li> <li><code>.env</code> file in current directory</li> <li>Environment variables</li> <li>Command-line arguments</li> </ol>"},{"location":"getting-started/configuration/#validating-configuration","title":"Validating Configuration","text":"<p>Check current configuration:</p> <pre><code>greenmining status\n</code></pre> <p>Output:</p> <pre><code>GreenMining Configuration\n=========================\nGITHUB_TOKEN: ***configured***\nMAX_REPOS: 100\nCOMMITS_PER_REPO: 1000\nOUTPUT_DIR: data\nENABLE_TEMPORAL: True\nTEMPORAL_GRANULARITY: quarter\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Commands - Use configuration with CLI</li> <li>Python API - Programmatic configuration</li> <li>Config Options Reference - Full reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers all methods to install GreenMining.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.9 or higher</li> <li>Operating System: Linux, macOS, or Windows</li> <li>GitHub Token: Required for repository fetching (optional for URL analysis)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-pip-recommended","title":"Method 1: pip (Recommended)","text":"<p>Install from PyPI:</p> <pre><code>pip install greenmining\n</code></pre> <p>Verify installation:</p> <pre><code>greenmining --version\n# Output: greenmining, version 1.0.3\n</code></pre>"},{"location":"getting-started/installation/#method-2-from-source","title":"Method 2: From Source","text":"<p>Clone and install for development:</p> <pre><code># Clone repository\ngit clone https://github.com/adam-bouafia/greenmining.git\ncd greenmining\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#method-3-docker","title":"Method 3: Docker","text":"<p>Run using Docker:</p> <pre><code># Pull image\ndocker pull adambouafia/greenmining:latest\n\n# Run with mounted data directory\ndocker run -v $(pwd)/data:/app/data \\\n           -e GITHUB_TOKEN=your_token \\\n           adambouafia/greenmining:latest --help\n\n# Run full pipeline\ndocker run -v $(pwd)/data:/app/data \\\n           -e GITHUB_TOKEN=your_token \\\n           adambouafia/greenmining:latest pipeline --max-repos 10\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>GreenMining automatically installs these dependencies:</p> Package Purpose <code>click&gt;=8.0</code> CLI framework <code>PyGithub&gt;=1.55</code> GitHub API access <code>pydriller&gt;=2.0</code> Repository mining <code>scipy&gt;=1.9</code> Statistical analysis <code>numpy&gt;=1.21</code> Numerical operations <code>python-dotenv</code> Environment variable loading <code>tqdm</code> Progress bars"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For energy measurement features:</p> <pre><code># CodeCarbon support\npip install codecarbon\n\n# Full development environment\npip install greenmining[dev]\n</code></pre>"},{"location":"getting-started/installation/#github-token-setup","title":"GitHub Token Setup","text":"<p>A GitHub personal access token is required for the <code>fetch</code> and <code>pipeline</code> commands.</p>"},{"location":"getting-started/installation/#creating-a-token","title":"Creating a Token","text":"<ol> <li>Go to GitHub Settings \u2192 Developer settings \u2192 Personal access tokens</li> <li>Click \"Generate new token (classic)\"</li> <li>Select scopes:<ul> <li><code>repo</code> (for private repositories)</li> <li><code>public_repo</code> (for public repositories only)</li> </ul> </li> <li>Copy the generated token</li> </ol>"},{"location":"getting-started/installation/#configuring-the-token","title":"Configuring the Token","text":"<p>Option 1: Environment Variable</p> <pre><code>export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre> <p>Option 2: .env File</p> <p>Create a <code>.env</code> file in your project directory:</p> <pre><code>GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre> <p>Option 3: Command Line</p> <p>Pass directly to commands (not recommended for security):</p> <pre><code>greenmining fetch --token ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>Run the following to verify everything works:</p> <pre><code># Check version\ngreenmining --version\n\n# Check available commands\ngreenmining --help\n\n# Test pattern detection\npython -c \"from greenmining import GSF_PATTERNS; print(f'{len(GSF_PATTERNS)} patterns loaded')\"\n# Output: 122 patterns loaded\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>Issue: <code>ModuleNotFoundError: No module named 'greenmining'</code></p> <p>Solution: Ensure you've activated your virtual environment and installed the package:</p> <pre><code>source venv/bin/activate\npip install greenmining\n</code></pre> <p>Issue: <code>GITHUB_TOKEN not set</code></p> <p>Solution: Set the environment variable or create a <code>.env</code> file:</p> <pre><code>export GITHUB_TOKEN=your_token_here\n</code></pre> <p>Issue: <code>Rate limit exceeded</code></p> <p>Solution: GitHub API has rate limits. Either:</p> <ul> <li>Wait for the rate limit to reset (1 hour)</li> <li>Use an authenticated token for higher limits</li> <li>Reduce <code>--max-repos</code> parameter</li> </ul> <p>Issue: <code>Permission denied</code> on RAPL energy measurement</p> <p>Solution: RAPL requires root access or specific permissions:</p> <pre><code># Option 1: Run with sudo\nsudo greenmining analyze-url https://github.com/user/repo --energy\n\n# Option 2: Grant read access to energy files\nsudo chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Run your first analysis</li> <li>Configuration - Customize GreenMining settings</li> <li>CLI Commands - Complete command reference</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with GreenMining in 5 minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>GitHub token (for <code>fetch</code>/<code>pipeline</code> commands)</li> <li>GreenMining installed (<code>pip install greenmining</code>)</li> </ul>"},{"location":"getting-started/quickstart/#option-1-analyze-by-github-url-fastest","title":"Option 1: Analyze by GitHub URL (Fastest)","text":"<p>Analyze any public GitHub repository directly:</p> <pre><code># Analyze a single repository\ngreenmining analyze-url https://github.com/pallets/flask\n\n# Limit commits for faster testing\ngreenmining analyze-url https://github.com/pallets/flask --max-commits 100\n</code></pre> <p>Output:</p> <pre><code>Analyzing repository: https://github.com/pallets/flask\nCloning to: /tmp/greenmining_repos/flask\nProcessing commits: 100/100 [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588] 100%\n\n============================================================\nANALYSIS RESULTS\n============================================================\nRepository: pallets/flask\nTotal commits: 100\nGreen-aware commits: 23 (23.0%)\n\nTop Patterns:\n  Cache Static Data: 8 commits\n  Use Async Instead of Sync: 5 commits\n  Lazy Loading: 4 commits\n</code></pre>"},{"location":"getting-started/quickstart/#option-2-full-pipeline-comprehensive","title":"Option 2: Full Pipeline (Comprehensive)","text":"<p>Run the complete analysis pipeline:</p> <pre><code># Set your GitHub token\nexport GITHUB_TOKEN=ghp_your_token_here\n\n# Run pipeline on microservices repositories\ngreenmining pipeline --max-repos 5 --max-commits 50\n</code></pre> <p>This executes:</p> <ol> <li>Fetch - Find repositories matching criteria</li> <li>Extract - Get commit history from each repo</li> <li>Analyze - Detect green patterns in commits</li> <li>Aggregate - Calculate statistics</li> <li>Report - Generate Markdown report</li> </ol>"},{"location":"getting-started/quickstart/#option-3-python-api","title":"Option 3: Python API","text":"<p>Use GreenMining programmatically:</p> <pre><code>from greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\n\n# Check pattern count\nprint(f\"Loaded {len(GSF_PATTERNS)} GSF patterns\")\n# Output: Loaded 122 GSF patterns\n\n# Test green awareness detection\nmessages = [\n    \"Optimize Redis caching for better performance\",\n    \"Fix typo in README\",\n    \"Enable gzip compression for API responses\",\n    \"Update dependencies to latest versions\",\n]\n\nfor msg in messages:\n    is_green = is_green_aware(msg)\n    patterns = get_pattern_by_keywords(msg) if is_green else []\n    status = \"\ud83c\udf31\" if is_green else \"  \"\n    print(f\"{status} {msg[:50]}\")\n    if patterns:\n        print(f\"   \u2192 Patterns: {patterns}\")\n</code></pre> <p>Output:</p> <pre><code>\ud83c\udf31 Optimize Redis caching for better performance\n   \u2192 Patterns: ['Cache Static Data']\n   Fix typo in README\n\ud83c\udf31 Enable gzip compression for API responses\n   \u2192 Patterns: ['Compress Transmitted Data', 'Enable Text Compression']\n   Update dependencies to latest versions\n</code></pre>"},{"location":"getting-started/quickstart/#step-by-step-commands","title":"Step-by-Step Commands","text":""},{"location":"getting-started/quickstart/#step-1-fetch-repositories","title":"Step 1: Fetch Repositories","text":"<pre><code>greenmining fetch --max-repos 10 --min-stars 100 --keywords \"kubernetes\"\n</code></pre> <p>Options:</p> Option Default Description <code>--max-repos</code> 100 Maximum repositories to fetch <code>--min-stars</code> 100 Minimum star count <code>--languages</code> Python,Java,Go,... Filter by language <code>--keywords</code> microservices Search keywords"},{"location":"getting-started/quickstart/#step-2-extract-commits","title":"Step 2: Extract Commits","text":"<pre><code>greenmining extract --max-commits 100\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-analyze-for-patterns","title":"Step 3: Analyze for Patterns","text":"<pre><code>greenmining analyze --enable-diff-analysis\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-aggregate-results","title":"Step 4: Aggregate Results","text":"<pre><code>greenmining aggregate --enable-temporal --temporal-granularity quarter --enable-stats\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-generate-report","title":"Step 5: Generate Report","text":"<pre><code>greenmining report\n</code></pre>"},{"location":"getting-started/quickstart/#output-files","title":"Output Files","text":"<p>All outputs are saved to the <code>data/</code> directory:</p> File Description <code>repositories.json</code> Fetched repository metadata <code>commits.json</code> Extracted commit data <code>analysis_results.json</code> Pattern detection results <code>aggregated_statistics.json</code> Summary statistics <code>aggregated_data.json</code> Full aggregated data <code>green_analysis.md</code> Markdown report"},{"location":"getting-started/quickstart/#quick-test-script","title":"Quick Test Script","text":"<p>Create <code>test_greenmining.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Quick test of GreenMining functionality.\"\"\"\n\nfrom greenmining import GSF_PATTERNS, GREEN_KEYWORDS, is_green_aware, get_pattern_by_keywords\n\n# Test 1: Check patterns loaded\nprint(f\"\u2713 Loaded {len(GSF_PATTERNS)} GSF patterns\")\nprint(f\"\u2713 Loaded {len(GREEN_KEYWORDS)} green keywords\")\n\n# Test 2: Get categories\ncategories = set(p[\"category\"] for p in GSF_PATTERNS.values())\nprint(f\"\u2713 Categories: {', '.join(sorted(categories))}\")\n\n# Test 3: Pattern detection\ntest_msg = \"Implement Redis caching to reduce database load\"\nis_green = is_green_aware(test_msg)\npatterns = get_pattern_by_keywords(test_msg)\n\nprint(f\"\u2713 Test message: '{test_msg}'\")\nprint(f\"  Green-aware: {is_green}\")\nprint(f\"  Patterns: {patterns}\")\n\nprint(\"\\n\u2705 All tests passed!\")\n</code></pre> <p>Run it:</p> <pre><code>python test_greenmining.py\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Customize settings</li> <li>CLI Commands - Full command reference</li> <li>Python API - Programmatic usage</li> <li>GSF Patterns - All 122 patterns</li> </ul>"},{"location":"reference/config-options/","title":"Configuration Options Reference","text":"<p>Complete reference for all GreenMining configuration options.</p>"},{"location":"reference/config-options/#configuration-methods","title":"Configuration Methods","text":"<p>GreenMining supports multiple configuration methods:</p> <ol> <li>Environment variables - <code>export OPTION=value</code></li> <li><code>.env</code> file - Key-value pairs in project root</li> <li>Python Config object - Programmatic configuration</li> <li>CLI arguments - Command-line overrides</li> </ol>"},{"location":"reference/config-options/#precedence-order","title":"Precedence Order","text":"<p>Later sources override earlier ones:</p> <ol> <li>Default values (lowest priority)</li> <li><code>.env</code> file</li> <li>Environment variables</li> <li>CLI arguments (highest priority)</li> </ol>"},{"location":"reference/config-options/#github-api-options","title":"GitHub API Options","text":""},{"location":"reference/config-options/#github_token","title":"GITHUB_TOKEN","text":"<p>GitHub personal access token for API access.</p> Property Value Type string Default (none - required) Environment <code>GITHUB_TOKEN</code> Required Yes (for fetch/pipeline commands) <pre><code>export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre>"},{"location":"reference/config-options/#repository-fetching-options","title":"Repository Fetching Options","text":""},{"location":"reference/config-options/#max_repos","title":"MAX_REPOS","text":"<p>Maximum number of repositories to fetch from GitHub.</p> Property Value Type integer Default 100 Environment <code>MAX_REPOS</code> CLI <code>--max-repos</code> <pre><code>export MAX_REPOS=50\ngreenmining fetch --max-repos 50\n</code></pre>"},{"location":"reference/config-options/#min_stars","title":"MIN_STARS","text":"<p>Minimum GitHub star count required for repositories.</p> Property Value Type integer Default 100 Environment <code>MIN_STARS</code> CLI <code>--min-stars</code> <pre><code>export MIN_STARS=500\ngreenmining fetch --min-stars 500\n</code></pre>"},{"location":"reference/config-options/#supported_languages","title":"SUPPORTED_LANGUAGES","text":"<p>Programming languages to filter repositories.</p> Property Value Type list (comma-separated) Default Python,Java,Go,JavaScript,TypeScript,C#,Rust Environment <code>SUPPORTED_LANGUAGES</code> CLI <code>--languages</code> <pre><code>export SUPPORTED_LANGUAGES=\"Python,Go,Rust\"\ngreenmining fetch --languages \"Python,Go,Rust\"\n</code></pre>"},{"location":"reference/config-options/#search_keywords","title":"SEARCH_KEYWORDS","text":"<p>Keywords for GitHub repository search.</p> Property Value Type string Default microservices Environment <code>SEARCH_KEYWORDS</code> CLI <code>--keywords</code> <pre><code>export SEARCH_KEYWORDS=\"kubernetes cloud-native\"\ngreenmining fetch --keywords \"kubernetes cloud-native\"\n</code></pre>"},{"location":"reference/config-options/#url-analysis-options","title":"URL Analysis Options","text":""},{"location":"reference/config-options/#repository_urls","title":"REPOSITORY_URLS","text":"<p>List of repository URLs to analyze directly.</p> Property Value Type list Default [] Environment <code>REPOSITORY_URLS</code> (comma-separated) CLI <code>--urls</code> <pre><code>export REPOSITORY_URLS=\"https://github.com/org/repo1,https://github.com/org/repo2\"\n</code></pre>"},{"location":"reference/config-options/#clone_path","title":"CLONE_PATH","text":"<p>Directory where repositories are cloned for analysis.</p> Property Value Type string (path) Default /tmp/greenmining_repos Environment <code>CLONE_PATH</code> CLI <code>--clone-path</code> <pre><code>export CLONE_PATH=/var/data/greenmining\ngreenmining analyze-url https://github.com/org/repo --clone-path /var/data/greenmining\n</code></pre>"},{"location":"reference/config-options/#cleanup_after_analysis","title":"CLEANUP_AFTER_ANALYSIS","text":"<p>Whether to delete cloned repositories after analysis.</p> Property Value Type boolean Default true Environment <code>CLEANUP_AFTER_ANALYSIS</code> CLI <code>--no-cleanup</code> <pre><code>export CLEANUP_AFTER_ANALYSIS=false\ngreenmining analyze-url https://github.com/org/repo --no-cleanup\n</code></pre>"},{"location":"reference/config-options/#commit-extraction-options","title":"Commit Extraction Options","text":""},{"location":"reference/config-options/#commits_per_repo","title":"COMMITS_PER_REPO","text":"<p>Maximum commits to extract per repository.</p> Property Value Type integer Default 1000 Environment <code>COMMITS_PER_REPO</code> CLI <code>--max-commits</code> <pre><code>export COMMITS_PER_REPO=500\ngreenmining extract --max-commits 500\n</code></pre>"},{"location":"reference/config-options/#exclude_merge_commits","title":"EXCLUDE_MERGE_COMMITS","text":"<p>Skip merge commits during extraction.</p> Property Value Type boolean Default true Environment <code>EXCLUDE_MERGE_COMMITS</code> CLI <code>--no-exclude-merges</code> <pre><code>export EXCLUDE_MERGE_COMMITS=false\n</code></pre>"},{"location":"reference/config-options/#exclude_bot_commits","title":"EXCLUDE_BOT_COMMITS","text":"<p>Skip commits from bot accounts.</p> Property Value Type boolean Default true Environment <code>EXCLUDE_BOT_COMMITS</code> CLI <code>--no-exclude-bots</code> <pre><code>export EXCLUDE_BOT_COMMITS=false\n</code></pre>"},{"location":"reference/config-options/#min_message_length","title":"MIN_MESSAGE_LENGTH","text":"<p>Minimum commit message length to include.</p> Property Value Type integer Default 10 Environment <code>MIN_MESSAGE_LENGTH</code> <pre><code>export MIN_MESSAGE_LENGTH=20\n</code></pre>"},{"location":"reference/config-options/#analysis-options","title":"Analysis Options","text":""},{"location":"reference/config-options/#enable_diff_analysis","title":"ENABLE_DIFF_ANALYSIS","text":"<p>Enable code diff analysis for pattern detection.</p> Property Value Type boolean Default false Environment <code>ENABLE_DIFF_ANALYSIS</code> CLI <code>--enable-diff-analysis</code> <pre><code>export ENABLE_DIFF_ANALYSIS=true\ngreenmining analyze --enable-diff-analysis\n</code></pre>"},{"location":"reference/config-options/#batch_size","title":"BATCH_SIZE","text":"<p>Number of commits to process per batch.</p> Property Value Type integer Default 10 Environment <code>BATCH_SIZE</code> CLI <code>--batch-size</code> <pre><code>export BATCH_SIZE=50\ngreenmining analyze --batch-size 50\n</code></pre>"},{"location":"reference/config-options/#pydriller-options","title":"PyDriller Options","text":""},{"location":"reference/config-options/#process_metrics_enabled","title":"PROCESS_METRICS_ENABLED","text":"<p>Compute process metrics (code churn, change set, etc.).</p> Property Value Type boolean Default true Environment <code>PROCESS_METRICS_ENABLED</code> <pre><code>export PROCESS_METRICS_ENABLED=true\n</code></pre>"},{"location":"reference/config-options/#structural_metrics_enabled","title":"STRUCTURAL_METRICS_ENABLED","text":"<p>Compute structural metrics (complexity, lines of code).</p> Property Value Type boolean Default true Environment <code>STRUCTURAL_METRICS_ENABLED</code> <pre><code>export STRUCTURAL_METRICS_ENABLED=true\n</code></pre>"},{"location":"reference/config-options/#dmm_enabled","title":"DMM_ENABLED","text":"<p>Enable Delta Maintainability Model metrics.</p> Property Value Type boolean Default true Environment <code>DMM_ENABLED</code> <pre><code>export DMM_ENABLED=true\n</code></pre>"},{"location":"reference/config-options/#statistical-analysis-options","title":"Statistical Analysis Options","text":""},{"location":"reference/config-options/#enable_stats","title":"ENABLE_STATS","text":"<p>Enable statistical analysis (correlations, effect sizes).</p> Property Value Type boolean Default false Environment <code>ENABLE_STATS</code> CLI <code>--enable-stats</code> <pre><code>export ENABLE_STATS=true\ngreenmining aggregate --enable-stats\n</code></pre>"},{"location":"reference/config-options/#enable_temporal","title":"ENABLE_TEMPORAL","text":"<p>Enable temporal trend analysis.</p> Property Value Type boolean Default false Environment <code>ENABLE_TEMPORAL</code> CLI <code>--enable-temporal</code> <pre><code>export ENABLE_TEMPORAL=true\ngreenmining aggregate --enable-temporal\n</code></pre>"},{"location":"reference/config-options/#temporal_granularity","title":"TEMPORAL_GRANULARITY","text":"<p>Time period granularity for temporal analysis.</p> Property Value Type string Default quarter Options day, week, month, quarter, year Environment <code>TEMPORAL_GRANULARITY</code> CLI <code>--temporal-granularity</code> <pre><code>export TEMPORAL_GRANULARITY=month\ngreenmining aggregate --temporal-granularity month\n</code></pre>"},{"location":"reference/config-options/#energy-measurement-options","title":"Energy Measurement Options","text":""},{"location":"reference/config-options/#energy_enabled","title":"ENERGY_ENABLED","text":"<p>Enable energy measurement during analysis.</p> Property Value Type boolean Default false Environment <code>ENERGY_ENABLED</code> CLI <code>--energy</code> <pre><code>export ENERGY_ENABLED=true\ngreenmining analyze-url https://github.com/org/repo --energy\n</code></pre>"},{"location":"reference/config-options/#energy_backend","title":"ENERGY_BACKEND","text":"<p>Energy measurement backend to use.</p> Property Value Type string Default rapl Options rapl, codecarbon, cpu_meter Environment <code>ENERGY_BACKEND</code> CLI <code>--energy-backend</code> <pre><code>export ENERGY_BACKEND=codecarbon\ngreenmining analyze-url https://github.com/org/repo --energy-backend codecarbon\n</code></pre>"},{"location":"reference/config-options/#carbon_tracking","title":"CARBON_TRACKING","text":"<p>Enable carbon emissions tracking (CodeCarbon only).</p> Property Value Type boolean Default false Environment <code>CARBON_TRACKING</code> <pre><code>export CARBON_TRACKING=true\n</code></pre>"},{"location":"reference/config-options/#country_iso","title":"COUNTRY_ISO","text":"<p>ISO country code for carbon intensity calculations.</p> Property Value Type string Default USA Environment <code>COUNTRY_ISO</code> <pre><code>export COUNTRY_ISO=DEU  # Germany\n</code></pre>"},{"location":"reference/config-options/#output-options","title":"Output Options","text":""},{"location":"reference/config-options/#output_dir","title":"OUTPUT_DIR","text":"<p>Directory for output files.</p> Property Value Type string (path) Default ./data Environment <code>OUTPUT_DIR</code> <pre><code>export OUTPUT_DIR=/var/results/greenmining\n</code></pre>"},{"location":"reference/config-options/#repos_file","title":"REPOS_FILE","text":"<p>Filename for repository data.</p> Property Value Type string Default repositories.json Environment <code>REPOS_FILE</code>"},{"location":"reference/config-options/#commits_file","title":"COMMITS_FILE","text":"<p>Filename for commit data.</p> Property Value Type string Default commits.json Environment <code>COMMITS_FILE</code>"},{"location":"reference/config-options/#analysis_file","title":"ANALYSIS_FILE","text":"<p>Filename for analysis results.</p> Property Value Type string Default analysis_results.json Environment <code>ANALYSIS_FILE</code>"},{"location":"reference/config-options/#stats_file","title":"STATS_FILE","text":"<p>Filename for aggregated statistics.</p> Property Value Type string Default aggregated_statistics.json Environment <code>STATS_FILE</code>"},{"location":"reference/config-options/#report_file","title":"REPORT_FILE","text":"<p>Filename for Markdown report.</p> Property Value Type string Default green_analysis.md Environment <code>REPORT_FILE</code>"},{"location":"reference/config-options/#sample-env-file","title":"Sample .env File","text":"<p>Complete <code>.env</code> configuration:</p> <pre><code># Required\nGITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n\n# Repository Fetching\nMAX_REPOS=100\nMIN_STARS=100\nSUPPORTED_LANGUAGES=Python,Java,Go,JavaScript,TypeScript\nSEARCH_KEYWORDS=microservices\n\n# URL Analysis\nCLONE_PATH=/tmp/greenmining_repos\nCLEANUP_AFTER_ANALYSIS=true\n\n# Commit Extraction\nCOMMITS_PER_REPO=1000\nEXCLUDE_MERGE_COMMITS=true\nEXCLUDE_BOT_COMMITS=true\nMIN_MESSAGE_LENGTH=10\n\n# Analysis\nENABLE_DIFF_ANALYSIS=false\nBATCH_SIZE=10\n\n# PyDriller\nPROCESS_METRICS_ENABLED=true\nSTRUCTURAL_METRICS_ENABLED=true\nDMM_ENABLED=true\n\n# Statistical Analysis\nENABLE_STATS=true\nENABLE_TEMPORAL=true\nTEMPORAL_GRANULARITY=quarter\n\n# Energy Measurement\nENERGY_ENABLED=false\nENERGY_BACKEND=rapl\nCARBON_TRACKING=false\nCOUNTRY_ISO=USA\n\n# Output\nOUTPUT_DIR=./data\nREPORT_FILE=green_analysis.md\n</code></pre>"},{"location":"reference/config-options/#python-config-class","title":"Python Config Class","text":"<pre><code>from greenmining.config import Config\n\nconfig = Config()\n\n# Access all options\nprint(f\"MAX_REPOS: {config.MAX_REPOS}\")\nprint(f\"COMMITS_PER_REPO: {config.COMMITS_PER_REPO}\")\nprint(f\"SUPPORTED_LANGUAGES: {config.SUPPORTED_LANGUAGES}\")\nprint(f\"CLONE_PATH: {config.CLONE_PATH}\")\nprint(f\"ENERGY_ENABLED: {config.ENERGY_ENABLED}\")\nprint(f\"ENERGY_BACKEND: {config.ENERGY_BACKEND}\")\n\n# Override at runtime\nconfig.MAX_REPOS = 50\nconfig.ENABLE_TEMPORAL = True\n</code></pre>"},{"location":"reference/config-options/#next-steps","title":"Next Steps","text":"<ul> <li>GSF Patterns - Pattern reference</li> <li>CLI Commands - Command-line usage</li> <li>Python API - Programmatic configuration</li> </ul>"},{"location":"reference/models/","title":"Data Models Reference","text":"<p>Reference for GreenMining data models and structures.</p>"},{"location":"reference/models/#core-models","title":"Core Models","text":""},{"location":"reference/models/#repository","title":"Repository","text":"<p>Represents a GitHub repository.</p> <pre><code>from greenmining.models import Repository\n\nrepo = Repository(\n    repo_id=12345,\n    name=\"flask\",\n    full_name=\"pallets/flask\",\n    owner=\"pallets\",\n    url=\"https://github.com/pallets/flask\",\n    clone_url=\"https://github.com/pallets/flask.git\",\n    stars=65000,\n    forks=16000,\n    watchers=2500,\n    open_issues=50,\n    language=\"Python\",\n    last_updated=\"2024-01-15T10:00:00Z\",\n    created_at=\"2010-04-06T12:00:00Z\",\n    description=\"The Python micro framework for building web applications.\",\n    main_branch=\"main\"\n)\n</code></pre>"},{"location":"reference/models/#fields","title":"Fields","text":"Field Type Description <code>repo_id</code> int GitHub repository ID <code>name</code> str Repository name <code>full_name</code> str Full name (owner/repo) <code>owner</code> str Repository owner <code>url</code> str GitHub URL <code>clone_url</code> str Git clone URL <code>stars</code> int Star count <code>forks</code> int Fork count <code>watchers</code> int Watcher count <code>open_issues</code> int Open issue count <code>language</code> str Primary language <code>last_updated</code> str Last update timestamp <code>created_at</code> str Creation timestamp <code>description</code> str Repository description <code>main_branch</code> str Default branch name"},{"location":"reference/models/#commit","title":"Commit","text":"<p>Represents a Git commit.</p> <pre><code>from greenmining.models import Commit\n\ncommit = Commit(\n    sha=\"abc123def456789\",\n    message=\"Implement Redis caching for user sessions\",\n    author=\"developer\",\n    date=\"2024-01-15T10:30:00Z\",\n    repository=\"pallets/flask\"\n)\n</code></pre>"},{"location":"reference/models/#fields_1","title":"Fields","text":"Field Type Description <code>sha</code> str Commit SHA hash <code>message</code> str Commit message <code>author</code> str Author username <code>date</code> str Commit timestamp (ISO 8601) <code>repository</code> str Repository full name"},{"location":"reference/models/#analysisresult","title":"AnalysisResult","text":"<p>Represents the analysis result for a commit.</p> <pre><code>from greenmining.models import AnalysisResult\n\nresult = AnalysisResult(\n    commit=commit,\n    green_aware=True,\n    patterns=[\"Cache Static Data\"],\n    confidence=0.85\n)\n</code></pre>"},{"location":"reference/models/#fields_2","title":"Fields","text":"Field Type Description <code>commit</code> Commit The analyzed commit <code>green_aware</code> bool Whether commit is green-aware <code>patterns</code> list[str] Matched pattern names <code>confidence</code> float Detection confidence (0-1)"},{"location":"reference/models/#analysis-output-structures","title":"Analysis Output Structures","text":""},{"location":"reference/models/#commit-analysis-dictionary","title":"Commit Analysis Dictionary","text":"<p>When commits are analyzed, they're returned as dictionaries:</p> <pre><code>{\n    \"sha\": \"abc123def456789\",\n    \"message\": \"Implement Redis caching for user sessions\",\n    \"author\": \"developer\",\n    \"date\": \"2024-01-15T10:30:00Z\",\n    \"repository\": \"pallets/flask\",\n    \"green_aware\": True,\n    \"patterns\": [\"Cache Static Data\"],\n    \"category\": \"caching\",\n    \"keywords_matched\": [\"redis\", \"caching\"],\n    \"confidence\": 0.85,\n\n    # If diff analysis enabled\n    \"diff_patterns\": [\"caching\"],\n    \"files_modified\": 3,\n\n    # If PyDriller metrics enabled\n    \"insertions\": 45,\n    \"deletions\": 12,\n    \"dmm_unit_size\": 0.85,\n    \"dmm_unit_complexity\": 0.72,\n    \"dmm_unit_interfacing\": 0.90\n}\n</code></pre>"},{"location":"reference/models/#aggregated-statistics-structure","title":"Aggregated Statistics Structure","text":"<p>Output from <code>DataAggregator.aggregate()</code>:</p> <pre><code>{\n    \"summary\": {\n        \"total_commits\": 5000,\n        \"green_aware_count\": 1250,\n        \"green_aware_percentage\": 25.0,\n        \"total_repos\": 50,\n        \"repos_with_green_commits\": 42,\n        \"analysis_date\": \"2024-01-15T10:00:00Z\"\n    },\n\n    \"pattern_distribution\": {\n        \"Cache Static Data\": 320,\n        \"Use Async Instead of Sync\": 180,\n        \"Compress Transmitted Data\": 150,\n        \"Lazy Loading\": 120,\n        \"Optimize Database Queries\": 95\n    },\n\n    \"category_distribution\": {\n        \"cloud\": 450,\n        \"caching\": 320,\n        \"async\": 210,\n        \"web\": 180,\n        \"database\": 95\n    },\n\n    \"per_repo_stats\": [\n        {\n            \"repository\": \"pallets/flask\",\n            \"total_commits\": 200,\n            \"green_aware_count\": 47,\n            \"green_aware_percentage\": 23.5,\n            \"top_patterns\": [\"Cache Static Data\", \"Lazy Loading\"]\n        }\n    ],\n\n    \"per_language_stats\": {\n        \"Python\": {\n            \"total_commits\": 2000,\n            \"green_aware_count\": 520,\n            \"green_aware_percentage\": 26.0\n        }\n    },\n\n    # If enable_temporal=True\n    \"temporal_analysis\": {\n        \"periods\": [\n            {\n                \"period\": \"2024-Q1\",\n                \"commit_count\": 500,\n                \"green_count\": 125,\n                \"green_awareness_rate\": 0.25\n            }\n        ],\n        \"overall_trend\": {\n            \"direction\": \"increasing\",\n            \"significant\": True\n        }\n    },\n\n    # If enable_stats=True\n    \"statistics\": {\n        \"pattern_correlations\": {\n            \"top_positive_correlations\": [\n                {\n                    \"pattern1\": \"caching\",\n                    \"pattern2\": \"performance\",\n                    \"correlation\": 0.75\n                }\n            ]\n        },\n        \"effect_size\": {\n            \"green_vs_nongreen_patterns\": {\n                \"cohens_d\": 0.65,\n                \"magnitude\": \"medium\"\n            }\n        },\n        \"descriptive\": {\n            \"patterns_per_commit\": {\n                \"mean\": 2.3,\n                \"median\": 2.0,\n                \"std\": 1.1\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"reference/models/#url-analysis-result-structure","title":"URL Analysis Result Structure","text":"<p>Output from <code>LocalRepoAnalyzer.analyze_repository()</code>:</p> <pre><code>{\n    \"repository\": {\n        \"name\": \"flask\",\n        \"url\": \"https://github.com/pallets/flask\",\n        \"owner\": \"pallets\",\n        \"clone_path\": \"/tmp/greenmining_repos/flask\"\n    },\n\n    \"total_commits\": 200,\n    \"green_aware_count\": 47,\n    \"green_aware_percentage\": 23.5,\n\n    \"commits\": [\n        {\n            \"sha\": \"abc123...\",\n            \"message\": \"Optimize template caching\",\n            \"author\": \"developer\",\n            \"date\": \"2024-03-15T10:30:00\",\n            \"green_aware\": True,\n            \"patterns\": [\"Cache Static Data\"],\n\n            # PyDriller metrics\n            \"modified_files\": 3,\n            \"insertions\": 45,\n            \"deletions\": 12,\n            \"files\": [\"app.py\", \"cache.py\", \"config.py\"],\n\n            # DMM metrics\n            \"dmm_unit_size\": 0.85,\n            \"dmm_unit_complexity\": 0.72,\n            \"dmm_unit_interfacing\": 0.90\n        }\n    ],\n\n    \"pattern_distribution\": {\n        \"Cache Static Data\": 15,\n        \"Use Async Instead of Sync\": 12,\n        \"Lazy Loading\": 8\n    },\n\n    \"process_metrics\": {\n        \"change_set\": {\n            \"max\": 25,\n            \"avg\": 5.2\n        },\n        \"code_churn\": {\n            \"added\": 5000,\n            \"removed\": 2000\n        },\n        \"contributors_count\": 45,\n        \"commits_count\": 200\n    }\n}\n</code></pre>"},{"location":"reference/models/#energy-metrics-structures","title":"Energy Metrics Structures","text":""},{"location":"reference/models/#energymetrics","title":"EnergyMetrics","text":"<pre><code>from greenmining.energy.base import EnergyMetrics\n\n@dataclass\nclass EnergyMetrics:\n    energy_joules: float       # Total energy consumed\n    duration_seconds: float    # Measurement duration\n    average_power_watts: float # Average power draw\n    start_time: datetime       # Start timestamp\n    end_time: datetime         # End timestamp\n\n    # CodeCarbon specific\n    energy_kwh: float = 0.0    # Energy in kWh\n    emissions_kg: float = 0.0  # CO2 emissions\n</code></pre>"},{"location":"reference/models/#commitenergyprofile","title":"CommitEnergyProfile","text":"<pre><code>from greenmining.energy.base import CommitEnergyProfile\n\n@dataclass\nclass CommitEnergyProfile:\n    commit_sha: str            # Commit identifier\n    energy_joules: float       # Energy for this commit\n    duration_seconds: float    # Analysis duration\n    patterns_detected: list    # Patterns found\n    files_analyzed: int        # Files in commit\n</code></pre>"},{"location":"reference/models/#pattern-structure","title":"Pattern Structure","text":"<p>GSF patterns are stored as dictionaries:</p> <pre><code>{\n    \"cache_static_data\": {\n        \"name\": \"Cache Static Data\",\n        \"category\": \"cloud\",\n        \"keywords\": [\"cache\", \"caching\", \"static\", \"cdn\", \"redis\", \"memcache\"],\n        \"description\": \"Cache static content to reduce server load and network transfers\",\n        \"sci_impact\": \"Reduces energy by minimizing redundant compute and network operations\"\n    }\n}\n</code></pre>"},{"location":"reference/models/#fields_3","title":"Fields","text":"Field Type Description <code>name</code> str Human-readable pattern name <code>category</code> str Pattern category <code>keywords</code> list[str] Detection keywords <code>description</code> str Pattern description <code>sci_impact</code> str Impact on software carbon intensity"},{"location":"reference/models/#working-with-models","title":"Working with Models","text":""},{"location":"reference/models/#serialization","title":"Serialization","text":"<pre><code>import json\n\n# Convert to JSON\nresult_json = json.dumps(analysis_result, default=str)\n\n# Load from JSON\nwith open(\"results.json\") as f:\n    results = json.load(f)\n</code></pre>"},{"location":"reference/models/#filtering-results","title":"Filtering Results","text":"<pre><code># Get only green-aware commits\ngreen_commits = [c for c in commits if c[\"green_aware\"]]\n\n# Group by pattern\nfrom collections import defaultdict\nby_pattern = defaultdict(list)\nfor commit in green_commits:\n    for pattern in commit[\"patterns\"]:\n        by_pattern[pattern].append(commit)\n\n# Get top patterns\ntop_patterns = sorted(\n    by_pattern.items(), \n    key=lambda x: len(x[1]), \n    reverse=True\n)[:10]\n</code></pre>"},{"location":"reference/models/#aggregating-custom-metrics","title":"Aggregating Custom Metrics","text":"<pre><code>import statistics\n\n# Calculate custom statistics\ngreen_counts = [r[\"green_aware_count\"] for r in per_repo_stats]\navg_green = statistics.mean(green_counts)\nmedian_green = statistics.median(green_counts)\nstd_green = statistics.stdev(green_counts) if len(green_counts) &gt; 1 else 0\n</code></pre>"},{"location":"reference/models/#next-steps","title":"Next Steps","text":"<ul> <li>Python API - Working with models</li> <li>GSF Patterns - Pattern reference</li> <li>Configuration - All options</li> </ul>"},{"location":"reference/patterns/","title":"GSF Patterns Reference","text":"<p>Complete reference for all 122 Green Software Foundation patterns supported by GreenMining.</p>"},{"location":"reference/patterns/#overview","title":"Overview","text":"<p>GreenMining detects patterns from the Green Software Foundation catalog, organized into 15 categories.</p> Statistic Value Total Patterns 122 Categories 15 Keywords 321"},{"location":"reference/patterns/#pattern-categories","title":"Pattern Categories","text":""},{"location":"reference/patterns/#cloud-patterns-40","title":"Cloud Patterns (40+)","text":"<p>Patterns for cloud-native and infrastructure optimization.</p> Pattern Keywords Description Cache Static Data cache, caching, static, cdn, redis, memcache Cache static content to reduce server load Choose Region Closest region, closest, proximity, latency Deploy in regions closest to users Compress Stored Data compress, storage, gzip, zstd Compress data at rest Compress Transmitted Data compress, transmission, gzip, brotli Compress data before network transfer Containerize Workload container, docker, kubernetes, pod Use containers for resource efficiency Delete Unused Storage delete, remove, unused, cleanup Remove unused storage resources Encrypt What Is Necessary encrypt, tls, ssl, crypto Only encrypt data that needs protection Evaluate CPU Architectures cpu, arm, graviton, processor Consider ARM and efficient CPUs Use Service Mesh service mesh, istio, linkerd, envoy Optimize service-to-service communication TLS Termination tls termination, ssl offload Terminate TLS at edge Implement Stateless Design stateless, session, horizontal Design without server-side state Match SLO Requirements slo, sla, service level Don't over-engineer beyond SLO Match VM Utilization vm, instance, size, utilization Right-size VMs to workload Move to Cloud cloud, migrate, migration Leverage cloud efficiency Optimize Average Utilization utilization, optimize, average Increase resource utilization Optimize High Utilization high utilization, maximize Target high utilization levels Optimize Network Traffic network, traffic, optimize Reduce unnecessary network traffic Scale Down Idle Resources scale down, idle, shutdown Reduce resources when idle Scale Infrastructure scaling, autoscaling, scale Scale based on demand Terminate Unused Resources terminate, unused, cleanup Remove unused resources Use Reserved Instances reserved, spot, savings Use cost-efficient instance types Use Serverless serverless, lambda, functions Use serverless for variable workloads Use Spot Instances spot, preemptible, interruption Use spot instances for cost savings"},{"location":"reference/patterns/#web-patterns-15","title":"Web Patterns (15+)","text":"<p>Patterns for web application optimization.</p> Pattern Keywords Description Enable Text Compression gzip, brotli, deflate Compress text responses Lazy Loading lazy, defer, on-demand Load content only when needed Minify CSS/JS minify, minification, uglify Reduce asset file sizes Optimize Images image, webp, avif, srcset Use modern image formats Use CDN cdn, content delivery, edge Serve from edge locations Cache HTTP Responses cache-control, etag, expires Enable browser caching Reduce DOM Size dom, elements, optimize Minimize DOM complexity Use Service Workers service worker, pwa, offline Enable offline caching Preconnect Resources preconnect, preload, prefetch Hint browser to connect early Remove Unused CSS unused, purge, tree-shake Remove dead CSS code Optimize Fonts font, woff2, subset Use efficient font loading Reduce JavaScript javascript, bundle, split Minimize JS payload Use HTTP/2 http2, multiplexing Use modern HTTP protocol Enable Keep-Alive keep-alive, persistent Reuse HTTP connections"},{"location":"reference/patterns/#aiml-patterns-10","title":"AI/ML Patterns (10+)","text":"<p>Patterns for machine learning optimization.</p> Pattern Keywords Description Model Optimization model, optimize, prune Optimize model architecture Quantization quantize, int8, fp16 Reduce model precision Knowledge Distillation distillation, student, teacher Train smaller models Efficient Training training, efficient, epoch Optimize training process Batch Inference batch, inference, throughput Process predictions in batches Model Caching model cache, warm, preload Cache loaded models Feature Selection feature, select, reduce Use fewer features Early Stopping early stop, convergence Stop training when converged Mixed Precision mixed precision, amp Use mixed precision training Gradient Checkpointing checkpoint, gradient, memory Trade compute for memory"},{"location":"reference/patterns/#caching-patterns-8","title":"Caching Patterns (8)","text":"<p>Patterns for caching strategies.</p> Pattern Keywords Description Redis Caching redis, cache, memory Use Redis for caching CDN Caching cdn, edge, cache Cache at CDN edge Database Query Cache query cache, mysql, postgres Cache database queries Application Cache app cache, memory, local In-memory application cache Distributed Cache distributed, memcached Multi-node caching Cache Invalidation invalidate, ttl, expire Proper cache expiration Write-Through Cache write-through, consistency Consistent caching Cache Warming warm, preload, prefetch Pre-populate caches"},{"location":"reference/patterns/#async-patterns-6","title":"Async Patterns (6)","text":"<p>Patterns for asynchronous processing.</p> Pattern Keywords Description Queue Non-Urgent Requests queue, async, defer Queue non-critical work Use Async Instead of Sync async, await, non-blocking Prefer async operations Batch Processing batch, bulk, aggregate Process in batches Event-Driven Architecture event, pub-sub, message Use event-driven design Background Jobs background, worker, job Process in background Stream Processing stream, reactive, flow Use streaming for large data"},{"location":"reference/patterns/#database-patterns-8","title":"Database Patterns (8)","text":"<p>Patterns for database optimization.</p> Pattern Keywords Description Optimize Database Queries query, optimize, explain Improve query performance Use Connection Pooling pool, connection, reuse Reuse database connections Index Optimization index, btree, covering Optimize database indexes Read Replicas replica, read, slave Scale reads with replicas Denormalization denormalize, join, embed Reduce join operations Partition Tables partition, shard, split Split large tables Use NoSQL nosql, document, key-value Use appropriate database type Lazy Loading Relations lazy, eager, n+1 Avoid N+1 query problems"},{"location":"reference/patterns/#network-patterns-6","title":"Network Patterns (6)","text":"<p>Patterns for network optimization.</p> Pattern Keywords Description HTTP Compression gzip, compress, transfer Compress HTTP responses Reduce API Calls batch, aggregate, graphql Minimize API requests Use WebSockets websocket, socket, realtime Use persistent connections Protocol Optimization http3, quic, protocol Use efficient protocols Edge Computing edge, close, proximity Process at the edge Connection Reuse keep-alive, persist, reuse Reuse network connections"},{"location":"reference/patterns/#resource-patterns-5","title":"Resource Patterns (5)","text":"<p>Patterns for resource management.</p> Pattern Keywords Description Memory Optimization memory, heap, gc Optimize memory usage CPU Optimization cpu, thread, parallel Optimize CPU usage I/O Optimization io, disk, buffer Optimize I/O operations Resource Pooling pool, reuse, recycle Pool expensive resources Garbage Collection Tuning gc, tuning, generational Tune GC parameters"},{"location":"reference/patterns/#code-patterns-4","title":"Code Patterns (4)","text":"<p>Patterns for code-level optimization.</p> Pattern Keywords Description Remove Dead Code dead code, unused, remove Eliminate unused code Algorithm Optimization algorithm, complexity, O(n) Use efficient algorithms Loop Optimization loop, iteration, vectorize Optimize loops Avoid Premature Optimization premature, profile, measure Profile before optimizing"},{"location":"reference/patterns/#infrastructure-patterns-4","title":"Infrastructure Patterns (4)","text":"<p>Patterns for infrastructure optimization.</p> Pattern Keywords Description Alpine Containers alpine, minimal, scratch Use minimal base images Infrastructure as Code iac, terraform, ansible Manage infrastructure as code Renewable Energy Regions renewable, green, carbon Use green energy regions Container Optimization container, layer, cache Optimize container builds"},{"location":"reference/patterns/#microservices-patterns-4","title":"Microservices Patterns (4)","text":"<p>Patterns for microservices architecture.</p> Pattern Keywords Description Service Decomposition decompose, microservice, split Right-size services Colocation Strategies colocate, affinity, proximity Place related services together Graceful Shutdown graceful, shutdown, sigterm Handle shutdown properly Service Mesh Optimization mesh, sidecar, istio Optimize service mesh overhead"},{"location":"reference/patterns/#monitoring-patterns-3","title":"Monitoring Patterns (3)","text":"<p>Patterns for observability optimization.</p> Pattern Keywords Description Efficient Logging logging, log level, structured Optimize log volume Metrics Aggregation metrics, aggregate, rollup Aggregate metrics efficiently Trace Sampling sampling, trace, opentelemetry Sample traces appropriately"},{"location":"reference/patterns/#general-patterns-8","title":"General Patterns (8)","text":"<p>General optimization patterns.</p> Pattern Keywords Description Feature Flags feature flag, toggle, switch Use feature flags Incremental Processing incremental, delta, diff Process only changes Precomputation precompute, materialize, cache Precompute expensive results Background Jobs background, async, worker Process in background Rate Limiting rate limit, throttle, backoff Limit request rates Circuit Breaker circuit, breaker, fallback Fail fast with fallbacks Retry with Backoff retry, backoff, exponential Retry with exponential backoff Timeout Configuration timeout, deadline, cancel Set appropriate timeouts"},{"location":"reference/patterns/#accessing-patterns-programmatically","title":"Accessing Patterns Programmatically","text":"<pre><code>from greenmining import GSF_PATTERNS\n\n# Count patterns\nprint(f\"Total patterns: {len(GSF_PATTERNS)}\")  # 122\n\n# Get all categories\ncategories = set(p[\"category\"] for p in GSF_PATTERNS.values())\nprint(f\"Categories: {sorted(categories)}\")\n\n# Find patterns by category\ncloud_patterns = [\n    p for p in GSF_PATTERNS.values() \n    if p[\"category\"] == \"cloud\"\n]\nprint(f\"Cloud patterns: {len(cloud_patterns)}\")\n\n# Get pattern details\npattern = GSF_PATTERNS[\"cache_static_data\"]\nprint(f\"Name: {pattern['name']}\")\nprint(f\"Category: {pattern['category']}\")\nprint(f\"Keywords: {pattern['keywords']}\")\nprint(f\"Description: {pattern['description']}\")\nprint(f\"SCI Impact: {pattern['sci_impact']}\")\n</code></pre>"},{"location":"reference/patterns/#green-keywords","title":"Green Keywords","text":"<p>The 321 green keywords used for detection:</p> <pre><code>from greenmining import GREEN_KEYWORDS\n\n# Categories of keywords\nkeyword_categories = {\n    \"energy\": [\"energy\", \"power\", \"watt\", \"joule\", \"consumption\"],\n    \"carbon\": [\"carbon\", \"emission\", \"co2\", \"greenhouse\", \"footprint\"],\n    \"efficiency\": [\"efficient\", \"efficiency\", \"optimize\", \"reduce\", \"minimize\"],\n    \"sustainability\": [\"sustainable\", \"green\", \"eco\", \"environmental\"],\n    \"performance\": [\"performance\", \"fast\", \"speed\", \"latency\", \"throughput\"],\n    \"resource\": [\"resource\", \"memory\", \"cpu\", \"disk\", \"network\"],\n    \"caching\": [\"cache\", \"cached\", \"caching\", \"redis\", \"memcache\"],\n    \"compression\": [\"compress\", \"gzip\", \"brotli\", \"minify\", \"compact\"],\n}\n\n# Sample keywords\nprint(GREEN_KEYWORDS[:20])\n# ['energy', 'power', 'carbon', 'emission', 'footprint', 'sustainability', ...]\n</code></pre>"},{"location":"reference/patterns/#pattern-detection-example","title":"Pattern Detection Example","text":"<pre><code>from greenmining import is_green_aware, get_pattern_by_keywords\n\n# Test messages\nmessages = [\n    \"Implement Redis caching for user sessions\",\n    \"Enable gzip compression on API responses\",\n    \"Migrate to serverless Lambda functions\",\n    \"Optimize database queries with proper indexing\",\n    \"Add lazy loading for images\",\n    \"Fix typo in documentation\",\n]\n\nfor msg in messages:\n    is_green = is_green_aware(msg)\n    patterns = get_pattern_by_keywords(msg) if is_green else []\n\n    if is_green:\n        print(f\"\ud83c\udf31 {msg}\")\n        print(f\"   Patterns: {patterns}\")\n    else:\n        print(f\"   {msg}\")\n</code></pre> <p>Output:</p> <pre><code>\ud83c\udf31 Implement Redis caching for user sessions\n   Patterns: ['Cache Static Data']\n\ud83c\udf31 Enable gzip compression on API responses\n   Patterns: ['Compress Transmitted Data', 'Enable Text Compression']\n\ud83c\udf31 Migrate to serverless Lambda functions\n   Patterns: ['Use Serverless']\n\ud83c\udf31 Optimize database queries with proper indexing\n   Patterns: ['Optimize Database Queries', 'Index Optimization']\n\ud83c\udf31 Add lazy loading for images\n   Patterns: ['Lazy Loading', 'Optimize Images']\n   Fix typo in documentation\n</code></pre>"},{"location":"reference/patterns/#contributing-patterns","title":"Contributing Patterns","text":"<p>To suggest new patterns or improvements:</p> <ol> <li>Check the GSF Patterns Catalog</li> <li>Open an issue on GitHub</li> <li>Submit a pull request with pattern additions</li> </ol>"},{"location":"reference/patterns/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Options - All configuration parameters</li> <li>Python API - Programmatic usage</li> <li>CLI Commands - Command-line reference</li> </ul>"},{"location":"user-guide/api/","title":"Python API Reference","text":"<p>Use GreenMining programmatically in your Python scripts.</p>"},{"location":"user-guide/api/#quick-import","title":"Quick Import","text":"<pre><code>from greenmining import (\n    GSF_PATTERNS,        # Dict of 122 GSF patterns\n    GREEN_KEYWORDS,      # List of 321 green keywords\n    is_green_aware,      # Check if message is green-aware\n    get_pattern_by_keywords,  # Get matched patterns\n    fetch_repositories,  # Fetch repos from GitHub\n    Config,              # Configuration class\n)\n</code></pre>"},{"location":"user-guide/api/#core-functions","title":"Core Functions","text":""},{"location":"user-guide/api/#is_green_aware","title":"is_green_aware()","text":"<p>Check if a commit message indicates green software awareness.</p> <pre><code>def is_green_aware(commit_message: str) -&gt; bool\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>commit_message</code> str The commit message to analyze <p>Returns: <code>bool</code> - True if message contains green keywords</p> <p>Example:</p> <pre><code>from greenmining import is_green_aware\n\n# Returns True\nis_green_aware(\"Optimize Redis caching for better performance\")\nis_green_aware(\"Enable gzip compression on API responses\")\nis_green_aware(\"Implement async batch processing\")\n\n# Returns False\nis_green_aware(\"Fix typo in README\")\nis_green_aware(\"Update dependencies\")\nis_green_aware(\"Refactor variable names\")\n</code></pre>"},{"location":"user-guide/api/#get_pattern_by_keywords","title":"get_pattern_by_keywords()","text":"<p>Find GSF patterns that match a commit message.</p> <pre><code>def get_pattern_by_keywords(commit_message: str) -&gt; list[str]\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>commit_message</code> str The commit message to analyze <p>Returns: <code>list[str]</code> - List of matched pattern names</p> <p>Example:</p> <pre><code>from greenmining import get_pattern_by_keywords\n\npatterns = get_pattern_by_keywords(\"Implement Redis caching layer\")\nprint(patterns)\n# Output: ['Cache Static Data']\n\npatterns = get_pattern_by_keywords(\"Enable gzip compression for API responses\")\nprint(patterns)\n# Output: ['Compress Transmitted Data', 'Enable Text Compression']\n\npatterns = get_pattern_by_keywords(\"Fix typo\")\nprint(patterns)\n# Output: []\n</code></pre>"},{"location":"user-guide/api/#fetch_repositories","title":"fetch_repositories()","text":"<p>Fetch repositories from GitHub matching search criteria.</p> <pre><code>def fetch_repositories(\n    github_token: str,\n    max_repos: int = 100,\n    min_stars: int = 100,\n    languages: list = None,\n    keywords: str = \"microservices\"\n) -&gt; list\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>github_token</code> str (required) GitHub personal access token <code>max_repos</code> int 100 Maximum repositories to fetch <code>min_stars</code> int 100 Minimum star count <code>languages</code> list None Filter by languages <code>keywords</code> str \"microservices\" Search keywords <p>Returns: <code>list</code> - List of Repository objects</p> <p>Example:</p> <pre><code>from greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=\"ghp_xxxx\",\n    max_repos=10,\n    min_stars=500,\n    keywords=\"kubernetes\",\n    languages=[\"Python\", \"Go\"]\n)\n\nfor repo in repos:\n    print(f\"{repo.full_name}: {repo.stars} stars\")\n</code></pre>"},{"location":"user-guide/api/#data-structures","title":"Data Structures","text":""},{"location":"user-guide/api/#gsf_patterns","title":"GSF_PATTERNS","text":"<p>Dictionary containing all 122 Green Software Foundation patterns.</p> <pre><code>from greenmining import GSF_PATTERNS\n\n# Structure\nGSF_PATTERNS = {\n    \"pattern_id\": {\n        \"name\": \"Pattern Name\",\n        \"category\": \"category_name\",\n        \"keywords\": [\"keyword1\", \"keyword2\"],\n        \"description\": \"Pattern description\",\n        \"sci_impact\": \"Impact on software carbon intensity\"\n    },\n    ...\n}\n</code></pre> <p>Example Usage:</p> <pre><code>from greenmining import GSF_PATTERNS\n\n# Get pattern count\nprint(f\"Total patterns: {len(GSF_PATTERNS)}\")  # 122\n\n# Get all categories\ncategories = set(p[\"category\"] for p in GSF_PATTERNS.values())\nprint(f\"Categories: {categories}\")\n# {'cloud', 'web', 'ai', 'caching', 'async', 'database', ...}\n\n# Find patterns by category\ncloud_patterns = [\n    p[\"name\"] for p in GSF_PATTERNS.values() \n    if p[\"category\"] == \"cloud\"\n]\nprint(f\"Cloud patterns: {len(cloud_patterns)}\")  # 40+\n\n# Get pattern details\ncache_pattern = GSF_PATTERNS[\"cache_static_data\"]\nprint(f\"Name: {cache_pattern['name']}\")\nprint(f\"Keywords: {cache_pattern['keywords']}\")\n</code></pre>"},{"location":"user-guide/api/#green_keywords","title":"GREEN_KEYWORDS","text":"<p>List of 321 keywords indicating green software practices.</p> <pre><code>from greenmining import GREEN_KEYWORDS\n\nprint(f\"Total keywords: {len(GREEN_KEYWORDS)}\")  # 321\n\n# Sample keywords\nprint(GREEN_KEYWORDS[:10])\n# ['energy', 'power', 'carbon', 'emission', 'footprint', \n#  'sustainability', 'sustainable', 'green', 'efficient', 'efficiency']\n</code></pre>"},{"location":"user-guide/api/#service-classes","title":"Service Classes","text":""},{"location":"user-guide/api/#dataanalyzer","title":"DataAnalyzer","text":"<p>Analyze commits for green software patterns.</p> <pre><code>from greenmining.services.data_analyzer import DataAnalyzer\n\nanalyzer = DataAnalyzer(\n    enable_diff_analysis=False,  # Analyze code diffs\n    patterns=None,               # Custom patterns (default: GSF_PATTERNS)\n    batch_size=10                # Commits per batch\n)\n</code></pre> <p>Methods:</p> <pre><code># Analyze a single commit\nresult = analyzer.analyze_commit(commit_dict)\n\n# Analyze multiple commits\nresults = analyzer.analyze_commits(commits_list)\n\n# Save results to file\nanalyzer.save_results(results, \"output.json\")\n</code></pre> <p>Example:</p> <pre><code>from greenmining.services.data_analyzer import DataAnalyzer\n\nanalyzer = DataAnalyzer(enable_diff_analysis=True)\n\ncommit = {\n    \"sha\": \"abc123\",\n    \"message\": \"Implement Redis caching for user sessions\",\n    \"author\": \"developer\",\n    \"date\": \"2024-01-15T10:00:00Z\"\n}\n\nresult = analyzer.analyze_commit(commit)\nprint(f\"Green-aware: {result['green_aware']}\")\nprint(f\"Patterns: {result['patterns']}\")\n</code></pre>"},{"location":"user-guide/api/#dataaggregator","title":"DataAggregator","text":"<p>Aggregate analysis results with statistics.</p> <pre><code>from greenmining.services.data_aggregator import DataAggregator\n\naggregator = DataAggregator(\n    config=None,                  # Config object\n    enable_stats=True,            # Statistical analysis\n    enable_temporal=True,         # Temporal trends\n    temporal_granularity=\"quarter\"  # day/week/month/quarter/year\n)\n</code></pre> <p>Methods:</p> <pre><code># Aggregate results\naggregated = aggregator.aggregate(analysis_results, repositories)\n\n# Save to files\naggregator.save_results(aggregated, \"stats.json\", \"stats.csv\", analysis_results)\n\n# Print summary\naggregator.print_summary(aggregated)\n</code></pre> <p>Example:</p> <pre><code>from greenmining.services.data_aggregator import DataAggregator\n\naggregator = DataAggregator(\n    enable_stats=True,\n    enable_temporal=True,\n    temporal_granularity=\"month\"\n)\n\n# Assuming analysis_results and repositories are already populated\naggregated = aggregator.aggregate(analysis_results, repositories)\n\nprint(f\"Total commits: {aggregated['summary']['total_commits']}\")\nprint(f\"Green-aware: {aggregated['summary']['green_aware_percentage']}%\")\n</code></pre>"},{"location":"user-guide/api/#localrepoanalyzer","title":"LocalRepoAnalyzer","text":"<p>Analyze repositories directly from GitHub URLs using PyDriller.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/greenmining_repos\",  # Clone directory\n    cleanup_after=True                     # Delete after analysis\n)\n</code></pre> <p>Methods:</p> <pre><code># Analyze single repository\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/owner/repo\",\n    max_commits=1000,\n    since_date=datetime(2024, 1, 1),\n    to_date=datetime(2024, 12, 31)\n)\n</code></pre> <p>Example:</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom datetime import datetime\n\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/analysis\",\n    cleanup_after=True\n)\n\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=100\n)\n\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Commits analyzed: {result['total_commits']}\")\nprint(f\"Green-aware: {result['green_aware_count']}\")\n\n# Access individual commits\nfor commit in result['commits'][:5]:\n    if commit['green_aware']:\n        print(f\"  \ud83c\udf31 {commit['message'][:50]}...\")\n</code></pre>"},{"location":"user-guide/api/#reportgenerator","title":"ReportGenerator","text":"<p>Generate Markdown reports from analysis results.</p> <pre><code>from greenmining.services.reports import ReportGenerator\n\ngenerator = ReportGenerator()\n</code></pre> <p>Methods:</p> <pre><code># Generate full report\nreport = generator.generate_report(aggregated_data)\n\n# Save to file\ngenerator.save_report(report, \"report.md\")\n</code></pre>"},{"location":"user-guide/api/#analyzer-classes","title":"Analyzer Classes","text":""},{"location":"user-guide/api/#statisticalanalyzer","title":"StatisticalAnalyzer","text":"<p>Compute statistical metrics on analysis results.</p> <pre><code>from greenmining.analyzers.statistical_analyzer import StatisticalAnalyzer\n\nanalyzer = StatisticalAnalyzer()\n\n# Pattern correlations\ncorrelations = analyzer.analyze_pattern_correlations(analysis_results)\n\n# Effect sizes\neffect_sizes = analyzer.analyze_effect_sizes(analysis_results)\n\n# Descriptive statistics\ndescriptive = analyzer.get_descriptive_statistics(analysis_results)\n</code></pre>"},{"location":"user-guide/api/#temporalanalyzer","title":"TemporalAnalyzer","text":"<p>Analyze patterns over time.</p> <pre><code>from greenmining.analyzers.temporal_analyzer import TemporalAnalyzer\n\nanalyzer = TemporalAnalyzer(granularity=\"quarter\")\n\n# Group commits by period\nperiods = analyzer.group_commits_by_period(commits)\n\n# Analyze trends\ntrends = analyzer.analyze_trends(periods)\n\n# Pattern evolution\nevolution = analyzer.analyze_pattern_evolution(commits)\n</code></pre>"},{"location":"user-guide/api/#qualitativeanalyzer","title":"QualitativeAnalyzer","text":"<p>Generate validation samples for manual review.</p> <pre><code>from greenmining.analyzers.qualitative_analyzer import QualitativeAnalyzer\n\nanalyzer = QualitativeAnalyzer(\n    sample_size=30,\n    stratify_by=\"pattern\"  # pattern/repository/time/random\n)\n\n# Generate samples\nsamples = analyzer.generate_validation_samples(analysis_results)\n\n# Export for review\nanalyzer.export_samples_for_review(samples, \"validation_samples.csv\")\n</code></pre>"},{"location":"user-guide/api/#codediffanalyzer","title":"CodeDiffAnalyzer","text":"<p>Analyze code changes for green patterns.</p> <pre><code>from greenmining.analyzers.code_diff_analyzer import CodeDiffAnalyzer\n\nanalyzer = CodeDiffAnalyzer()\n\n# Check if file is analyzable\nis_code = analyzer.is_code_file(\"app.py\")  # True\n\n# Analyze diff content\npatterns = analyzer.detect_patterns_in_diff(diff_text)\n</code></pre>"},{"location":"user-guide/api/#configuration-class","title":"Configuration Class","text":"<pre><code>from greenmining.config import Config\n\nconfig = Config()\n\n# Access configuration values\nprint(config.MAX_REPOS)           # 100\nprint(config.COMMITS_PER_REPO)    # 1000\nprint(config.SUPPORTED_LANGUAGES) # ['Python', 'Java', ...]\nprint(config.OUTPUT_DIR)          # 'data'\n\n# URL Analysis options\nprint(config.REPOSITORY_URLS)     # []\nprint(config.CLONE_PATH)          # '/tmp/greenmining_repos'\n\n# Energy options\nprint(config.ENERGY_ENABLED)      # False\nprint(config.ENERGY_BACKEND)      # 'rapl'\n\n# PyDriller options\nprint(config.PROCESS_METRICS_ENABLED)  # True\nprint(config.DMM_ENABLED)              # True\n</code></pre>"},{"location":"user-guide/api/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Complete GreenMining analysis workflow.\"\"\"\n\nfrom greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom greenmining.services.data_aggregator import DataAggregator\nfrom greenmining.services.reports import ReportGenerator\n\n# 1. Analyze repository\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=200\n)\n\nprint(f\"Analyzed {result['total_commits']} commits\")\nprint(f\"Green-aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\n# 2. Aggregate results\naggregator = DataAggregator(enable_stats=True, enable_temporal=True)\naggregated = aggregator.aggregate(\n    analysis_results=result['commits'],\n    repositories=[result['repository']]\n)\n\n# 3. Generate report\ngenerator = ReportGenerator()\nreport = generator.generate_report(aggregated)\ngenerator.save_report(report, \"flask_analysis.md\")\n\nprint(\"Report saved to flask_analysis.md\")\n</code></pre>"},{"location":"user-guide/api/#next-steps","title":"Next Steps","text":"<ul> <li>URL Analysis - Deep dive into URL-based analysis</li> <li>Energy Measurement - Power profiling with RAPL/CodeCarbon</li> <li>GSF Patterns Reference - All 122 patterns</li> </ul>"},{"location":"user-guide/cli/","title":"CLI Commands Reference","text":"<p>Complete reference for all GreenMining command-line interface commands.</p>"},{"location":"user-guide/cli/#global-options","title":"Global Options","text":"<p>These options apply to all commands:</p> Option Description <code>--help</code> Show help message and exit <code>--version</code> Show version number"},{"location":"user-guide/cli/#commands-overview","title":"Commands Overview","text":"Command Description <code>fetch</code> Fetch repositories from GitHub <code>extract</code> Extract commit history <code>analyze</code> Analyze commits for green patterns <code>aggregate</code> Aggregate and summarize results <code>report</code> Generate analysis report <code>pipeline</code> Run complete analysis pipeline <code>status</code> Show current analysis status <code>analyze-url</code> Analyze a single repository by URL <code>analyze-urls</code> Analyze multiple repositories by URL"},{"location":"user-guide/cli/#fetch","title":"fetch","text":"<p>Fetch repositories from GitHub based on search criteria.</p> <pre><code>greenmining fetch [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#options","title":"Options","text":"Option Type Default Description <code>--max-repos</code> INTEGER 100 Maximum number of repositories to fetch <code>--min-stars</code> INTEGER 100 Minimum GitHub stars required <code>--languages</code> TEXT Python,Java,Go,... Comma-separated language list <code>--keywords</code> TEXT microservices Search keywords <code>--token</code> TEXT (from env) GitHub personal access token"},{"location":"user-guide/cli/#examples","title":"Examples","text":"<pre><code># Fetch Python repositories about kubernetes\ngreenmining fetch --max-repos 50 --keywords \"kubernetes\" --languages \"Python\"\n\n# Fetch popular microservices projects\ngreenmining fetch --max-repos 100 --min-stars 500\n\n# Fetch with custom languages\ngreenmining fetch --languages \"Python,Go,Rust\" --keywords \"cloud-native\"\n</code></pre>"},{"location":"user-guide/cli/#output","title":"Output","text":"<p>Creates <code>data/repositories.json</code>:</p> <pre><code>[\n  {\n    \"name\": \"kubernetes\",\n    \"full_name\": \"kubernetes/kubernetes\",\n    \"url\": \"https://github.com/kubernetes/kubernetes\",\n    \"stars\": 98000,\n    \"language\": \"Go\",\n    \"forks\": 35000\n  }\n]\n</code></pre>"},{"location":"user-guide/cli/#extract","title":"extract","text":"<p>Extract commit history from previously fetched repositories.</p> <pre><code>greenmining extract [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#options_1","title":"Options","text":"Option Type Default Description <code>--max-commits</code> INTEGER 1000 Maximum commits per repository <code>--exclude-merges</code> FLAG true Exclude merge commits <code>--exclude-bots</code> FLAG true Exclude bot commits"},{"location":"user-guide/cli/#examples_1","title":"Examples","text":"<pre><code># Extract up to 500 commits per repo\ngreenmining extract --max-commits 500\n\n# Include merge commits\ngreenmining extract --no-exclude-merges\n</code></pre>"},{"location":"user-guide/cli/#output_1","title":"Output","text":"<p>Creates <code>data/commits.json</code>:</p> <pre><code>[\n  {\n    \"sha\": \"abc123def456\",\n    \"message\": \"Optimize caching layer for better performance\",\n    \"author\": \"developer\",\n    \"date\": \"2024-01-15T10:30:00Z\",\n    \"repository\": \"org/repo\"\n  }\n]\n</code></pre>"},{"location":"user-guide/cli/#analyze","title":"analyze","text":"<p>Analyze extracted commits for green software patterns.</p> <pre><code>greenmining analyze [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#options_2","title":"Options","text":"Option Type Default Description <code>--batch-size</code> INTEGER 10 Number of commits per batch <code>--enable-diff-analysis</code> FLAG false Enable code diff analysis (slower)"},{"location":"user-guide/cli/#examples_2","title":"Examples","text":"<pre><code># Basic analysis\ngreenmining analyze\n\n# With diff analysis (more accurate, slower)\ngreenmining analyze --enable-diff-analysis\n\n# Custom batch size\ngreenmining analyze --batch-size 50\n</code></pre>"},{"location":"user-guide/cli/#output_2","title":"Output","text":"<p>Creates <code>data/analysis_results.json</code>:</p> <pre><code>[\n  {\n    \"sha\": \"abc123def456\",\n    \"message\": \"Optimize caching layer\",\n    \"green_aware\": true,\n    \"patterns\": [\"Cache Static Data\"],\n    \"category\": \"caching\",\n    \"confidence\": 0.85,\n    \"keywords_matched\": [\"cache\", \"optimize\"]\n  }\n]\n</code></pre>"},{"location":"user-guide/cli/#aggregate","title":"aggregate","text":"<p>Aggregate analysis results with statistical summaries.</p> <pre><code>greenmining aggregate [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#options_3","title":"Options","text":"Option Type Default Description <code>--enable-stats</code> FLAG false Enable statistical analysis <code>--enable-temporal</code> FLAG false Enable temporal trend analysis <code>--temporal-granularity</code> CHOICE quarter day/week/month/quarter/year"},{"location":"user-guide/cli/#examples_3","title":"Examples","text":"<pre><code># Basic aggregation\ngreenmining aggregate\n\n# With temporal analysis by quarter\ngreenmining aggregate --enable-temporal --temporal-granularity quarter\n\n# Full statistical analysis\ngreenmining aggregate --enable-stats --enable-temporal\n</code></pre>"},{"location":"user-guide/cli/#output_3","title":"Output","text":"<p>Creates <code>data/aggregated_statistics.json</code>:</p> <pre><code>{\n  \"summary\": {\n    \"total_commits\": 5000,\n    \"green_aware_count\": 1250,\n    \"green_aware_percentage\": 25.0,\n    \"total_repos\": 50\n  },\n  \"pattern_distribution\": {\n    \"Cache Static Data\": 320,\n    \"Use Async Instead of Sync\": 180\n  },\n  \"temporal_analysis\": {\n    \"periods\": [...]\n  }\n}\n</code></pre>"},{"location":"user-guide/cli/#report","title":"report","text":"<p>Generate a comprehensive analysis report.</p> <pre><code>greenmining report [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#options_4","title":"Options","text":"Option Type Default Description <code>--format</code> CHOICE markdown Output format (markdown/json) <code>--output</code> PATH data/green_analysis.md Output file path"},{"location":"user-guide/cli/#examples_4","title":"Examples","text":"<pre><code># Generate Markdown report\ngreenmining report\n\n# Custom output path\ngreenmining report --output reports/analysis.md\n</code></pre>"},{"location":"user-guide/cli/#output_4","title":"Output","text":"<p>Creates a Markdown report with:</p> <ul> <li>Executive summary</li> <li>Pattern distribution charts</li> <li>Per-repository statistics</li> <li>Temporal trends (if enabled)</li> <li>Statistical analysis (if enabled)</li> </ul>"},{"location":"user-guide/cli/#pipeline","title":"pipeline","text":"<p>Run the complete analysis pipeline (fetch \u2192 extract \u2192 analyze \u2192 aggregate \u2192 report).</p> <pre><code>greenmining pipeline [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#options_5","title":"Options","text":"Option Type Default Description <code>--max-repos</code> INTEGER 100 Maximum repositories <code>--max-commits</code> INTEGER 1000 Maximum commits per repo <code>--keywords</code> TEXT microservices Search keywords <code>--min-stars</code> INTEGER 100 Minimum stars"},{"location":"user-guide/cli/#examples_5","title":"Examples","text":"<pre><code># Quick test pipeline\ngreenmining pipeline --max-repos 5 --max-commits 50\n\n# Full pipeline for research\ngreenmining pipeline --max-repos 100 --max-commits 1000 --keywords \"kubernetes\"\n</code></pre>"},{"location":"user-guide/cli/#status","title":"status","text":"<p>Show current analysis status and configuration.</p> <pre><code>greenmining status\n</code></pre>"},{"location":"user-guide/cli/#output_5","title":"Output","text":"<pre><code>GreenMining Analysis Status\n===========================\nConfiguration:\n  GITHUB_TOKEN: \u2713 configured\n  OUTPUT_DIR: data/\n  MAX_REPOS: 100\n\nData Files:\n  repositories.json: 50 repositories\n  commits.json: 12,500 commits\n  analysis_results.json: 12,500 analyzed\n\nAnalysis Summary:\n  Green-aware commits: 3,125 (25.0%)\n  Top pattern: Cache Static Data (890)\n</code></pre>"},{"location":"user-guide/cli/#analyze-url","title":"analyze-url","text":"<p>Analyze a single GitHub repository by URL using PyDriller.</p> <pre><code>greenmining analyze-url URL [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#arguments","title":"Arguments","text":"Argument Description <code>URL</code> GitHub repository URL (HTTPS or SSH)"},{"location":"user-guide/cli/#options_6","title":"Options","text":"Option Type Default Description <code>--max-commits</code> INTEGER 1000 Maximum commits to analyze <code>--since</code> DATE None Start date (YYYY-MM-DD) <code>--to</code> DATE None End date (YYYY-MM-DD) <code>--clone-path</code> PATH /tmp/greenmining_repos Clone directory <code>--no-cleanup</code> FLAG false Keep cloned repository <code>--output</code> PATH None Output JSON file"},{"location":"user-guide/cli/#examples_6","title":"Examples","text":"<pre><code># Basic analysis\ngreenmining analyze-url https://github.com/pallets/flask\n\n# Limit commits\ngreenmining analyze-url https://github.com/pallets/flask --max-commits 100\n\n# Date range\ngreenmining analyze-url https://github.com/pallets/flask \\\n  --since 2024-01-01 --to 2024-06-30\n\n# Save results\ngreenmining analyze-url https://github.com/pallets/flask \\\n  --output flask_analysis.json\n</code></pre>"},{"location":"user-guide/cli/#supported-url-formats","title":"Supported URL Formats","text":"<pre><code># HTTPS\nhttps://github.com/owner/repo\nhttps://github.com/owner/repo.git\n\n# SSH\ngit@github.com:owner/repo.git\n</code></pre>"},{"location":"user-guide/cli/#analyze-urls","title":"analyze-urls","text":"<p>Analyze multiple repositories by URL.</p> <pre><code>greenmining analyze-urls [OPTIONS]\n</code></pre>"},{"location":"user-guide/cli/#options_7","title":"Options","text":"Option Type Default Description <code>--urls</code> TEXT None Comma-separated URLs <code>--file</code> PATH None File with URLs (one per line) <code>--max-commits</code> INTEGER 1000 Max commits per repo <code>--output-dir</code> PATH data/ Output directory"},{"location":"user-guide/cli/#examples_7","title":"Examples","text":"<pre><code># Multiple URLs inline\ngreenmining analyze-urls --urls \"https://github.com/org/repo1,https://github.com/org/repo2\"\n\n# From file\necho \"https://github.com/pallets/flask\nhttps://github.com/django/django\nhttps://github.com/fastapi/fastapi\" &gt; repos.txt\n\ngreenmining analyze-urls --file repos.txt --max-commits 200\n</code></pre>"},{"location":"user-guide/cli/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 General error 2 Configuration error 3 GitHub API error 4 File I/O error"},{"location":"user-guide/cli/#environment-variables","title":"Environment Variables","text":"<p>All CLI options can be set via environment variables:</p> <pre><code>export GREENMINING_MAX_REPOS=50\nexport GREENMINING_MIN_STARS=200\nexport GREENMINING_KEYWORDS=\"cloud-native\"\n</code></pre>"},{"location":"user-guide/cli/#next-steps","title":"Next Steps","text":"<ul> <li>Python API - Programmatic usage</li> <li>URL Analysis - Detailed URL analysis guide</li> <li>Configuration - All config options</li> </ul>"},{"location":"user-guide/energy/","title":"Energy Measurement","text":"<p>Measure energy consumption of your analysis workloads with RAPL and CodeCarbon.</p>"},{"location":"user-guide/energy/#overview","title":"Overview","text":"<p>GreenMining includes energy measurement capabilities to profile the power consumption of analysis operations. This is useful for:</p> <ul> <li>Research - Quantify energy cost of mining operations</li> <li>Optimization - Identify energy-intensive analysis steps</li> <li>Reporting - Include energy metrics in analysis reports</li> </ul>"},{"location":"user-guide/energy/#supported-backends","title":"Supported Backends","text":"Backend Platform Features RAPL Linux (Intel/AMD) Direct CPU/DRAM power reading CodeCarbon Cross-platform Emissions tracking, cloud support CPU Energy Meter Linux Alternative to RAPL (future)"},{"location":"user-guide/energy/#rapl-backend","title":"RAPL Backend","text":"<p>Intel's Running Average Power Limit (RAPL) provides direct power measurements on Linux systems with Intel or AMD processors.</p>"},{"location":"user-guide/energy/#requirements","title":"Requirements","text":"<ul> <li>Linux operating system</li> <li>Intel Core 2nd generation+ or AMD Ryzen</li> <li>Read access to <code>/sys/class/powercap/intel-rapl/</code></li> </ul>"},{"location":"user-guide/energy/#checking-availability","title":"Checking Availability","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\nif meter.is_available():\n    print(\"RAPL is available on this system\")\nelse:\n    print(\"RAPL not available - try running as root\")\n</code></pre>"},{"location":"user-guide/energy/#basic-usage","title":"Basic Usage","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\n\n# Start measurement\nmeter.start()\n\n# Your workload here\nresult = expensive_computation()\n\n# Stop and get metrics\nmetrics = meter.stop()\n\nprint(f\"Energy consumed: {metrics.energy_joules:.2f} J\")\nprint(f\"Duration: {metrics.duration_seconds:.2f} s\")\nprint(f\"Average power: {metrics.average_power_watts:.2f} W\")\n</code></pre>"},{"location":"user-guide/energy/#with-context-manager","title":"With Context Manager","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\n\nwith meter.measure() as measurement:\n    # Your analysis code\n    analyzer.analyze_repository(repo_url)\n\nprint(f\"Analysis consumed {measurement.energy_joules:.2f} J\")\n</code></pre>"},{"location":"user-guide/energy/#permission-setup","title":"Permission Setup","text":"<p>RAPL typically requires root access. To allow non-root users:</p> <pre><code># Grant read access to RAPL files\nsudo chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\nsudo chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/intel-rapl:0:*/energy_uj\n\n# Or create a udev rule for persistent access\necho 'SUBSYSTEM==\"powercap\", ACTION==\"add\", RUN+=\"/bin/chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\"' | sudo tee /etc/udev/rules.d/99-rapl.rules\n</code></pre>"},{"location":"user-guide/energy/#codecarbon-backend","title":"CodeCarbon Backend","text":"<p>CodeCarbon tracks energy consumption and CO2 emissions across platforms.</p>"},{"location":"user-guide/energy/#installation","title":"Installation","text":"<pre><code>pip install codecarbon\n</code></pre>"},{"location":"user-guide/energy/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from greenmining.energy.codecarbon_meter import CodeCarbonMeter\n\nmeter = CodeCarbonMeter(\n    country_iso_code=\"USA\",  # For carbon intensity\n    project_name=\"greenmining-analysis\",\n    tracking_mode=\"process\"  # or \"machine\"\n)\n\n# Start tracking\nmeter.start()\n\n# Your workload\nresult = analyzer.analyze_repository(repo_url)\n\n# Stop and get metrics\nmetrics = meter.stop()\n\nprint(f\"Energy: {metrics.energy_kwh:.6f} kWh\")\nprint(f\"CO2 emissions: {metrics.emissions_kg:.6f} kg\")\nprint(f\"Duration: {metrics.duration_seconds:.2f} s\")\n</code></pre>"},{"location":"user-guide/energy/#configuration-options","title":"Configuration Options","text":"<pre><code>meter = CodeCarbonMeter(\n    country_iso_code=\"FRA\",      # France\n    region=\"ile-de-france\",       # Optional region\n    project_name=\"my-analysis\",\n    output_dir=\"./energy_logs\",   # Where to save logs\n    save_to_file=True,            # Save detailed logs\n    tracking_mode=\"process\"       # process or machine\n)\n</code></pre>"},{"location":"user-guide/energy/#carbon-tracking","title":"Carbon Tracking","text":"<pre><code>from greenmining.energy.codecarbon_meter import CodeCarbonMeter\n\nmeter = CodeCarbonMeter(\n    country_iso_code=\"DEU\",  # Germany - higher carbon intensity\n    project_name=\"greenmining\"\n)\n\nmeter.start()\n# ... analysis ...\nmetrics = meter.stop()\n\nprint(f\"Carbon footprint: {metrics.emissions_kg * 1000:.2f} g CO2\")\nprint(f\"Equivalent to: {metrics.emissions_kg / 0.0002:.0f} Google searches\")\n</code></pre>"},{"location":"user-guide/energy/#cli-integration","title":"CLI Integration","text":"<p>Enable energy measurement in CLI commands:</p> <pre><code># With RAPL (Linux)\ngreenmining analyze-url https://github.com/pallets/flask --energy --energy-backend rapl\n\n# With CodeCarbon\ngreenmining analyze-url https://github.com/pallets/flask --energy --energy-backend codecarbon\n</code></pre>"},{"location":"user-guide/energy/#environment-configuration","title":"Environment Configuration","text":"<pre><code>export ENERGY_ENABLED=true\nexport ENERGY_BACKEND=rapl\nexport CARBON_TRACKING=true\nexport COUNTRY_ISO=USA\n</code></pre>"},{"location":"user-guide/energy/#energy-metrics","title":"Energy Metrics","text":""},{"location":"user-guide/energy/#energymetrics-class","title":"EnergyMetrics Class","text":"<pre><code>from greenmining.energy.base import EnergyMetrics\n\n@dataclass\nclass EnergyMetrics:\n    energy_joules: float       # Total energy in Joules\n    duration_seconds: float    # Measurement duration\n    average_power_watts: float # Average power draw\n    start_time: datetime       # Measurement start\n    end_time: datetime         # Measurement end\n\n    # CodeCarbon specific\n    energy_kwh: float = 0.0    # Energy in kilowatt-hours\n    emissions_kg: float = 0.0  # CO2 emissions in kg\n</code></pre>"},{"location":"user-guide/energy/#commitenergyprofile","title":"CommitEnergyProfile","text":"<p>Track energy per commit analysis:</p> <pre><code>from greenmining.energy.base import CommitEnergyProfile\n\n@dataclass\nclass CommitEnergyProfile:\n    commit_sha: str\n    energy_joules: float\n    duration_seconds: float\n    patterns_detected: list\n    files_analyzed: int\n</code></pre>"},{"location":"user-guide/energy/#research-applications","title":"Research Applications","text":""},{"location":"user-guide/energy/#measuring-analysis-efficiency","title":"Measuring Analysis Efficiency","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nmeter = RAPLEnergyMeter()\nanalyzer = LocalRepoAnalyzer()\n\nrepos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n]\n\nresults = []\nfor url in repos:\n    meter.start()\n    analysis = analyzer.analyze_repository(url, max_commits=100)\n    energy = meter.stop()\n\n    results.append({\n        \"repo\": url,\n        \"commits\": analysis[\"total_commits\"],\n        \"energy_joules\": energy.energy_joules,\n        \"joules_per_commit\": energy.energy_joules / analysis[\"total_commits\"]\n    })\n\n# Compare efficiency\nfor r in results:\n    print(f\"{r['repo']}: {r['joules_per_commit']:.3f} J/commit\")\n</code></pre>"},{"location":"user-guide/energy/#energy-aware-batch-processing","title":"Energy-Aware Batch Processing","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\nenergy_budget_joules = 1000  # Set energy budget\n\ntotal_energy = 0\nanalyzed_repos = []\n\nfor repo in repositories:\n    meter.start()\n    result = analyze(repo)\n    metrics = meter.stop()\n\n    total_energy += metrics.energy_joules\n    analyzed_repos.append(repo)\n\n    if total_energy &gt;= energy_budget_joules:\n        print(f\"Energy budget reached after {len(analyzed_repos)} repos\")\n        break\n</code></pre>"},{"location":"user-guide/energy/#comparing-backends","title":"Comparing Backends","text":"Feature RAPL CodeCarbon Accuracy High (hardware) Medium (estimation) Platform Linux only Cross-platform Granularity Microseconds Seconds CO2 tracking No Yes Cloud support No Yes Setup May need root pip install"},{"location":"user-guide/energy/#recommendation","title":"Recommendation","text":"<ul> <li>Use RAPL for precise measurements on Linux</li> <li>Use CodeCarbon for cross-platform and carbon tracking</li> </ul>"},{"location":"user-guide/energy/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/energy/#rapl-not-available","title":"RAPL Not Available","text":"<pre><code># Check if RAPL files exist\nimport os\nrapl_path = \"/sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\"\nprint(f\"RAPL exists: {os.path.exists(rapl_path)}\")\n\n# Check permissions\nif os.path.exists(rapl_path):\n    print(f\"Readable: {os.access(rapl_path, os.R_OK)}\")\n</code></pre>"},{"location":"user-guide/energy/#codecarbon-import-error","title":"CodeCarbon Import Error","text":"<pre><code># Install with all dependencies\npip install codecarbon[viz]\n\n# Or minimal install\npip install codecarbon\n</code></pre>"},{"location":"user-guide/energy/#virtual-machine-limitations","title":"Virtual Machine Limitations","text":"<p>RAPL typically doesn't work in VMs. Use CodeCarbon instead:</p> <pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\nfrom greenmining.energy.codecarbon_meter import CodeCarbonMeter\n\n# Try RAPL first, fall back to CodeCarbon\nrapl = RAPLEnergyMeter()\nif rapl.is_available():\n    meter = rapl\nelse:\n    print(\"RAPL not available, using CodeCarbon\")\n    meter = CodeCarbonMeter()\n</code></pre>"},{"location":"user-guide/energy/#next-steps","title":"Next Steps","text":"<ul> <li>Python API - Full API reference</li> <li>CLI Commands - Command-line usage</li> <li>Configuration - Energy settings</li> </ul>"},{"location":"user-guide/url-analysis/","title":"URL Analysis","text":"<p>Analyze GitHub repositories directly by URL using PyDriller.</p>"},{"location":"user-guide/url-analysis/#overview","title":"Overview","text":"<p>URL analysis allows you to analyze any GitHub repository without using the GitHub API rate limits. It uses PyDriller to clone repositories locally and extract commit data with full diff information.</p>"},{"location":"user-guide/url-analysis/#benefits","title":"Benefits","text":"<ul> <li>No GitHub API limits - Clone and analyze directly</li> <li>Full commit data - Access diffs, modified files, metrics</li> <li>Process metrics - Code churn, change set size, contributor count</li> <li>DMM metrics - Delta Maintainability Model scores</li> <li>Historical analysis - Analyze any date range</li> </ul>"},{"location":"user-guide/url-analysis/#cli-usage","title":"CLI Usage","text":""},{"location":"user-guide/url-analysis/#single-repository","title":"Single Repository","text":"<pre><code># Basic analysis\ngreenmining analyze-url https://github.com/pallets/flask\n\n# With options\ngreenmining analyze-url https://github.com/pallets/flask \\\n  --max-commits 200 \\\n  --since 2024-01-01 \\\n  --to 2024-12-31 \\\n  --output flask_results.json\n</code></pre>"},{"location":"user-guide/url-analysis/#multiple-repositories","title":"Multiple Repositories","text":"<pre><code># From command line\ngreenmining analyze-urls \\\n  --urls \"https://github.com/pallets/flask,https://github.com/django/django\"\n\n# From file\ncat &gt; repos.txt &lt;&lt; EOF\nhttps://github.com/pallets/flask\nhttps://github.com/django/django\nhttps://github.com/fastapi/fastapi\nEOF\n\ngreenmining analyze-urls --file repos.txt --max-commits 100\n</code></pre>"},{"location":"user-guide/url-analysis/#python-api","title":"Python API","text":""},{"location":"user-guide/url-analysis/#localrepoanalyzer","title":"LocalRepoAnalyzer","text":"<p>The main class for URL-based analysis.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/greenmining_repos\",  # Where to clone\n    cleanup_after=True                     # Delete after analysis\n)\n</code></pre>"},{"location":"user-guide/url-analysis/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>clone_path</code> str /tmp/greenmining_repos Directory for cloning <code>cleanup_after</code> bool True Delete cloned repo after analysis"},{"location":"user-guide/url-analysis/#analyze_repository","title":"analyze_repository()","text":"<p>Analyze a single repository.</p> <pre><code>result = analyzer.analyze_repository(\n    repo_url=\"https://github.com/owner/repo\",\n    max_commits=1000,\n    since_date=datetime(2024, 1, 1),\n    to_date=datetime(2024, 12, 31)\n)\n</code></pre>"},{"location":"user-guide/url-analysis/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>repo_url</code> str (required) GitHub repository URL <code>max_commits</code> int 1000 Maximum commits to analyze <code>since_date</code> datetime None Start date filter <code>to_date</code> datetime None End date filter"},{"location":"user-guide/url-analysis/#return-value","title":"Return Value","text":"<pre><code>{\n    \"repository\": {\n        \"name\": \"flask\",\n        \"url\": \"https://github.com/pallets/flask\",\n        \"owner\": \"pallets\",\n        \"clone_path\": \"/tmp/greenmining_repos/flask\"\n    },\n    \"total_commits\": 200,\n    \"green_aware_count\": 47,\n    \"green_aware_percentage\": 23.5,\n    \"commits\": [\n        {\n            \"sha\": \"abc123...\",\n            \"message\": \"Optimize template caching\",\n            \"author\": \"developer\",\n            \"date\": \"2024-03-15T10:30:00\",\n            \"green_aware\": True,\n            \"patterns\": [\"Cache Static Data\"],\n            \"modified_files\": 3,\n            \"insertions\": 45,\n            \"deletions\": 12,\n            \"dmm_unit_size\": 0.85,\n            \"dmm_unit_complexity\": 0.72,\n            \"dmm_unit_interfacing\": 0.90\n        },\n        ...\n    ],\n    \"pattern_distribution\": {\n        \"Cache Static Data\": 15,\n        \"Use Async Instead of Sync\": 12,\n        ...\n    },\n    \"process_metrics\": {\n        \"change_set\": {\"max\": 25, \"avg\": 5.2},\n        \"code_churn\": {\"added\": 5000, \"removed\": 2000},\n        \"contributors_count\": 45\n    }\n}\n</code></pre>"},{"location":"user-guide/url-analysis/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Analyze Flask repository for green patterns.\"\"\"\n\nfrom datetime import datetime\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\n# Initialize analyzer\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/flask_analysis\",\n    cleanup_after=True\n)\n\n# Analyze repository\nprint(\"Analyzing Flask repository...\")\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=100,\n    since_date=datetime(2024, 1, 1)\n)\n\n# Print summary\nprint(f\"\\n{'='*60}\")\nprint(\"ANALYSIS RESULTS\")\nprint(f\"{'='*60}\")\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Total commits: {result['total_commits']}\")\nprint(f\"Green-aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\n# Top patterns\nprint(f\"\\nTop Patterns:\")\nfor pattern, count in sorted(\n    result['pattern_distribution'].items(), \n    key=lambda x: x[1], \n    reverse=True\n)[:5]:\n    print(f\"  {pattern}: {count}\")\n\n# Sample green commits\nprint(f\"\\nSample Green Commits:\")\ngreen_commits = [c for c in result['commits'] if c['green_aware']]\nfor commit in green_commits[:5]:\n    print(f\"  \ud83c\udf31 {commit['message'][:60]}...\")\n    print(f\"     Patterns: {commit['patterns']}\")\n</code></pre> <p>Output:</p> <pre><code>Analyzing Flask repository...\n\n============================================================\nANALYSIS RESULTS\n============================================================\nRepository: flask\nTotal commits: 100\nGreen-aware: 23 (23.0%)\n\nTop Patterns:\n  Cache Static Data: 8\n  Use Async Instead of Sync: 5\n  Lazy Loading: 4\n  Compress Transmitted Data: 3\n  Optimize Database Queries: 3\n\nSample Green Commits:\n  \ud83c\udf31 Implement response caching for static assets...\n     Patterns: ['Cache Static Data']\n  \ud83c\udf31 Add async support for request handling...\n     Patterns: ['Use Async Instead of Sync']\n</code></pre>"},{"location":"user-guide/url-analysis/#supported-url-formats","title":"Supported URL Formats","text":"<pre><code># HTTPS (recommended)\n\"https://github.com/owner/repo\"\n\"https://github.com/owner/repo.git\"\n\n# SSH\n\"git@github.com:owner/repo.git\"\n\n# With branch (coming soon)\n\"https://github.com/owner/repo/tree/branch-name\"\n</code></pre>"},{"location":"user-guide/url-analysis/#pydriller-integration","title":"PyDriller Integration","text":"<p>GreenMining uses PyDriller for commit extraction with these metrics:</p>"},{"location":"user-guide/url-analysis/#commit-metrics","title":"Commit Metrics","text":"Metric Description <code>modified_files</code> Number of files changed <code>insertions</code> Lines added <code>deletions</code> Lines removed <code>files</code> List of modified file paths"},{"location":"user-guide/url-analysis/#dmm-metrics-delta-maintainability-model","title":"DMM Metrics (Delta Maintainability Model)","text":"Metric Range Description <code>dmm_unit_size</code> 0-1 Unit size maintainability <code>dmm_unit_complexity</code> 0-1 Cyclomatic complexity impact <code>dmm_unit_interfacing</code> 0-1 Interface complexity"},{"location":"user-guide/url-analysis/#process-metrics","title":"Process Metrics","text":"Metric Description <code>change_set</code> Files changed per commit (max, avg) <code>code_churn</code> Lines added/removed over time <code>contributors_count</code> Unique contributors <code>commits_count</code> Total commits in period"},{"location":"user-guide/url-analysis/#configuration-options","title":"Configuration Options","text":"<p>Configure URL analysis via environment variables or Config:</p> <pre><code># Environment variables\nexport CLONE_PATH=/custom/path\nexport CLEANUP_AFTER_ANALYSIS=false\nexport PROCESS_METRICS_ENABLED=true\nexport DMM_ENABLED=true\n</code></pre> <pre><code># Python configuration\nfrom greenmining.config import Config\n\nconfig = Config()\nprint(config.CLONE_PATH)               # /tmp/greenmining_repos\nprint(config.CLEANUP_AFTER_ANALYSIS)   # True\nprint(config.PROCESS_METRICS_ENABLED)  # True\nprint(config.DMM_ENABLED)              # True\n</code></pre>"},{"location":"user-guide/url-analysis/#batch-analysis","title":"Batch Analysis","text":"<p>Analyze multiple repositories efficiently:</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nimport json\n\nrepos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n    \"https://github.com/fastapi/fastapi\",\n]\n\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\nall_results = []\n\nfor url in repos:\n    print(f\"Analyzing {url}...\")\n    result = analyzer.analyze_repository(url, max_commits=100)\n    all_results.append(result)\n    print(f\"  \u2713 {result['green_aware_count']}/{result['total_commits']} green-aware\")\n\n# Save combined results\nwith open(\"batch_results.json\", \"w\") as f:\n    json.dump(all_results, f, indent=2, default=str)\n\n# Summary\nprint(f\"\\nTotal repositories: {len(all_results)}\")\ntotal_commits = sum(r['total_commits'] for r in all_results)\ntotal_green = sum(r['green_aware_count'] for r in all_results)\nprint(f\"Total commits: {total_commits}\")\nprint(f\"Total green-aware: {total_green} ({total_green/total_commits*100:.1f}%)\")\n</code></pre>"},{"location":"user-guide/url-analysis/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/url-analysis/#clone-failures","title":"Clone Failures","text":"<pre><code># Increase timeout\nanalyzer = LocalRepoAnalyzer(clone_timeout=300)  # 5 minutes\n\n# Use SSH for private repos\nresult = analyzer.analyze_repository(\"git@github.com:org/private-repo.git\")\n</code></pre>"},{"location":"user-guide/url-analysis/#large-repositories","title":"Large Repositories","text":"<pre><code># Limit commits for large repos\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/kubernetes/kubernetes\",\n    max_commits=500  # Limit for faster analysis\n)\n</code></pre>"},{"location":"user-guide/url-analysis/#disk-space","title":"Disk Space","text":"<pre><code># Always cleanup\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\n\n# Or manual cleanup\nimport shutil\nshutil.rmtree(\"/tmp/greenmining_repos\")\n</code></pre>"},{"location":"user-guide/url-analysis/#next-steps","title":"Next Steps","text":"<ul> <li>Energy Measurement - Measure energy during analysis</li> <li>Python API - Full API reference</li> <li>CLI Commands - Command-line options</li> </ul>"}]}