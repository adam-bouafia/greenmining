{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GreenMining Documentation","text":"<p> An empirical Python library for Mining Software Repositories (MSR) in Green IT research </p> <p> </p>"},{"location":"#what-is-greenmining","title":"What is GreenMining?","text":"<p>GreenMining is an empirical research tool for Mining Software Repositories (MSR) focused on Green IT and sustainable software practices. It provides researchers and practitioners with a comprehensive toolkit for:</p> <ul> <li>Mining repositories at scale to study green software evolution</li> <li>Classifying commits using the Green Software Foundation (GSF) pattern catalog</li> <li>Measuring energy consumption of software systems and analysis workloads</li> <li>Analyzing temporal trends and adoption patterns across projects</li> <li>Generating research-ready datasets with statistical analysis</li> </ul> <p>The library performs deep repository analysis with full commit extraction and supports multiple energy measurement backends (RAPL, CodeCarbon) for empirical Green IT research.</p>"},{"location":"#key-capabilities","title":"Key Capabilities","text":"Feature Description 124 GSF Patterns Detect patterns across 15 categories (cloud, web, AI, caching, etc.) 332 Green Keywords Comprehensive keyword matching for green-aware commits GitHub Mining Fetch repositories by keywords, stars, language filters URL Analysis Analyze any GitHub repo directly via URL with full commit extraction Statistical Analysis Pattern correlations, temporal trends, effect sizes Energy Measurement RAPL and CodeCarbon backends for power profiling Multiple Outputs JSON, CSV, and Markdown reports"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install greenmining\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Python API:</p> <pre><code>from greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\n\n# Check if a commit message is green-aware\nmessage = \"Optimize Redis caching to reduce energy consumption\"\nprint(is_green_aware(message))  # True\n\n# Get matched patterns\npatterns = get_pattern_by_keywords(message)\nprint(patterns)  # ['Cache Static Data']\n</code></pre> <p>Analyze a Repository:</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\"https://github.com/pallets/flask\")\n\nprint(f\"Green-aware: {result['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"#pattern-categories","title":"Pattern Categories","text":"<p>GreenMining detects 124 patterns across 15 categories:</p> Category Patterns Examples Cloud 40+ Caching, compression, auto-scaling, serverless Web 15+ Lazy loading, image optimization, minification AI/ML 10+ Model optimization, quantization, efficient training Caching 8 Redis, CDN, static data caching Async 6 Batch processing, queue-based architecture Database 8 Query optimization, connection pooling Network 6 Compression, CDN, efficient protocols Resource 5 Memory management, CPU optimization Code 4 Dead code removal, algorithm efficiency Infrastructure 4 Container optimization, IaC Microservices 4 Service decomposition, graceful shutdown Monitoring 3 Efficient logging, metrics collection General 8 Feature flags, incremental processing"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Install GreenMining via pip, source, or Docker</li> <li>Quick Start - Run your first analysis in 5 minutes</li> <li>Configuration - Configure via environment variables or config files</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Python API - Use GreenMining programmatically</li> <li>URL Analysis - Analyze repositories directly by URL</li> <li>Energy Measurement - Measure energy consumption with RAPL/CodeCarbon</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>GSF Patterns - All 124 patterns with categories and keywords</li> <li>Configuration - All configuration parameters</li> <li>Data Models - Repository, Commit, and AnalysisResult models</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Complete Pipeline - Full experiment workflow example</li> </ul>"},{"location":"#project","title":"Project","text":"<ul> <li>Contributing - How to contribute to GreenMining</li> <li>Changelog - Version history and release notes</li> </ul>"},{"location":"#example-output","title":"Example Output","text":"<p>When analyzing a repository, GreenMining produces reports like:</p> <pre><code>============================================================\nGREENMINING ANALYSIS RESULTS\n============================================================\n\nRepository: kubernetes/kubernetes\nCommits analyzed: 1000\nGreen-aware commits: 247 (24.7%)\n\nTop Patterns Detected:\n  1. Cache Static Data (89 commits)\n  2. Use Async Instead of Sync (67 commits)\n  3. Containerize Workload (45 commits)\n  4. Compress Transmitted Data (31 commits)\n\nCategories Distribution:\n  cloud: 45.2%\n  caching: 23.1%\n  async: 18.5%\n  infrastructure: 13.2%\n</code></pre>"},{"location":"#research-applications","title":"Research Applications","text":"<p>GreenMining is designed for empirical MSR research in Green IT:</p>"},{"location":"#mining-software-repositories-msr","title":"Mining Software Repositories (MSR)","text":"<ul> <li>Large-scale repository mining with GitHub API and GraphQL</li> <li>Configurable filters (stars, languages, dates, keywords)</li> <li>Batch processing with rate limit handling</li> </ul>"},{"location":"#green-it-pattern-analysis","title":"Green IT Pattern Analysis","text":"<ul> <li>124 GSF patterns across 15 sustainability categories</li> <li>Keyword-based commit classification with confidence scoring</li> <li>Pattern co-occurrence and correlation analysis</li> </ul>"},{"location":"#temporal-statistical-analysis","title":"Temporal &amp; Statistical Analysis","text":"<ul> <li>Trend analysis at configurable granularity (day/week/month/quarter/year)</li> <li>Effect size calculations (Cohen's d, Cliff's delta)</li> <li>Cross-repository comparative studies</li> </ul>"},{"location":"#energy-measurement","title":"Energy Measurement","text":"<ul> <li>RAPL backend for direct CPU/DRAM power measurement (Linux)</li> <li>CodeCarbon integration for cross-platform emissions tracking</li> <li>Energy profiling of analysis workloads</li> </ul>"},{"location":"#research-outputs","title":"Research Outputs","text":"<ul> <li>JSON, CSV, and Markdown report generation</li> <li>Publication-ready statistical summaries</li> <li>Reproducible analysis pipelines</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use GreenMining in your research, please cite:</p> <pre><code>@software{greenmining2024,\n  author = {Bouafia, Adam},\n  title = {GreenMining: Mining Green Software Patterns},\n  year = {2024},\n  url = {https://github.com/adam-bouafia/greenmining}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>GreenMining is released under the MIT License.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#120-2026-01-31","title":"1.2.0 - 2026-01-31","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li><code>clone_repositories()</code> top-level function for cloning repos into <code>./greenmining_repos/</code> with sanitized directory names</li> <li>Repository name sanitization (<code>_sanitize_repo_name</code>) to prevent filesystem issues from special characters</li> <li>2 missing official GSF patterns: \"Match Utilization Requirements with Pre-configured Servers\", \"Optimize Impact on Customer Devices and Equipment\"</li> <li>11 new green keywords (energy proportionality, backward compatible, customer device, device lifetime, etc.)</li> <li>GSF pattern database now covers 100% of the official Green Software Foundation catalog (61/61)</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Repositories now clone to <code>./greenmining_repos/</code> instead of <code>/tmp</code> (fixes OS cleanup and permission issues)</li> <li><code>fetch_repositories()</code> takes direct parameters -- no Config intermediary</li> <li>All function defaults are explicit parameters instead of config file values</li> <li>Default supported languages updated from 7 to 20 (matches experiment scope)</li> <li>Library reference documentation added to mkdocs navigation</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li><code>config.py</code> module entirely (Config class, get_config singleton, .env/YAML loading layer)</li> <li><code>__version__.py</code> (stale orphaned file with wrong version 1.0.5)</li> <li><code>services/github_fetcher.py</code> (empty deprecated REST API stub)</li> <li><code>analyzers/power_regression.py</code> (PowerRegressionDetector -- requires running repo code, not feasible in current pipeline)</li> <li><code>analyzers/version_power_analyzer.py</code> (VersionPowerAnalyzer -- same reason)</li> <li><code>analyzers/qualitative_analyzer.py</code> (QualitativeAnalyzer -- unused)</li> <li><code>presenters/</code> module (ConsolePresenter -- never used by any code)</li> <li><code>docs/reference/config-options.md</code> (obsolete config reference page)</li> <li>10 dead utility functions (estimate_tokens, estimate_cost, print_banner, print_section, load_csv_file, handle_github_rate_limit, format_duration, truncate_text, create_checkpoint, load_checkpoint)</li> <li>35+ unused Config attributes that were set but never read</li> <li>Dead imports across 14 files</li> <li>Dead methods: DataAnalyzer._check_green_awareness, DataAnalyzer._detect_known_pattern, CommitExtractor._extract_commit_metadata, StatisticalAnalyzer.pattern_adoption_rate_analysis, CodeCarbonMeter.get_carbon_intensity, Config.validate</li> </ul>"},{"location":"changelog/#119-2026-01-31","title":"1.1.9 - 2026-01-31","text":""},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Web dashboard module (<code>greenmining/dashboard/</code>) and Flask dependency</li> <li>Dashboard documentation page and all dashboard references</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>ReadTheDocs experiment page not rendering (trailing whitespace in mkdocs nav)</li> <li>Plotly rendering in notebook (nbformat dependency)</li> </ul>"},{"location":"changelog/#116-2026-01-31","title":"1.1.6 - 2026-01-31","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>EnergyMetrics property aliases (<code>energy_joules</code>, <code>average_power_watts</code>)</li> <li>Parallel energy measurement conflict with shared meter instance</li> <li>StatisticalAnalyzer timezone-aware date handling</li> <li>DataFrame column collision in pattern correlation analysis</li> </ul>"},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li><code>since_date</code> / <code>to_date</code> parameters for date-bounded commit analysis</li> <li><code>created_before</code> / <code>pushed_after</code> search filters</li> <li>GraphQL API and experiment documentation pages</li> <li>Full process metrics and method-level metrics documentation</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Energy measurement demonstrates all 4 backends: RAPL, CPU Meter, CodeCarbon, tracemalloc</li> <li>Removed all PyDriller references (replaced with gitpython + lizard)</li> </ul>"},{"location":"changelog/#removed_2","title":"Removed","text":"<ul> <li>Qualitative Validation and Carbon Footprint Reporting steps from experiment</li> </ul>"},{"location":"changelog/#0112-2025-12-03","title":"0.1.12 - 2025-12-03","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Custom search keywords for repository fetching (<code>--keywords</code> option)</li> <li><code>fetch_repositories()</code> function exposed in public API</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>README updated to reflect 122 patterns (was showing 76 in PyPI description)</li> </ul>"},{"location":"changelog/#0111-2025-12-03","title":"0.1.11 - 2025-12-03","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Expanded pattern database from 76 to 122 patterns</li> <li>Added 9 new categories</li> <li>Expanded keywords from 190 to 321</li> <li>VU Amsterdam 2024 research patterns for ML systems</li> </ul>"},{"location":"changelog/#010-2025-12-02","title":"0.1.0 - 2025-12-02","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Initial release</li> <li>Core functionality for GSF pattern mining</li> <li>Support for 100 microservices repositories</li> <li>Pattern matching with 76 GSF patterns</li> <li>Green awareness analysis</li> <li>Docker containerization</li> </ul>"},{"location":"contributing/","title":"Contributing to GreenMining","text":"<p>Thank you for your interest in contributing to GreenMining! </p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code># Fork on GitHub, then:\ngit clone https://github.com/YOUR-USERNAME/greenmining.git\ncd greenmining\n</code></pre>"},{"location":"contributing/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<pre><code># Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\n# or: venv\\Scripts\\activate  # Windows\n\n# Install in development mode\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\npytest\n\n# With coverage\npytest --cov=greenmining --cov-report=html\n\n# Specific test file\npytest tests/test_gsf_patterns.py -v\n</code></pre>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We follow PEP 8 with these tools:</p> <pre><code># Format code\nblack greenmining/ tests/\n\n# Check types\nmypy greenmining/\n\n# Lint\nruff check greenmining/\n</code></pre>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Configuration (<code>.pre-commit-config.yaml</code>):</p> <pre><code>repos:\n  - repo: https://github.com/psf/black\n    rev: 23.12.1\n    hooks:\n      - id: black\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.1.9\n    hooks:\n      - id: ruff\n</code></pre>"},{"location":"contributing/#contribution-types","title":"Contribution Types","text":""},{"location":"contributing/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<ol> <li>Search existing issues first</li> <li>Create a new issue with:</li> <li>GreenMining version (<code>pip show greenmining</code>)</li> <li>Python version</li> <li>Operating system</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Error messages/tracebacks</li> </ol>"},{"location":"contributing/#feature-requests","title":"\u2728 Feature Requests","text":"<ol> <li>Check existing issues and roadmap</li> <li>Create an issue describing:</li> <li>Use case</li> <li>Proposed solution</li> <li>Alternatives considered</li> </ol>"},{"location":"contributing/#documentation","title":"\ud83d\udcdd Documentation","text":"<ul> <li>Fix typos and clarify explanations</li> <li>Add examples</li> <li>Improve docstrings</li> <li>Update README</li> </ul>"},{"location":"contributing/#code-contributions","title":"\ud83d\udd27 Code Contributions","text":"<ol> <li>Open an issue to discuss major changes</li> <li>Fork and create a feature branch</li> <li>Write tests for new functionality</li> <li>Ensure all tests pass</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#adding-new-gsf-patterns","title":"Adding New GSF Patterns","text":""},{"location":"contributing/#1-update-pattern-definition","title":"1. Update Pattern Definition","text":"<p>Edit <code>greenmining/gsf_patterns.py</code>:</p> <pre><code>GSF_PATTERNS = {\n    # ... existing patterns ...\n\n    \"new_pattern_key\": {\n        \"name\": \"New Pattern Name\",\n        \"category\": \"category_name\",  # cloud, web, ai, caching, etc.\n        \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n        \"description\": \"Brief description of the pattern\",\n        \"sci_impact\": \"How this pattern reduces software carbon intensity\"\n    },\n}\n</code></pre>"},{"location":"contributing/#2-add-keywords-to-green_keywords","title":"2. Add Keywords to GREEN_KEYWORDS","text":"<pre><code>GREEN_KEYWORDS = [\n    # ... existing keywords ...\n    \"keyword1\",\n    \"keyword2\", \n    \"keyword3\",\n]\n</code></pre>"},{"location":"contributing/#3-write-tests","title":"3. Write Tests","text":"<pre><code># tests/test_gsf_patterns.py\n\ndef test_new_pattern_detection():\n    \"\"\"Test that new pattern is detected correctly.\"\"\"\n    from greenmining import is_green_aware, get_pattern_by_keywords\n\n    # Should detect\n    assert is_green_aware(\"message with keyword1\")\n    patterns = get_pattern_by_keywords(\"message with keyword1\")\n    assert \"New Pattern Name\" in patterns\n\n    # Should not detect\n    assert not is_green_aware(\"unrelated message\")\n</code></pre>"},{"location":"contributing/#4-update-documentation","title":"4. Update Documentation","text":"<p>Add pattern to <code>docs/reference/patterns.md</code></p>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>greenmining/\n\u251c\u2500\u2500 __init__.py          # Package exports\n\u251c\u2500\u2500 __main__.py          # Entry point\n\u251c\u2500\u2500 config.py            # Configuration\n\u251c\u2500\u2500 gsf_patterns.py      # Pattern definitions\n\u251c\u2500\u2500 utils.py             # Utilities\n\u251c\u2500\u2500 analyzers/           # Analysis modules\n\u2502   \u251c\u2500\u2500 qualitative_analyzer.py\n\u2502   \u251c\u2500\u2500 statistical_analyzer.py\n\u2502   \u2514\u2500\u2500 temporal_analyzer.py\n\u251c\u2500\u2500 controllers/         # Controllers\n\u2502   \u2514\u2500\u2500 repository_controller.py\n\u251c\u2500\u2500 energy/              # Energy measurement\n\u2502   \u251c\u2500\u2500 base.py\n\u2502   \u251c\u2500\u2500 rapl_backend.py\n\u2502   \u2514\u2500\u2500 codecarbon_backend.py\n\u251c\u2500\u2500 models/              # Data models\n\u251c\u2500\u2500 presenters/          # Output formatting\n\u2502   \u2514\u2500\u2500 console_presenter.py\n\u2514\u2500\u2500 services/            # Core services\n    \u251c\u2500\u2500 commit_extractor.py\n    \u251c\u2500\u2500 data_aggregator.py\n    \u251c\u2500\u2500 data_analyzer.py\n    \u251c\u2500\u2500 github_fetcher.py\n    \u251c\u2500\u2500 local_repo_analyzer.py\n    \u2514\u2500\u2500 reports.py\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#1-create-branch","title":"1. Create Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/issue-description\n</code></pre>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write clean, documented code</li> <li>Add tests for new functionality</li> <li>Update documentation if needed</li> </ul>"},{"location":"contributing/#3-commit-with-clear-messages","title":"3. Commit with Clear Messages","text":"<pre><code>git add .\ngit commit -m \"feat: add new pattern detection for X\"\n# or\ngit commit -m \"fix: resolve issue with Y\"\n# or\ngit commit -m \"docs: update installation instructions\"\n</code></pre> <p>Follow Conventional Commits: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation - <code>test:</code> - Tests - <code>refactor:</code> - Code refactoring - <code>chore:</code> - Maintenance</p>"},{"location":"contributing/#4-push-and-create-pr","title":"4. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub with: - Clear title - Description of changes - Link to related issue - Screenshots if UI changes</p>"},{"location":"contributing/#5-address-review-feedback","title":"5. Address Review Feedback","text":"<p>Respond to reviewer comments and make requested changes.</p>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#test-structure","title":"Test Structure","text":"<pre><code># tests/test_module.py\n\nimport pytest\nfrom greenmining import function_to_test\n\nclass TestFunctionName:\n    \"\"\"Tests for function_to_test.\"\"\"\n\n    def test_basic_functionality(self):\n        \"\"\"Test basic happy path.\"\"\"\n        result = function_to_test(\"input\")\n        assert result == \"expected\"\n\n    def test_edge_case(self):\n        \"\"\"Test edge case handling.\"\"\"\n        result = function_to_test(\"\")\n        assert result is None\n\n    def test_error_handling(self):\n        \"\"\"Test error conditions.\"\"\"\n        with pytest.raises(ValueError):\n            function_to_test(None)\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest\n\n# Verbose output\npytest -v\n\n# Specific test\npytest tests/test_gsf_patterns.py::test_pattern_detection -v\n\n# With coverage\npytest --cov=greenmining --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help newcomers</li> <li>Acknowledge contributions</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b Issue Tracker</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"examples/experiment/","title":"Experiment: Unified Repository Analysis Pipeline","text":"<p>This page describes the reference experiment included with GreenMining. The Jupyter notebook (<code>experiment/beta/experiment.ipynb</code>) demonstrates every feature of the library in a single, end-to-end pipeline applied to 13 repositories.</p>"},{"location":"examples/experiment/#overview","title":"Overview","text":"<p>The experiment follows a four-part structure:</p> Part Purpose Library Features Used A: Setup Install, import, configure <code>greenmining</code> package, <code>GSF_PATTERNS</code>, <code>GREEN_KEYWORDS</code> B: Data Gathering Fetch and analyze repositories <code>fetch_repositories</code>, <code>analyze_repositories</code> C: Unified Analysis Apply every analyzer to the combined dataset All analyzers, energy, visualization, export D: Summary Feature coverage table and output files --"},{"location":"examples/experiment/#part-a-setup","title":"Part A: Setup","text":""},{"location":"examples/experiment/#step-1-install","title":"Step 1 -- Install","text":"<pre><code>!pip install greenmining[energy] --upgrade --quiet\n</code></pre> <p>The <code>[energy]</code> extra installs optional dependencies for energy measurement (CodeCarbon).</p>"},{"location":"examples/experiment/#step-2-imports","title":"Step 2 -- Imports","text":"<p>All modules are imported from the library:</p> <pre><code>from greenmining import (\n    fetch_repositories,\n    analyze_repositories,\n    GSF_PATTERNS,\n    GREEN_KEYWORDS,\n    is_green_aware,\n    get_pattern_by_keywords,\n)\nfrom greenmining.analyzers import (\n    StatisticalAnalyzer,\n    TemporalAnalyzer,\n    CodeDiffAnalyzer,\n    PowerRegressionDetector,\n    MetricsPowerCorrelator,\n    VersionPowerAnalyzer,\n)\nfrom greenmining.energy import get_energy_meter, CPUEnergyMeter\nimport tracemalloc\n</code></pre> <pre><code>GreenMining version: 1.1.9\nGSF Patterns: 124\nGreen Keywords: 332\n</code></pre>"},{"location":"examples/experiment/#step-3-configuration","title":"Step 3 -- Configuration","text":"<p>Shared parameters used across the entire pipeline:</p> Parameter Value Description <code>MAX_COMMITS</code> 20 Commits analyzed per repository <code>MIN_STARS</code> 3 Minimum GitHub stars for search <code>PARALLEL_WORKERS</code> 2 Concurrent repository analysis <code>LANGUAGES</code> 20 Top programming languages <code>CREATED_AFTER</code> <code>2020-01-01</code> Repository creation lower bound <code>CREATED_BEFORE</code> <code>2023-12-31</code> Repository creation upper bound <code>PUSHED_AFTER</code> <code>2023-01-01</code> Recent activity filter <code>COMMIT_DATE_FROM</code> <code>2023-01-01</code> Commit analysis start date <code>COMMIT_DATE_TO</code> <code>2025-12-31</code> Commit analysis end date <p>The GitHub token is loaded from the environment or a <code>.env</code> file.</p>"},{"location":"examples/experiment/#part-b-data-gathering","title":"Part B: Data Gathering","text":""},{"location":"examples/experiment/#step-4-search-devops-repositories","title":"Step 4 -- Search DevOps Repositories","text":"<p>Uses <code>fetch_repositories()</code> with the GraphQL API v4 to find 10 devops repositories:</p> <pre><code>search_repos = fetch_repositories(\n    github_token=GITHUB_TOKEN,\n    max_repos=10,\n    min_stars=MIN_STARS,\n    languages=LANGUAGES,\n    keywords=\"devops\",\n    created_after=CREATED_AFTER,\n    created_before=CREATED_BEFORE,\n    pushed_after=PUSHED_AFTER,\n)\n</code></pre> <pre><code>Found 10 devops repositories:\n   1. antonputra/tutorials (4410 stars, HCL)\n   2. geerlingguy/ansible-for-devops (9598 stars, Python)\n   3. in28minutes/devops-master-class (2818 stars, Java)\n   4. hygieia/hygieia (3905 stars, TypeScript)\n   5. stacksimplify/aws-eks-kubernetes-masterclass (1710 stars, Java)\n   6. iam-veeramalla/python-for-devops (4260 stars, Python)\n   7. milanm/DevOps-Roadmap (18639 stars, None)\n   8. bregman-arie/devops-exercises (80815 stars, Python)\n   9. ravdy/DevOps (256 stars, Jinja)\n  10. MicrosoftDocs/mslearn-tailspin-spacegame-web (212 stars, Shell)\n</code></pre> <p>Library feature: <code>greenmining.fetch_repositories</code> wraps <code>GitHubGraphQLFetcher.search_repositories</code>. When more than 5 languages are provided, the language filter is skipped to stay within GitHub query complexity limits. Date filters (<code>created_after</code>, <code>created_before</code>, <code>pushed_after</code>) narrow the search to repositories within specific time windows.</p>"},{"location":"examples/experiment/#step-5-analyze-all-13-repositories","title":"Step 5 -- Analyze All 13 Repositories","text":"<p>Combines the 10 searched repositories with 3 manually selected ones (Flask, Requests, FastAPI), then runs the full analysis pipeline on all of them at once:</p> <pre><code>raw_results = analyze_repositories(\n    urls=all_urls,\n    max_commits=MAX_COMMITS,\n    parallel_workers=PARALLEL_WORKERS,\n    output_format=\"dict\",\n    energy_tracking=True,\n    energy_backend=\"auto\",\n    method_level_analysis=True,\n    include_source_code=True,\n    github_token=GITHUB_TOKEN,\n    since_date=COMMIT_DATE_FROM,\n    to_date=COMMIT_DATE_TO,\n)\nresults = [r.to_dict() for r in raw_results]\n</code></pre> <pre><code>Analysis complete: 13 repositories\n</code></pre> <p><code>analyze_repositories()</code> returns a list of <code>RepositoryAnalysis</code> dataclass objects. These are converted to dictionaries with <code>.to_dict()</code> for downstream processing.</p> <p>Library features applied per repository:</p> <ul> <li>GSF pattern detection (124 patterns, 15 categories, 332 keywords)</li> <li>Process metrics (DMM unit size, complexity, interfacing)</li> <li>Method-level analysis (per-function complexity via Lizard)</li> <li>Source code capture (before/after for each modified file)</li> <li>Energy measurement (auto-detected backend)</li> <li>Date-bounded commit analysis (<code>since_date</code> / <code>to_date</code>)</li> </ul>"},{"location":"examples/experiment/#step-6-results-overview","title":"Step 6 -- Results Overview","text":"<p>Prints a summary table and builds a flat <code>all_commits</code> list used by every subsequent analysis step.</p> <pre><code>======================================================================\nUNIFIED ANALYSIS SUMMARY\n======================================================================\nRepositories analyzed: 13\nTotal commits: 187\nGreen-aware commits: 55\nOverall green rate: 29.4%\n\nRepository                               Commits    Green      Rate\n----------------------------------------------------------------------\ngeerlingguy/ansible-for-devops           14         7          50.0%\nin28minutes/devops-master-class          2          1          50.0%\nhygieia/hygieia                          2          0          0.0%\nstacksimplify/aws-eks-kubernetes-masterclass 20     4          20.0%\nantonputra/tutorials                     20         16         80.0%\niam-veeramalla/python-for-devops         20         0          0.0%\nmilanm/DevOps-Roadmap                    20         2          10.0%\nravdy/DevOps                             0          0          0.0%\nMicrosoftDocs/mslearn-tailspin-spacegame-web 9      2          22.2%\nbregman-arie/devops-exercises            20         10         50.0%\npsf/requests                             20         1          5.0%\npallets/flask                            20         6          30.0%\ntiangolo/fastapi                         20         6          30.0%\n\nFlattened commit pool: 187 commits\n</code></pre>"},{"location":"examples/experiment/#part-c-unified-analysis","title":"Part C: Unified Analysis","text":"<p>Every analyzer operates on the combined dataset of all 13 repositories.</p>"},{"location":"examples/experiment/#step-7-gsf-pattern-analysis","title":"Step 7 -- GSF Pattern Analysis","text":"<p>Counts pattern frequency across all commits and groups patterns by category.</p> <p>Library features: <code>GSF_PATTERNS</code> dictionary (124 patterns), category grouping.</p> <pre><code>Unique patterns detected: 46\n\nTop 20 GSF Patterns:\nPattern                                       Count    % of Commits\n-----------------------------------------------------------------\nKeep Request Counts Low                       17       9.1%\nContainerize Your Workload                    13       7.0%\nUse Compiled Languages                        13       7.0%\nPerformance Profiling                         10       5.3%\nDelete Unused Storage Resources               9        4.8%\nMinimize Deployed Environments                8        4.3%\nRemove Unused Assets                          7        3.7%\nServe Images in Modern Formats                7        3.7%\nScale Down Kubernetes Workloads               6        3.2%\nScale Kubernetes Workloads Based on Events    6        3.2%\nProperly Sized Images                         6        3.2%\nAvoid Excessive DOM Size                      4        2.1%\nDefer Offscreen Images                        4        2.1%\ngRPC for Service Communication                4        2.1%\nMatch VM Utilization Requirements             3        1.6%\nEliminate Polling                             3        1.6%\nReduce Network Traversal Between VMs          3        1.6%\nAvoid Chaining Critical Requests              3        1.6%\nEvaluate Using a Service Mesh                 3        1.6%\nService Mesh Optimization                     3        1.6%\n\nGSF Categories (15):\n  ai: 19 patterns          async: 3 patterns\n  caching: 2 patterns      cloud: 40 patterns\n  code: 4 patterns         data: 3 patterns\n  database: 5 patterns     general: 8 patterns\n  infrastructure: 4        microservices: 4\n  monitoring: 3            network: 6 patterns\n  networking: 2            resource: 2 patterns\n  web: 17 patterns\n</code></pre>"},{"location":"examples/experiment/#step-8-green-awareness-detection","title":"Step 8 -- Green Awareness Detection","text":"<p>Demonstrates <code>is_green_aware()</code> on sample commit messages and <code>get_pattern_by_keywords()</code> for pattern lookup.</p> <p>Library features: <code>greenmining.is_green_aware</code>, <code>greenmining.get_pattern_by_keywords</code>.</p> <pre><code>Green Awareness Detection:\n  [GREEN] Optimize database queries for energy efficiency\n  [-----] Fix typo in README\n  [GREEN] Implement lazy loading for images to reduce bandwidth\n  [-----] Add unit tests for login\n  [GREEN] Reduce memory footprint of cache layer\n  [GREEN] Refactor to async I/O for better resource utilization\n\nPattern Lookup Examples:\n  \"cache\"        -&gt; ['Cache Static Data']\n  \"lazy loading\" -&gt; ['Scale Infrastructure with User Load', 'Defer Offscreen Images', 'Lazy Loading']\n  \"compression\"  -&gt; ['Compress Stored Data', 'Compress Transmitted Data', 'Enable Text Compression']\n  \"async\"        -&gt; ['Queue Non-Urgent Requests', 'Use Async Instead of Sync', 'Lazy Loading']\n</code></pre>"},{"location":"examples/experiment/#step-9-process-metrics","title":"Step 9 -- Process Metrics","text":"<p>Inspects DMM scores, structural complexity, and method-level data collected during analysis.</p> <p>Library features: Process metrics from <code>analyze_repositories</code> with <code>method_level_analysis=True</code>.</p> Metric Description <code>dmm_unit_size</code> Delta Maintainability Model -- size (0-1) <code>dmm_unit_complexity</code> Delta Maintainability Model -- complexity (0-1) <code>dmm_unit_interfacing</code> Delta Maintainability Model -- interfacing (0-1) <code>total_nloc</code> Total non-comment lines of code <code>total_complexity</code> Cyclomatic complexity sum <code>max_complexity</code> Highest single-function complexity <code>methods_count</code> Number of methods analyzed <pre><code>Process Metrics Summary\n======================================================================\nMetric                           Avg        Min        Max      N\n-----------------------------------------------------------------\ndmm_unit_size                   0.69       0.00       1.00     27\ndmm_unit_complexity             0.87       0.00       1.00     27\ndmm_unit_interfacing            0.86       0.00       1.00     27\ntotal_nloc                    141.00       0.00    3868.00    187\ntotal_complexity               22.30       0.00     503.00    187\nmax_complexity                 14.37       0.00     308.00    187\nmethods_count                  15.12       0.00     303.00    187\ninsertions                   1899.17       0.00   59094.00    187\ndeletions                       8.36       0.00     257.00    187\n\nMethod-Level Analysis:\n  Total methods analyzed: 2828\n  Sample from stacksimplify/aws-eks-kubernetes-masterclass (5e1e0c15):\n    NotificationsApplication::main: nloc=3, complexity=1\n    NotificationsApplication::configure: nloc=3, complexity=1\n    CorsFilter::CorsFilter: nloc=2, complexity=1\n\nSource code changes captured: 2125\n</code></pre>"},{"location":"examples/experiment/#step-10-statistical-analysis","title":"Step 10 -- Statistical Analysis","text":"<p>Applies three statistical methods to the combined dataset:</p> <pre><code>stat_analyzer = StatisticalAnalyzer()\nstat_analyzer.analyze_pattern_correlations(commits_df)\nstat_analyzer.temporal_trend_analysis(commits_df)\nstat_analyzer.effect_size_analysis(green_cx, non_green_cx)\n</code></pre> <p>Library feature: <code>greenmining.analyzers.StatisticalAnalyzer</code></p> Method Output <code>analyze_pattern_correlations()</code> Significant co-occurrence pairs <code>temporal_trend_analysis()</code> Trend direction, significance, correlation <code>effect_size_analysis()</code> Cohen's d, magnitude, mean difference <pre><code>Pattern Correlation Analysis:\n  Significant pairs: 22\n    Containerize Your Workload &lt;-&gt; Scale Down Kubernetes Workloads: 0.666 (moderate)\n    Scale Down Kubernetes Workloads &lt;-&gt; Scale Kubernetes Workloads Based on Events: 1.000 (strong)\n    Delete Unused Storage Resources &lt;-&gt; Remove Unused Assets: 0.877 (strong)\n    Properly Sized Images &lt;-&gt; Defer Offscreen Images: 0.812 (strong)\n    Defer Offscreen Images &lt;-&gt; Serve Images in Modern Formats: 0.750 (strong)\n    Compress Transmitted Data &lt;-&gt; Reduce Network Traversal Between VMs: 0.814 (strong)\n\nTemporal Trend:\n  Direction: decreasing\n  Significant: False\n  Correlation: nan\n\nEffect Size (Green vs Non-Green Complexity):\n  Cohen's d: 0.148 (negligible)\n  Mean difference: 11.30\n  Significant: False\n</code></pre>"},{"location":"examples/experiment/#step-11-temporal-analysis","title":"Step 11 -- Temporal Analysis","text":"<p>Groups commits by quarter and tracks green awareness evolution over time:</p> <pre><code>temporal = TemporalAnalyzer(granularity=\"quarter\")\ntemporal_results = temporal.analyze_trends(all_commits, analysis_results_fmt)\n</code></pre> <p>Library feature: <code>greenmining.analyzers.TemporalAnalyzer</code></p> <pre><code>Temporal Analysis (10 periods):\nPeriod               Commits    Green      Rate       Patterns\n------------------------------------------------------------\n2022-Q4              1          0          0.0%      0\n2023-Q1              107        0          0.0%      0\n2023-Q2              23         0          0.0%      0\n2023-Q3              4          0          0.0%      0\n2023-Q4              25         0          0.0%      0\n2024-Q1              6          0          0.0%      0\n2024-Q2              4          0          0.0%      0\n2024-Q3              1          0          0.0%      0\n2025-Q1              1          0          0.0%      0\n2025-Q2              15         0          0.0%      0\n</code></pre> <p>Output includes period-by-period green commit rates, unique pattern counts, overall trend direction, and peak period identification.</p>"},{"location":"examples/experiment/#step-12-code-diff-pattern-signatures","title":"Step 12 -- Code Diff Pattern Signatures","text":"<p>Inspects the 15 green pattern categories that <code>CodeDiffAnalyzer</code> detects directly in code diffs:</p> <pre><code>diff_analyzer = CodeDiffAnalyzer()\nprint(diff_analyzer.PATTERN_SIGNATURES)\n</code></pre> <p>Library feature: <code>greenmining.analyzers.CodeDiffAnalyzer</code></p> <pre><code>Code Diff Pattern Signatures: 15 types\n  caching             resource_optimization    database_optimization\n  async_processing    lazy_loading             serverless_computing\n  cdn_edge            compression              model_optimization\n  efficient_protocols container_optimization   green_regions\n  auto_scaling        code_splitting           green_ml_training\n</code></pre>"},{"location":"examples/experiment/#step-13-energy-measurement","title":"Step 13 -- Energy Measurement","text":"<p>Demonstrates all four energy measurement approaches:</p> <p>RAPL (Running Average Power Limit):</p> <pre><code>from greenmining.energy import RAPLEnergyMeter\nrapl = RAPLEnergyMeter()\nresult, energy = rapl.measure(workload_function)\n</code></pre> <p>CPU Meter (universal fallback):</p> <pre><code>meter = CPUEnergyMeter()\nresult, energy = meter.measure(workload_function)\n</code></pre> <p>Memory Profiling (tracemalloc):</p> <pre><code>tracemalloc.start()\n# ... workload ...\ncurrent, peak = tracemalloc.get_traced_memory()\ntracemalloc.stop()\n</code></pre> <p>CO2 Emissions (CodeCarbon):</p> <pre><code>from codecarbon import EmissionsTracker\ntracker = EmissionsTracker(project_name=\"greenmining-experiment\")\ntracker.start()\n# ... workload ...\nemissions = tracker.stop()\n</code></pre> Backend Platform What It Measures RAPL Linux (Intel/AMD) Hardware energy counters (Joules) CPU Meter Universal CPU utilization + TDP estimate (Joules) tracemalloc Universal Peak memory allocation (bytes) CodeCarbon Cross-platform CO2 emissions (kg CO2eq) <pre><code>Available Energy Backends:\n  rapl: available (RAPLEnergyMeter)\n  codecarbon: available (CodeCarbonMeter)\n  cpu_meter: available (CPUEnergyMeter)\n\n--- RAPL Energy Meter ---\n  Energy: 2.7681 J\n  Power avg: 33.15 W\n  Duration: 0.083 s\n\n--- CPU Energy Meter ---\n  Energy: 3.4150 J\n  Power avg: 27.22 W\n  Duration: 0.125 s\n  Backend: cpu_meter\n\n--- CodeCarbon CO2 Tracking ---\n  CO2 emissions: 0.00000018 kg\n  Equivalent: 0.1765 mg CO2\n\n--- tracemalloc Memory Profiling ---\n  Current memory: 1.5 KB\n  Peak memory: 21.3 KB\n\n--- Analysis Energy (from repository pipeline) ---\n  geerlingguy/ansible-for-devops:\n    Total: 38.1038 J\n    Avg power: 27.65 W\n</code></pre>"},{"location":"examples/experiment/#step-14-power-regression-detection","title":"Step 14 -- Power Regression Detection","text":"<p>Identifies commits that caused energy consumption increases:</p> <pre><code>detector = PowerRegressionDetector(\n    test_command=\"python -c 'sum(range(100000))'\",\n    energy_backend=\"cpu_meter\",\n    threshold_percent=5.0,\n    iterations=3,\n)\nregressions = detector.detect(\n    repo_path=\"./my-repo\",\n    baseline_commit=\"HEAD~10\",\n    target_commit=\"HEAD\",\n)\n</code></pre> <p>Library feature: <code>greenmining.analyzers.PowerRegressionDetector</code></p> <p>Runs the test command at each commit, measures energy, and flags regressions above the threshold.</p> <pre><code>PowerRegressionDetector configured:\n  Test command: python -c \"sum(range(100000))\"\n  Backend: cpu_meter\n  Threshold: 5.0%\n  Iterations: 3, Warmup: 1\n</code></pre>"},{"location":"examples/experiment/#step-15-metrics-to-power-correlation","title":"Step 15 -- Metrics-to-Power Correlation","text":"<p>Correlates code metrics (complexity, NLOC, churn) with energy consumption:</p> <pre><code>correlator = MetricsPowerCorrelator(significance_level=0.05)\ncorrelator.fit(metric_names, metrics_values, power_measurements)\nsummary = correlator.summary()\n</code></pre> <p>Library feature: <code>greenmining.analyzers.MetricsPowerCorrelator</code></p> <p>Computes Pearson and Spearman coefficients with significance testing, plus feature importance ranking across all metrics.</p> <pre><code>Insufficient data (0 points, need &gt;= 3)\nEnable energy_tracking=True to collect per-commit energy data.\n</code></pre>"},{"location":"examples/experiment/#step-16-version-power-analysis","title":"Step 16 -- Version Power Analysis","text":"<p>Compares energy consumption across software versions:</p> <pre><code>version_analyzer = VersionPowerAnalyzer(\n    test_command=\"python -c 'sum(range(100000))'\",\n    energy_backend=\"cpu_meter\",\n    iterations=5,\n)\nreport = version_analyzer.analyze_versions(\n    repo_path=\"./my-repo\",\n    versions=[\"v1.0\", \"v2.0\", \"v3.0\"],\n)\n</code></pre> <p>Library feature: <code>greenmining.analyzers.VersionPowerAnalyzer</code></p> <p>Reports per-version power consumption, overall trend, most/least efficient versions, and total change percentage.</p> <pre><code>VersionPowerAnalyzer configured:\n  Backend: cpu_meter, Iterations: 5, Warmup: 1\n</code></pre>"},{"location":"examples/experiment/#step-17-visualization-matplotlib","title":"Step 17 -- Visualization (matplotlib)","text":"<p>Static 2x2 chart grid:</p> <ol> <li>Green commit rate per repository (horizontal bar)</li> <li>Top 10 GSF patterns (horizontal bar)</li> <li>Total vs green commit breakdown (grouped bar)</li> <li>Complexity distribution (histogram)</li> </ol> <p>Saved to <code>data/analysis_plots.png</code>.</p>"},{"location":"examples/experiment/#step-18-interactive-visualization-plotly","title":"Step 18 -- Interactive Visualization (Plotly)","text":"<p>Interactive charts:</p> <ul> <li>Sunburst: Repository &gt; Green/Non-Green &gt; Pattern</li> <li>Scatter: Complexity vs Lines of Code, colored by green awareness</li> </ul>"},{"location":"examples/experiment/#step-19-export-results","title":"Step 19 -- Export Results","text":"<p>Exports the combined dataset in three formats:</p> Format File Content JSON <code>data/analysis_results.json</code> Full analysis data (all fields) CSV <code>data/analysis_results.csv</code> Flattened commit-level rows DataFrame In-memory pandas DataFrame for further analysis <pre><code>Exported data/analysis_results.json\nExported 187 commits to data/analysis_results.csv\nDataFrame shape: (187, 20)\n</code></pre>"},{"location":"examples/experiment/#part-d-summary","title":"Part D: Summary","text":""},{"location":"examples/experiment/#feature-coverage","title":"Feature Coverage","text":"<p>The experiment applies every library feature to every repository equally:</p> Feature Module Applied GSF Pattern Detection (124 patterns, 15 categories) <code>greenmining</code> Yes Process Metrics (DMM size, complexity, interfacing) <code>greenmining</code> Yes Method-Level Analysis (per-function complexity) <code>greenmining</code> Yes Source Code Capture (before/after) <code>greenmining</code> Yes Energy Measurement (RAPL + CPU Meter) <code>greenmining.energy</code> Yes Memory Profiling (tracemalloc) <code>tracemalloc</code> Yes CO2 Emissions (CodeCarbon) <code>codecarbon</code> Yes Statistical Analysis (correlations, effect sizes) <code>greenmining.analyzers</code> Yes Temporal Analysis (quarterly trends) <code>greenmining.analyzers</code> Yes Code Diff Pattern Signatures <code>greenmining.analyzers</code> Yes Power Regression Detection <code>greenmining.analyzers</code> Demonstrated Metrics-to-Power Correlation (Pearson/Spearman) <code>greenmining.analyzers</code> Yes Version Power Comparison <code>greenmining.analyzers</code> Demonstrated Visualization (matplotlib + Plotly) External Yes Export (JSON, CSV, DataFrame) Built-in Yes"},{"location":"examples/experiment/#output-files","title":"Output Files","text":"File Description <code>data/analysis_results.json</code> Full analysis data <code>data/analysis_results.csv</code> Flattened commit-level data <code>data/analysis_plots.png</code> Static visualizations"},{"location":"examples/experiment/#running-the-experiment","title":"Running the Experiment","text":"<pre><code># Install the library\npip install greenmining[energy]\n\n# Set your GitHub token\nexport GITHUB_TOKEN=ghp_your_token_here\n\n# Open the notebook\njupyter notebook experiment/beta/experiment.ipynb\n</code></pre> <p>Run all cells sequentially. The data gathering steps require a valid GitHub token and internet access. Analysis steps operate on the collected data.</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>GreenMining uses direct function parameters -- no config files, no env vars, no intermediary objects.</p>"},{"location":"getting-started/configuration/#github-token","title":"GitHub Token","text":"<p>The only external requirement is a GitHub personal access token for API access:</p> <pre><code>export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre> <p>Or pass it directly in your code:</p> <pre><code>from greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=\"ghp_xxxxxxxxxxxxxxxxxxxx\",\n    max_repos=50,\n    keywords=\"kubernetes\",\n)\n</code></pre>"},{"location":"getting-started/configuration/#function-parameters","title":"Function Parameters","text":""},{"location":"getting-started/configuration/#fetch_repositories","title":"<code>fetch_repositories()</code>","text":"Parameter Type Default Description <code>github_token</code> str (required) GitHub personal access token <code>max_repos</code> int 100 Maximum repositories to fetch <code>min_stars</code> int 100 Minimum GitHub stars required <code>keywords</code> str None Search keywords for GitHub API <code>languages</code> list[str] None Programming language filters <code>created_after</code> str None Repos created after date (YYYY-MM-DD) <code>created_before</code> str None Repos created before date (YYYY-MM-DD) <code>pushed_after</code> str None Repos pushed after date (YYYY-MM-DD) <code>pushed_before</code> str None Repos pushed before date (YYYY-MM-DD) <code>output_dir</code> str \"./data\" Output directory for metadata"},{"location":"getting-started/configuration/#clone_repositories","title":"<code>clone_repositories()</code>","text":"Parameter Type Default Description <code>repositories</code> list (required) Repository objects from <code>fetch_repositories()</code> <code>github_token</code> str None GitHub token (optional) <code>output_dir</code> str \"./data\" Output directory for metadata <code>cleanup_existing</code> bool False Remove existing clones before re-cloning <p>Repositories are cloned into <code>./greenmining_repos/</code> with sanitized directory names.</p>"},{"location":"getting-started/configuration/#analyze_repositories","title":"<code>analyze_repositories()</code>","text":"Parameter Type Default Description <code>urls</code> list (required) List of GitHub repository URLs <code>max_commits</code> int 500 Maximum commits per repository <code>parallel_workers</code> int 1 Parallel analysis workers <code>output_format</code> str \"dict\" Output format (dict, json, csv) <code>energy_tracking</code> bool False Enable energy measurement <code>energy_backend</code> str \"rapl\" Energy backend (rapl, codecarbon, cpu_meter, auto) <code>method_level_analysis</code> bool False Per-method complexity metrics <code>include_source_code</code> bool False Include source code before/after <code>ssh_key_path</code> str None SSH key for private repos <code>github_token</code> str None Token for private HTTPS repos <code>since_date</code> str None Analyze commits from date (YYYY-MM-DD) <code>to_date</code> str None Analyze commits up to date (YYYY-MM-DD)"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Get started with examples</li> <li>Python API - Full programmatic usage</li> <li>GSF Patterns - All 124 patterns</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers all methods to install GreenMining.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.9 or higher</li> <li>Operating System: Linux, macOS, or Windows</li> <li>GitHub Token: Required for repository fetching (optional for URL analysis)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-pip-recommended","title":"Method 1: pip (Recommended)","text":"<p>Install from PyPI:</p> <pre><code>pip install greenmining\n</code></pre> <p>Verify installation:</p> <pre><code>python -c \"import greenmining; print(f'greenmining v{greenmining.__version__}')\"\n# Output: greenmining v1.0.4\n</code></pre>"},{"location":"getting-started/installation/#method-2-from-source","title":"Method 2: From Source","text":"<p>Clone and install for development:</p> <pre><code># Clone repository\ngit clone https://github.com/adam-bouafia/greenmining.git\ncd greenmining\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#method-3-docker","title":"Method 3: Docker","text":"<p>Run using Docker:</p> <pre><code># Pull image\ndocker pull adambouafia/greenmining:latest\n\n# Interactive Python shell\ndocker run -it -v $(pwd)/data:/app/data \\\n           adambouafia/greenmining:latest python\n\n# Run a Python script\ndocker run -v $(pwd)/data:/app/data \\\n           -e GITHUB_TOKEN=your_token \\\n           adambouafia/greenmining:latest python your_script.py\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>GreenMining automatically installs these dependencies:</p> Package Purpose <code>PyGithub&gt;=2.1.1</code> GitHub API access <code>gitpython&gt;=3.1.0</code> Git repository access and commit extraction <code>lizard&gt;=1.17.0</code> Method-level code complexity analysis <code>pandas&gt;=2.2.0</code> Data manipulation <code>scipy&gt;=1.10.0</code> Statistical analysis <code>numpy&gt;=1.24.0</code> Numerical operations <code>python-dotenv</code> Environment variable loading <code>tqdm</code> Progress bars"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For energy measurement features:</p> <pre><code># CodeCarbon support\npip install codecarbon\n\n# Full development environment\npip install greenmining[dev]\n</code></pre>"},{"location":"getting-started/installation/#github-token-setup","title":"GitHub Token Setup","text":"<p>A GitHub personal access token is required for the <code>fetch</code> and <code>pipeline</code> commands.</p>"},{"location":"getting-started/installation/#creating-a-token","title":"Creating a Token","text":"<ol> <li>Go to GitHub Settings \u2192 Developer settings \u2192 Personal access tokens</li> <li>Click \"Generate new token (classic)\"</li> <li>Select scopes:<ul> <li><code>repo</code> (for private repositories)</li> <li><code>public_repo</code> (for public repositories only)</li> </ul> </li> <li>Copy the generated token</li> </ol>"},{"location":"getting-started/installation/#configuring-the-token","title":"Configuring the Token","text":"<p>Option 1: Environment Variable</p> <pre><code>export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre> <p>Option 2: .env File</p> <p>Create a <code>.env</code> file in your project directory:</p> <pre><code>GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre> <p>Option 3: Pass to Functions</p> <p>Pass directly to API functions:</p> <pre><code>from greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=\"ghp_xxxxxxxxxxxxxxxxxxxx\",\n    max_repos=10\n)\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>Run the following to verify everything works:</p> <pre><code># Check version\nimport greenmining\nprint(f\"greenmining v{greenmining.__version__}\")\n\n# Check available exports\nfrom greenmining import GSF_PATTERNS, GREEN_KEYWORDS, is_green_aware\nprint(f\"{len(GSF_PATTERNS)} patterns loaded\")\nprint(f\"{len(GREEN_KEYWORDS)} keywords loaded\")\n\n# Test pattern detection\nprint(is_green_aware(\"Enable caching\"))  # True\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>Issue: <code>ModuleNotFoundError: No module named 'greenmining'</code></p> <p>Solution: Ensure you've activated your virtual environment and installed the package:</p> <pre><code>source venv/bin/activate\npip install greenmining\n</code></pre> <p>Issue: <code>GITHUB_TOKEN not set</code></p> <p>Solution: Set the environment variable or create a <code>.env</code> file:</p> <pre><code>export GITHUB_TOKEN=your_token_here\n</code></pre> <p>Issue: <code>Rate limit exceeded</code></p> <p>Solution: GitHub API has rate limits. Either:</p> <ul> <li>Wait for the rate limit to reset (1 hour)</li> <li>Use an authenticated token for higher limits</li> <li>Reduce <code>--max-repos</code> parameter</li> </ul> <p>Issue: <code>Permission denied</code> on RAPL energy measurement</p> <p>Solution: RAPL requires root access or specific permissions:</p> <pre><code># Grant read access to energy files\nsudo chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\n</code></pre> <p>Or use CodeCarbon instead (no root needed):</p> <pre><code>from greenmining.energy import CodeCarbonMeter\nmeter = CodeCarbonMeter()\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Run your first analysis</li> <li>Configuration - Customize GreenMining settings</li> <li>Python API - Complete API reference</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with GreenMining in 5 minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>GitHub token (for fetching repositories)</li> <li>GreenMining installed (<code>pip install greenmining</code>)</li> </ul>"},{"location":"getting-started/quickstart/#option-1-analyze-a-repository-by-url","title":"Option 1: Analyze a Repository by URL","text":"<p>Analyze any public GitHub repository directly:</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\n    \"https://github.com/pallets/flask\",\n    max_commits=100\n)\n\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Total commits: {result['total_commits']}\")\nprint(f\"Green-aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\nprint(\"\\nTop Patterns:\")\nfor pattern, count in sorted(\n    result['pattern_distribution'].items(),\n    key=lambda x: x[1],\n    reverse=True\n)[:5]:\n    print(f\"  - {pattern}: {count}\")\n</code></pre> <p>Output:</p> <pre><code>Repository: flask\nTotal commits: 100\nGreen-aware: 23 (23.0%)\n\nTop Patterns:\n  - Cache Static Data: 8\n  - Use Async Instead of Sync: 5\n  - Lazy Loading: 4\n</code></pre>"},{"location":"getting-started/quickstart/#option-2-full-pipeline-comprehensive","title":"Option 2: Full Pipeline (Comprehensive)","text":"<p>Run the complete analysis pipeline programmatically:</p> <pre><code>import os\nfrom greenmining.services import (\n    GitHubFetcher,\n    CommitExtractor,\n    DataAnalyzer,\n    DataAggregator,\n)\n\n# Set your GitHub token\ngithub_token = os.environ.get(\"GITHUB_TOKEN\")\n\n# 1. Fetch repositories\nfetcher = GitHubFetcher(token=github_token, max_repos=5, min_stars=500)\nrepos = fetcher.search_repositories()\n\n# 2. Extract commits\nextractor = CommitExtractor(max_commits=50)\ncommits = extractor.extract_from_repositories(repos)\n\n# 3. Analyze commits\nanalyzer = DataAnalyzer()\nresults = analyzer.analyze_commits(commits)\n\n# 4. Aggregate statistics\naggregator = DataAggregator(enable_stats=True, enable_temporal=True)\nstats = aggregator.aggregate(results, repos)\n\n# Print results\nprint(f\"Repositories analyzed: {len(repos)}\")\nprint(f\"Total commits: {len(commits)}\")\nprint(f\"Green-aware: {stats['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"getting-started/quickstart/#option-3-quick-pattern-detection","title":"Option 3: Quick Pattern Detection","text":"<p>Use GreenMining programmatically:</p> <pre><code>from greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\n\n# Check pattern count\nprint(f\"Loaded {len(GSF_PATTERNS)} GSF patterns\")\n# Output: Loaded 124 GSF patterns\n\n# Test green awareness detection\nmessages = [\n    \"Optimize Redis caching for better performance\",\n    \"Fix typo in README\",\n    \"Enable gzip compression for API responses\",\n    \"Update dependencies to latest versions\",\n]\n\nfor msg in messages:\n    is_green = is_green_aware(msg)\n    patterns = get_pattern_by_keywords(msg) if is_green else []\n    status = \"\ud83c\udf31\" if is_green else \"  \"\n    print(f\"{status} {msg[:50]}\")\n    if patterns:\n        print(f\"   \u2192 Patterns: {patterns}\")\n</code></pre> <p>Output:</p> <pre><code>\ud83c\udf31 Optimize Redis caching for better performance\n   \u2192 Patterns: ['Cache Static Data']\n   Fix typo in README\n\ud83c\udf31 Enable gzip compression for API responses\n   \u2192 Patterns: ['Compress Transmitted Data', 'Enable Text Compression']\n   Update dependencies to latest versions\n</code></pre>"},{"location":"getting-started/quickstart/#output-files","title":"Output Files","text":"<p>When running the pipeline, outputs are saved to the <code>data/</code> directory:</p> File Description <code>repositories.json</code> Fetched repository metadata <code>commits.json</code> Extracted commit data <code>analysis_results.json</code> Pattern detection results <code>aggregated_statistics.json</code> Summary statistics <code>aggregated_data.json</code> Full aggregated data"},{"location":"getting-started/quickstart/#quick-test-script","title":"Quick Test Script","text":"<p>Create <code>test_greenmining.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Quick test of GreenMining functionality.\"\"\"\n\nfrom greenmining import GSF_PATTERNS, GREEN_KEYWORDS, is_green_aware, get_pattern_by_keywords\n\n# Test 1: Check patterns loaded\nprint(f\"\u2713 Loaded {len(GSF_PATTERNS)} GSF patterns\")\nprint(f\"\u2713 Loaded {len(GREEN_KEYWORDS)} green keywords\")\n\n# Test 2: Get categories\ncategories = set(p[\"category\"] for p in GSF_PATTERNS.values())\nprint(f\"\u2713 Categories: {', '.join(sorted(categories))}\")\n\n# Test 3: Pattern detection\ntest_msg = \"Implement Redis caching to reduce database load\"\nis_green = is_green_aware(test_msg)\npatterns = get_pattern_by_keywords(test_msg)\n\nprint(f\"\u2713 Test message: '{test_msg}'\")\nprint(f\"  Green-aware: {is_green}\")\nprint(f\"  Patterns: {patterns}\")\n\nprint(\"\\n\u2705 All tests passed!\")\n</code></pre> <p>Run it:</p> <pre><code>python test_greenmining.py\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Customize settings</li> <li>Python API - Programmatic usage</li> <li>GSF Patterns - All 124 patterns</li> <li>URL Analysis - Analyze by URL</li> </ul>"},{"location":"reference/graphql/","title":"GraphQL API Reference","text":"<p>GreenMining uses GitHub's GraphQL API v4 for repository search and metadata retrieval.</p>"},{"location":"reference/graphql/#overview","title":"Overview","text":"<p>The <code>GitHubGraphQLFetcher</code> class provides efficient repository discovery using a single GraphQL query per page of results, compared to multiple REST API calls.</p> <pre><code>from greenmining.services.github_graphql_fetcher import GitHubGraphQLFetcher\n\nfetcher = GitHubGraphQLFetcher(token=\"ghp_your_token\")\n</code></pre> <p>Or use the top-level wrapper:</p> <pre><code>from greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=\"ghp_your_token\",\n    max_repos=10,\n    keywords=\"blockchain\",\n)\n</code></pre>"},{"location":"reference/graphql/#githubgraphqlfetcher","title":"GitHubGraphQLFetcher","text":""},{"location":"reference/graphql/#constructor","title":"Constructor","text":"<pre><code>GitHubGraphQLFetcher(token: str)\n</code></pre> Parameter Type Description <code>token</code> str GitHub personal access token with <code>repo</code> scope"},{"location":"reference/graphql/#search_repositories","title":"search_repositories()","text":"<p>Search for repositories matching criteria.</p> <pre><code>def search_repositories(\n    keywords: str = \"microservices\",\n    max_repos: int = 100,\n    min_stars: int = 100,\n    languages: list[str] | None = None,\n    created_after: str | None = None,\n    created_before: str | None = None,\n    pushed_after: str | None = None,\n    pushed_before: str | None = None,\n) -&gt; list[Repository]\n</code></pre> Parameter Type Default Description <code>keywords</code> str <code>\"microservices\"</code> Search keywords <code>max_repos</code> int <code>100</code> Maximum repositories to return <code>min_stars</code> int <code>100</code> Minimum GitHub stars <code>languages</code> list <code>None</code> Filter by programming languages <code>created_after</code> str <code>None</code> Created after date (YYYY-MM-DD) <code>created_before</code> str <code>None</code> Created before date (YYYY-MM-DD) <code>pushed_after</code> str <code>None</code> Last pushed after date (YYYY-MM-DD) <code>pushed_before</code> str <code>None</code> Last pushed before date (YYYY-MM-DD) <p>Returns: List of <code>Repository</code> objects.</p> <p>Example:</p> <pre><code>fetcher = GitHubGraphQLFetcher(token)\n\nrepos = fetcher.search_repositories(\n    keywords=\"blockchain\",\n    max_repos=10,\n    min_stars=3,\n    languages=[\"Python\", \"Go\", \"Rust\"],\n    created_after=\"2020-01-01\",\n)\n\nfor repo in repos:\n    print(f\"{repo.full_name} ({repo.stars} stars, {repo.language})\")\n</code></pre>"},{"location":"reference/graphql/#get_repository_commits","title":"get_repository_commits()","text":"<p>Fetch commits for a specific repository.</p> <pre><code>def get_repository_commits(\n    owner: str,\n    name: str,\n    max_commits: int = 100,\n) -&gt; list[dict]\n</code></pre> Parameter Type Default Description <code>owner</code> str (required) Repository owner <code>name</code> str (required) Repository name <code>max_commits</code> int <code>100</code> Maximum commits to fetch <p>Returns: List of commit dictionaries with keys:</p> Key Type Description <code>sha</code> str Commit SHA (oid) <code>message</code> str Commit message <code>date</code> str Committed date (ISO 8601) <code>author</code> str Author name <code>author_email</code> str Author email <code>additions</code> int Lines added <code>deletions</code> int Lines deleted <code>changed_files</code> int Number of files changed <p>Example:</p> <pre><code>commits = fetcher.get_repository_commits(\"pallets\", \"flask\", max_commits=50)\nfor c in commits:\n    print(f\"{c['sha'][:8]} | {c['author']} | {c['message'][:60]}\")\n</code></pre>"},{"location":"reference/graphql/#save_results","title":"save_results()","text":"<p>Save repositories to a JSON file.</p> <pre><code>def save_results(repositories: list[Repository], output_file: str) -&gt; None\n</code></pre>"},{"location":"reference/graphql/#search-query-building","title":"Search Query Building","text":"<p>The <code>_build_search_query()</code> method constructs a GitHub search query string from the parameters. Understanding how queries are built helps debug search results.</p>"},{"location":"reference/graphql/#query-format","title":"Query Format","text":"<pre><code>{keywords} stars:&gt;={min_stars} [language:{lang}]... [created:&gt;={date}] [pushed:&gt;={date}]\n</code></pre>"},{"location":"reference/graphql/#language-filter-limit","title":"Language Filter Limit","text":"<p>GitHub's search API has a complexity limit on queries. When more than 5 languages are specified, the language filter is skipped entirely to avoid empty results:</p> <pre><code># 3 languages -&gt; filter applied\n# Query: \"blockchain stars:&gt;=3 language:Python language:Go language:Rust\"\nrepos = fetch_repositories(languages=[\"Python\", \"Go\", \"Rust\"], ...)\n\n# 20 languages -&gt; filter skipped (returns repos in any language)\n# Query: \"blockchain stars:&gt;=3\"\nrepos = fetch_repositories(languages=LANGUAGES_20, ...)\n</code></pre> <p>This is by design. When using many languages, the search returns results in any language, which is more useful than returning zero results from an overly complex query.</p>"},{"location":"reference/graphql/#date-filters","title":"Date Filters","text":"<p>All date filters use the <code>YYYY-MM-DD</code> format:</p> <pre><code># Repos created in 2023\nrepos = fetcher.search_repositories(\n    created_after=\"2023-01-01\",\n    created_before=\"2023-12-31\",\n)\n\n# Repos with recent activity\nrepos = fetcher.search_repositories(\n    pushed_after=\"2024-01-01\",\n)\n</code></pre>"},{"location":"reference/graphql/#rate-limiting","title":"Rate Limiting","text":"<p>The GraphQL API has a point-based rate limit (5,000 points/hour for authenticated users). Each search query costs 1 point.</p> <p>GreenMining handles rate limits automatically:</p> <ul> <li>Prints remaining quota after each query</li> <li>Sleeps for 60 seconds when remaining points drop below 100</li> <li>Supports pagination with cursor-based <code>after</code> parameter</li> </ul> <pre><code>Rate Limit: 4998/5000 (cost: 1)\n</code></pre>"},{"location":"reference/graphql/#checking-your-rate-limit","title":"Checking Your Rate Limit","text":"<pre><code>fetcher = GitHubGraphQLFetcher(token)\n# Rate limit info is printed with every query\nrepos = fetcher.search_repositories(max_repos=1)\n</code></pre>"},{"location":"reference/graphql/#pagination","title":"Pagination","text":"<p>Results are paginated using GitHub's cursor-based pagination. The fetcher handles this automatically:</p> <ul> <li>Fetches up to 100 results per page (GitHub's maximum)</li> <li>Follows <code>pageInfo.hasNextPage</code> and <code>endCursor</code> cursors</li> <li>Stops when <code>max_repos</code> is reached or no more results exist</li> </ul> <p>For large result sets:</p> <pre><code># Fetches 3 pages of 100 results each\nrepos = fetcher.search_repositories(max_repos=300)\n</code></pre>"},{"location":"reference/graphql/#repository-model","title":"Repository Model","text":"<p>Each search result is parsed into a <code>Repository</code> dataclass:</p> Field Type Source (GraphQL) <code>repo_id</code> int Sequential index <code>name</code> str <code>name</code> <code>owner</code> str Parsed from <code>nameWithOwner</code> <code>full_name</code> str <code>nameWithOwner</code> <code>url</code> str <code>url</code> <code>clone_url</code> str <code>url</code> + <code>.git</code> <code>language</code> str <code>primaryLanguage.name</code> <code>stars</code> int <code>stargazerCount</code> <code>forks</code> int <code>forkCount</code> <code>watchers</code> int <code>watchers.totalCount</code> <code>open_issues</code> int 0 (not queried) <code>last_updated</code> str <code>updatedAt</code> <code>created_at</code> str <code>createdAt</code> <code>description</code> str <code>description</code> <code>main_branch</code> str <code>defaultBranchRef.name</code> <code>archived</code> bool <code>isArchived</code> <code>license</code> str <code>licenseInfo.name</code>"},{"location":"reference/graphql/#graphql-query-schema","title":"GraphQL Query Schema","text":"<p>The search query fetches these fields per repository:</p> <pre><code>query($searchQuery: String!, $first: Int!) {\n  search(query: $searchQuery, type: REPOSITORY, first: $first) {\n    repositoryCount\n    pageInfo { hasNextPage, endCursor }\n    nodes {\n      ... on Repository {\n        id, name, nameWithOwner, description, url\n        createdAt, updatedAt, pushedAt\n        stargazerCount, forkCount\n        watchers { totalCount }\n        primaryLanguage { name }\n        languages(first: 5) { nodes { name } }\n        licenseInfo { name }\n        isArchived, isFork\n        defaultBranchRef { name }\n      }\n    }\n  }\n  rateLimit { limit, cost, remaining, resetAt }\n}\n</code></pre>"},{"location":"reference/graphql/#next-steps","title":"Next Steps","text":"<ul> <li>Python API - <code>fetch_repositories</code> wrapper</li> <li>Data Models - Repository dataclass fields</li> <li>Experiment - Full pipeline example</li> </ul>"},{"location":"reference/library-reference/","title":"GreenMining Library Reference","text":""},{"location":"reference/library-reference/#description","title":"Description","text":"<p>GreenMining is an empirical Python library for Mining Software Repositories (MSR) in Green IT research. It analyzes GitHub repositories to detect green software engineering practices by matching commit messages and code changes against the Green Software Foundation (GSF) pattern catalog. The library supports energy measurement during analysis using Intel RAPL, CodeCarbon, or CPU utilization-based estimation, and provides statistical, temporal, and qualitative analysis capabilities.</p> <p>Version: 1.1.9 License: MIT PyPI: greenmining Documentation: greenmining.readthedocs.io</p>"},{"location":"reference/library-reference/#file-tree","title":"File Tree","text":"<pre><code>greenmining/\n    __init__.py\n    __main__.py\n    __version__.py\n    config.py\n    gsf_patterns.py\n    utils.py\n    models/\n        __init__.py\n        repository.py\n        commit.py\n        analysis_result.py\n        aggregated_stats.py\n    services/\n        __init__.py\n        github_fetcher.py          (deprecated)\n        github_graphql_fetcher.py\n        commit_extractor.py\n        data_analyzer.py\n        data_aggregator.py\n        local_repo_analyzer.py\n        reports.py\n    analyzers/\n        __init__.py\n        statistical_analyzer.py\n        temporal_analyzer.py\n        qualitative_analyzer.py\n        code_diff_analyzer.py\n        metrics_power_correlator.py\n        power_regression.py\n        version_power_analyzer.py\n    energy/\n        __init__.py\n        base.py\n        rapl.py\n        cpu_meter.py\n        codecarbon_meter.py\n        carbon_reporter.py\n    controllers/\n        __init__.py\n        repository_controller.py\n    presenters/\n        __init__.py\n        console_presenter.py\n</code></pre>"},{"location":"reference/library-reference/#module-reference","title":"Module Reference","text":""},{"location":"reference/library-reference/#greenmining__init__py","title":"<code>greenmining/__init__.py</code>","text":"<p>Top-level package entry point. Exposes the two main high-level API functions and the GSF pattern utilities.</p> Function Parameters Description <code>fetch_repositories()</code> <code>github_token</code>, <code>max_repos</code>, <code>min_stars</code>, <code>languages</code>, <code>keywords</code>, <code>created_after</code>, <code>created_before</code>, <code>pushed_after</code>, <code>pushed_before</code> Search GitHub for repositories using GraphQL API v4. Returns a list of <code>Repository</code> objects matching the given filters. <code>analyze_repositories()</code> <code>urls</code>, <code>max_commits</code>, <code>parallel_workers</code>, <code>output_format</code>, <code>energy_tracking</code>, <code>energy_backend</code>, <code>method_level_analysis</code>, <code>include_source_code</code>, <code>ssh_key_path</code>, <code>github_token</code>, <code>since_date</code>, <code>to_date</code> Clone and analyze multiple repositories from URLs using PyDriller. Supports energy measurement, method-level Lizard metrics, and source code extraction. Returns a list of <code>RepositoryAnalysis</code> objects. <p>Exports: <code>Config</code>, <code>GSF_PATTERNS</code>, <code>GREEN_KEYWORDS</code>, <code>is_green_aware</code>, <code>get_pattern_by_keywords</code>, <code>fetch_repositories</code>, <code>analyze_repositories</code>, <code>__version__</code></p>"},{"location":"reference/library-reference/#greenmining__main__py","title":"<code>greenmining/__main__.py</code>","text":"<p>Allows running as <code>python -m greenmining</code>. Prints version and usage information.</p>"},{"location":"reference/library-reference/#greenminingconfigpy","title":"<code>greenmining/config.py</code>","text":"<p>Configuration management supporting <code>.env</code> files, environment variables, and YAML configuration (<code>greenmining.yaml</code>).</p>"},{"location":"reference/library-reference/#_load_yaml_configyaml_path","title":"<code>_load_yaml_config(yaml_path)</code>","text":"<p>Loads YAML configuration from file. Returns empty dict if the file does not exist or PyYAML is not installed.</p>"},{"location":"reference/library-reference/#class-config","title":"class <code>Config</code>","text":"Attribute Default Source Description <code>GITHUB_TOKEN</code> required env GitHub personal access token <code>GITHUB_SEARCH_KEYWORDS</code> <code>[\"microservices\", ...]</code> YAML/env Search keywords for repository discovery <code>SUPPORTED_LANGUAGES</code> <code>[\"Java\", \"Python\", \"Go\", ...]</code> YAML/env Languages to filter <code>MAX_REPOS</code> <code>100</code> env Maximum repositories to fetch <code>COMMITS_PER_REPO</code> <code>50</code> YAML/env Maximum commits per repository <code>DAYS_BACK</code> <code>730</code> YAML/env Analysis time window in days <code>SKIP_MERGES</code> <code>True</code> YAML Skip merge commits <code>MIN_STARS</code> <code>100</code> YAML/env Minimum stars filter <code>ENERGY_ENABLED</code> <code>False</code> YAML/env Enable energy measurement <code>ENERGY_BACKEND</code> <code>\"rapl\"</code> YAML/env Energy backend selection <code>CARBON_TRACKING</code> <code>False</code> YAML/env Enable CO2 tracking <code>COUNTRY_ISO</code> <code>\"USA\"</code> YAML/env Country for carbon intensity <code>OUTPUT_DIR</code> <code>./data</code> YAML/env Output directory for results Method Description <code>__init__(env_file, yaml_file)</code> Load configuration from environment and YAML. YAML values take precedence for supported options. <code>validate()</code> Validate that all required configuration attributes are present. <code>_parse_repository_urls(urls_str)</code> Parse comma-separated repository URLs from environment variable."},{"location":"reference/library-reference/#get_configenv_file","title":"<code>get_config(env_file)</code>","text":"<p>Singleton factory that returns or creates a global <code>Config</code> instance.</p>"},{"location":"reference/library-reference/#greenmininggsf_patternspy","title":"<code>greenmining/gsf_patterns.py</code>","text":"<p>Contains the Green Software Foundation pattern catalog and keyword matching logic.</p> <p><code>GSF_PATTERNS</code> -- Dictionary of 124 green software patterns across 15 categories:</p> Category Count Examples cloud 35+ Cache Static Data, Autoscaling, Serverless, Right-size Resources web 15+ Lazy Loading, Minimize Data Transfer, Optimize Images ai 15+ Model Quantization, Knowledge Distillation, Early Stopping database 5+ Database Indexing, Query Optimization, Prepared Statements networking/network 8+ Connection Pooling, gRPC Optimization, API Gateway general 8+ Async Processing, Batch Processing, Memoization resource 2 Resource Limits, Dynamic Resource Allocation caching 2 Multi-Level Caching, Cache Invalidation Strategy data 3 Data Deduplication, Efficient Serialization, Pagination async 3 Event-Driven Architecture, Eliminate Polling, Reactive Streams code 4 Algorithm Optimization, Code Efficiency, Garbage Collection Tuning monitoring 3 Energy-Aware Monitoring, Performance Profiling, APM microservices 4 Service Decomposition, Service Co-location, Graceful Shutdown infrastructure 4 Minimal Container Images, Renewable Energy Regions, IaC <p>Each pattern has: <code>name</code>, <code>category</code>, <code>keywords</code> (list), <code>description</code>, <code>sci_impact</code>.</p> <p><code>GREEN_KEYWORDS</code> -- List of 332 keywords used for green awareness detection (e.g., \"energy\", \"cache\", \"optimize\", \"serverless\", \"quantization\").</p> Function Parameters Description <code>get_pattern_by_keywords(commit_message)</code> <code>commit_message: str</code> Match a commit message against all GSF patterns. Returns list of matched pattern names. <code>is_green_aware(commit_message)</code> <code>commit_message: str</code> Check if a commit message contains any green software keyword. Returns boolean."},{"location":"reference/library-reference/#greenminingutilspy","title":"<code>greenmining/utils.py</code>","text":"<p>Utility functions for file I/O, formatting, retry logic, and console output.</p> Function Parameters Description <code>format_timestamp(dt)</code> <code>dt: Optional[datetime]</code> Format datetime as ISO 8601 string. Defaults to <code>utcnow()</code>. <code>load_json_file(path)</code> <code>path: Path</code> Load and parse a JSON file. <code>save_json_file(data, path, indent)</code> <code>data: dict, path: Path, indent: int</code> Save data to JSON file, creating parent directories. <code>load_csv_file(path)</code> <code>path: Path</code> Load CSV file as pandas DataFrame. <code>save_csv_file(df, path)</code> <code>df: DataFrame, path: Path</code> Save DataFrame to CSV file. <code>estimate_tokens(text)</code> <code>text: str</code> Estimate token count (len/4). <code>estimate_cost(tokens, model)</code> <code>tokens: int, model: str</code> Estimate API cost based on Claude Sonnet 4 pricing. <code>retry_on_exception(max_retries, delay, exponential_backoff, exceptions)</code> decorator args Decorator that retries a function on exception with configurable backoff. <code>colored_print(text, color)</code> <code>text: str, color: str</code> Print colored text using colorama. Supported colors: red, green, yellow, blue, magenta, cyan, white. <code>handle_github_rate_limit(response)</code> <code>response</code> Check for HTTP 403 and raise exception on rate limit. <code>format_number(num)</code> <code>num: int</code> Format number with thousand separators. <code>format_percentage(value, decimals)</code> <code>value: float, decimals: int</code> Format float as percentage string. <code>format_duration(seconds)</code> <code>seconds: float</code> Format duration as human-readable string (e.g., \"2m 30s\"). <code>truncate_text(text, max_length)</code> <code>text: str, max_length: int</code> Truncate text with \"...\" suffix. <code>create_checkpoint(checkpoint_file, data)</code> <code>checkpoint_file: Path, data: dict</code> Save checkpoint JSON for resumable operations. <code>load_checkpoint(checkpoint_file)</code> <code>checkpoint_file: Path</code> Load checkpoint data if file exists. <code>print_banner(title)</code> <code>title: str</code> Print formatted banner with decorators. <code>print_section(title)</code> <code>title: str</code> Print section header with separator line."},{"location":"reference/library-reference/#models","title":"Models","text":""},{"location":"reference/library-reference/#greenminingmodelsrepositorypy","title":"<code>greenmining/models/repository.py</code>","text":""},{"location":"reference/library-reference/#class-repository-dataclass","title":"class <code>Repository</code> (dataclass)","text":"<p>Represents a GitHub repository with all metadata.</p> Field Type Description <code>repo_id</code> <code>int</code> Sequential identifier <code>name</code> <code>str</code> Repository name <code>owner</code> <code>str</code> Repository owner/organization <code>full_name</code> <code>str</code> <code>owner/name</code> format <code>url</code> <code>str</code> HTML URL <code>clone_url</code> <code>str</code> Git clone URL <code>language</code> <code>Optional[str]</code> Primary programming language <code>stars</code> <code>int</code> Star count <code>forks</code> <code>int</code> Fork count <code>watchers</code> <code>int</code> Watcher count <code>open_issues</code> <code>int</code> Open issue count <code>last_updated</code> <code>str</code> Last update ISO date <code>created_at</code> <code>str</code> Creation ISO date <code>description</code> <code>Optional[str]</code> Repository description <code>main_branch</code> <code>str</code> Default branch name <code>topics</code> <code>list[str]</code> Repository topics <code>size</code> <code>int</code> Repository size in KB <code>archived</code> <code>bool</code> Whether archived <code>license</code> <code>Optional[str]</code> License key Method Description <code>to_dict()</code> Convert to dictionary. <code>from_dict(data)</code> Class method: create from dictionary. <code>from_github_repo(repo, repo_id)</code> Class method: create from PyGithub repository object."},{"location":"reference/library-reference/#greenminingmodelscommitpy","title":"<code>greenmining/models/commit.py</code>","text":""},{"location":"reference/library-reference/#class-commit-dataclass","title":"class <code>Commit</code> (dataclass)","text":"<p>Represents a Git commit with metadata.</p> Field Type Description <code>commit_id</code> <code>str</code> Commit SHA hash <code>repo_name</code> <code>str</code> Repository full name <code>date</code> <code>str</code> Commit date ISO string <code>author</code> <code>str</code> Author name <code>author_email</code> <code>str</code> Author email <code>message</code> <code>str</code> Commit message <code>files_changed</code> <code>list[str]</code> Modified file paths <code>lines_added</code> <code>int</code> Lines added <code>lines_deleted</code> <code>int</code> Lines deleted <code>insertions</code> <code>int</code> Insertions count <code>deletions</code> <code>int</code> Deletions count <code>is_merge</code> <code>bool</code> Whether this is a merge commit <code>in_main_branch</code> <code>bool</code> Whether commit is in main branch Method Description <code>to_dict()</code> Convert to dictionary. <code>from_dict(data)</code> Class method: create from dictionary. <code>from_pydriller_commit(commit, repo_name)</code> Class method: create from PyDriller commit object."},{"location":"reference/library-reference/#greenminingmodelsanalysis_resultpy","title":"<code>greenmining/models/analysis_result.py</code>","text":""},{"location":"reference/library-reference/#class-analysisresult-dataclass","title":"class <code>AnalysisResult</code> (dataclass)","text":"<p>Represents the analysis result for a single commit.</p> Field Type Description <code>commit_id</code> <code>str</code> Commit SHA <code>repo_name</code> <code>str</code> Repository name <code>date</code> <code>str</code> Commit date <code>commit_message</code> <code>str</code> Full commit message <code>green_aware</code> <code>bool</code> Whether commit is green-aware <code>green_evidence</code> <code>Optional[str]</code> Evidence for green classification <code>known_pattern</code> <code>Optional[str]</code> Matched GSF pattern name <code>pattern_confidence</code> <code>Optional[str]</code> Confidence level (HIGH/MEDIUM/LOW) <code>emergent_pattern</code> <code>Optional[str]</code> Novel pattern description <code>files_changed</code> <code>list</code> Modified files <code>lines_added</code> <code>int</code> Lines added <code>lines_deleted</code> <code>int</code> Lines deleted"},{"location":"reference/library-reference/#greenminingmodelsaggregated_statspy","title":"<code>greenmining/models/aggregated_stats.py</code>","text":""},{"location":"reference/library-reference/#class-aggregatedstats-dataclass","title":"class <code>AggregatedStats</code> (dataclass)","text":"<p>Holds aggregated analysis statistics.</p> Field Type Description <code>summary</code> <code>dict</code> Overall summary statistics <code>known_patterns</code> <code>dict</code> Pattern frequency data <code>repositories</code> <code>list[dict]</code> Per-repository statistics <code>languages</code> <code>dict</code> Per-language statistics <code>timestamp</code> <code>Optional[str]</code> Aggregation timestamp"},{"location":"reference/library-reference/#services","title":"Services","text":""},{"location":"reference/library-reference/#greenminingservicesgithub_graphql_fetcherpy","title":"<code>greenmining/services/github_graphql_fetcher.py</code>","text":""},{"location":"reference/library-reference/#class-githubgraphqlfetcher","title":"class <code>GitHubGraphQLFetcher</code>","text":"<p>Fetches repositories and commits from GitHub using GraphQL API v4. Handles pagination and rate limiting.</p> Method Parameters Description <code>__init__(token)</code> <code>token: str</code> Initialize with GitHub personal access token. <code>search_repositories(keywords, max_repos, min_stars, languages, created_after, created_before, pushed_after, pushed_before)</code> see params Search for repositories matching criteria. Paginates automatically. Returns list of <code>Repository</code> objects. <code>get_repository_commits(owner, name, max_commits)</code> <code>owner: str, name: str, max_commits: int</code> Fetch commit history for a specific repository. Returns list of commit dictionaries. <code>save_results(repositories, output_file)</code> <code>repositories: List[Repository], output_file: str</code> Save repository list to JSON file. <code>_build_search_query(...)</code> internal Build GitHub search query string with filters (stars, languages, dates). <code>_execute_query(query, variables)</code> internal Execute a GraphQL query against GitHub API. <code>_parse_repository(node, repo_id)</code> internal Parse GraphQL response node into <code>Repository</code> object."},{"location":"reference/library-reference/#greenminingservicesgithub_fetcherpy","title":"<code>greenmining/services/github_fetcher.py</code>","text":"<p>Deprecated. Legacy REST API fetcher. Use <code>GitHubGraphQLFetcher</code> instead.</p>"},{"location":"reference/library-reference/#greenminingservicescommit_extractorpy","title":"<code>greenmining/services/commit_extractor.py</code>","text":""},{"location":"reference/library-reference/#class-commitextractor","title":"class <code>CommitExtractor</code>","text":"<p>Extracts commit data from repositories using the GitHub REST API (PyGithub).</p> Method Parameters Description <code>__init__(max_commits, skip_merges, days_back, github_token, timeout)</code> see params Initialize with extraction settings. Default: 50 commits, skip merges, 730 days back, 60s timeout. <code>extract_from_repositories(repositories)</code> <code>repositories: list</code> Extract commits from a list of repositories. Handles timeouts and errors per repository. Uses SIGALRM for timeout enforcement. <code>save_results(commits, output_file, repos_count)</code> <code>commits: list, output_file: Path, repos_count: int</code> Save extracted commits to JSON with metadata. <code>_extract_repo_commits(repo)</code> internal Extract commits from a single repository via GitHub API. Decorated with <code>@retry_on_exception</code>. <code>_extract_commit_metadata(commit, repo_name)</code> internal Extract metadata from a PyDriller commit object. <code>_extract_commit_metadata_from_github(commit, repo_name)</code> internal Extract metadata from a PyGithub commit object."},{"location":"reference/library-reference/#greenminingservicesdata_analyzerpy","title":"<code>greenmining/services/data_analyzer.py</code>","text":""},{"location":"reference/library-reference/#class-dataanalyzer","title":"class <code>DataAnalyzer</code>","text":"<p>Analyzes commits for green software patterns using GSF keywords and optional code diff analysis.</p> Method Parameters Description <code>__init__(batch_size, enable_diff_analysis)</code> <code>batch_size: int, enable_diff_analysis: bool</code> Initialize with GSF patterns. Optionally enables <code>CodeDiffAnalyzer</code> for deeper code inspection. <code>analyze_commits(commits, resume_from)</code> <code>commits: list, resume_from: int</code> Analyze a list of commit dictionaries. Returns analysis results with green awareness, matched patterns, confidence, and metadata. <code>save_results(results, output_file)</code> <code>results: list, output_file: Path</code> Save analysis results to JSON with summary statistics. <code>_analyze_commit(commit)</code> internal Analyze a single commit: check green awareness via <code>is_green_aware()</code>, match GSF patterns via <code>get_pattern_by_keywords()</code>, calculate confidence, and optionally run diff analysis. <code>_check_green_awareness(message, files)</code> internal Check commit message and file names for green keywords. <code>_detect_known_pattern(message, files)</code> internal Detect known green software pattern from message and file names."},{"location":"reference/library-reference/#greenminingservicesdata_aggregatorpy","title":"<code>greenmining/services/data_aggregator.py</code>","text":""},{"location":"reference/library-reference/#class-dataaggregator","title":"class <code>DataAggregator</code>","text":"<p>Aggregates analysis results and generates summary statistics. Optionally integrates statistical and temporal analysis.</p> Method Parameters Description <code>__init__(enable_stats, enable_temporal, temporal_granularity)</code> <code>enable_stats: bool, enable_temporal: bool, temporal_granularity: str</code> Initialize aggregator. When <code>enable_stats=True</code>, creates a <code>StatisticalAnalyzer</code>. When <code>enable_temporal=True</code>, creates a <code>TemporalAnalyzer</code>. <code>aggregate(analysis_results, repositories)</code> <code>analysis_results: list, repositories: list</code> Compute summary, pattern analysis, per-repo stats, per-language stats. Optionally adds statistical analysis (trends, correlations, effect sizes) and temporal trend analysis. <code>save_results(aggregated_data, json_file, csv_file, analysis_results)</code> see params Save aggregated data to JSON and detailed results to CSV. <code>print_summary(aggregated_data)</code> <code>aggregated_data: dict</code> Print formatted summary tables to console using tabulate. <code>_generate_summary(results, repos)</code> internal Calculate total commits, green count, percentage, repos with green commits. <code>_analyze_known_patterns(results)</code> internal Count and rank detected GSF patterns with confidence breakdown. <code>_analyze_emergent_patterns(results)</code> internal Collect novel patterns not in the GSF catalog. <code>_generate_repo_stats(results, repos)</code> internal Compute per-repository green commit statistics. <code>_generate_language_stats(results, repos)</code> internal Compute per-language green commit statistics. <code>_generate_statistics(results)</code> internal Run temporal trends (Mann-Kendall), pattern correlations, effect sizes (Cohen's d), and descriptive statistics."},{"location":"reference/library-reference/#greenminingserviceslocal_repo_analyzerpy","title":"<code>greenmining/services/local_repo_analyzer.py</code>","text":"<p>The core analysis engine. Clones repositories from URLs and analyzes commits using PyDriller.</p>"},{"location":"reference/library-reference/#class-methodmetrics-dataclass","title":"class <code>MethodMetrics</code> (dataclass)","text":"<p>Per-method analysis metrics extracted via Lizard integration through PyDriller.</p> Field Type Description <code>name</code> <code>str</code> Method name <code>long_name</code> <code>str</code> Fully qualified method name <code>filename</code> <code>str</code> Source file name <code>nloc</code> <code>int</code> Lines of code <code>complexity</code> <code>int</code> Cyclomatic complexity <code>token_count</code> <code>int</code> Token count <code>parameters</code> <code>int</code> Parameter count <code>start_line</code> <code>int</code> Start line number <code>end_line</code> <code>int</code> End line number"},{"location":"reference/library-reference/#class-sourcecodechange-dataclass","title":"class <code>SourceCodeChange</code> (dataclass)","text":"<p>Source code before/after a commit for refactoring detection.</p> Field Type Description <code>filename</code> <code>str</code> File name <code>source_code_before</code> <code>Optional[str]</code> Source code before the commit <code>source_code_after</code> <code>Optional[str]</code> Source code after the commit <code>diff</code> <code>Optional[str]</code> Unified diff <code>added_lines</code> <code>int</code> Lines added <code>deleted_lines</code> <code>int</code> Lines deleted <code>change_type</code> <code>str</code> ADD, DELETE, MODIFY, or RENAME"},{"location":"reference/library-reference/#class-commitanalysis-dataclass","title":"class <code>CommitAnalysis</code> (dataclass)","text":"<p>Full analysis result for a single commit, including GSF patterns, DMM metrics, structural metrics, method-level analysis, source code, and energy data.</p> Field Type Description <code>hash</code> <code>str</code> Commit SHA <code>message</code> <code>str</code> Commit message <code>author</code> / <code>author_email</code> <code>str</code> Author info <code>date</code> <code>datetime</code> Author date <code>green_aware</code> <code>bool</code> Green awareness flag <code>gsf_patterns_matched</code> <code>List[str]</code> Matched GSF pattern names <code>pattern_count</code> <code>int</code> Number of patterns matched <code>pattern_details</code> <code>List[Dict]</code> Full pattern info (name, category, description, sci_impact) <code>confidence</code> <code>str</code> high / medium / low <code>files_modified</code> <code>List[str]</code> Modified file names <code>insertions</code> / <code>deletions</code> <code>int</code> Line change counts <code>dmm_unit_size</code> <code>Optional[float]</code> Delta Maintainability Model: unit size <code>dmm_unit_complexity</code> <code>Optional[float]</code> DMM: unit complexity <code>dmm_unit_interfacing</code> <code>Optional[float]</code> DMM: unit interfacing <code>total_nloc</code> <code>int</code> Total lines of code across modified files <code>total_complexity</code> <code>int</code> Total cyclomatic complexity <code>max_complexity</code> <code>int</code> Maximum complexity of any modified file <code>methods_count</code> <code>int</code> Total methods across modified files <code>methods</code> <code>List[MethodMetrics]</code> Per-method metrics (when <code>method_level_analysis=True</code>) <code>source_changes</code> <code>List[SourceCodeChange]</code> Source code changes (when <code>include_source_code=True</code>) <code>energy_joules</code> <code>Optional[float]</code> Energy consumed (when <code>energy_tracking=True</code>) <code>energy_watts_avg</code> <code>Optional[float]</code> Average power draw"},{"location":"reference/library-reference/#class-repositoryanalysis-dataclass","title":"class <code>RepositoryAnalysis</code> (dataclass)","text":"<p>Complete analysis result for a repository.</p> Field Type Description <code>url</code> <code>str</code> Repository URL <code>name</code> <code>str</code> Repository full name <code>total_commits</code> <code>int</code> Total commits analyzed <code>green_commits</code> <code>int</code> Green-aware commit count <code>green_commit_rate</code> <code>float</code> Green commit percentage <code>commits</code> <code>List[CommitAnalysis]</code> Per-commit analysis results <code>process_metrics</code> <code>Dict</code> PyDriller process metrics <code>energy_metrics</code> <code>Optional[Dict]</code> Energy measurement results"},{"location":"reference/library-reference/#class-localrepoanalyzer","title":"class <code>LocalRepoAnalyzer</code>","text":"Method Parameters Description <code>__init__(clone_path, max_commits, days_back, skip_merges, compute_process_metrics, cleanup_after, ssh_key_path, github_token, energy_tracking, energy_backend, method_level_analysis, include_source_code, process_metrics, since_date, to_date)</code> see params Initialize analyzer with all analysis options. <code>analyze_repository(url)</code> <code>url: str</code> Clone and analyze a single repository. Handles authentication (HTTPS token injection, SSH key). Creates a fresh energy meter per repository for thread safety. Returns <code>RepositoryAnalysis</code>. <code>analyze_repositories(urls, parallel_workers, output_format)</code> <code>urls: List[str], parallel_workers: int, output_format: str</code> Analyze multiple repositories sequentially or in parallel using ThreadPoolExecutor. <code>analyze_commit(commit)</code> <code>commit</code> (PyDriller) Analyze a single PyDriller commit object. Extracts green awareness, GSF patterns, DMM metrics, structural metrics, optional method-level and source code data. <code>_compute_process_metrics(repo_path)</code> internal Compute 8 PyDriller process metrics: ChangeSet, CodeChurn, CommitsCount, ContributorsCount, ContributorsExperience, HistoryComplexity, HunksCount, LinesCount. <code>_prepare_auth_url(url)</code> internal Inject GitHub token into HTTPS URL for private repository access. <code>_setup_ssh_env()</code> internal Configure SSH environment for private repository cloning. <code>_parse_repo_url(url)</code> internal Parse owner and name from HTTPS or SSH GitHub URLs. <code>_extract_method_metrics(commit)</code> internal Extract per-method Lizard metrics from modified files. <code>_extract_source_changes(commit)</code> internal Extract source code before/after for each modified file."},{"location":"reference/library-reference/#greenminingservicesreportspy","title":"<code>greenmining/services/reports.py</code>","text":""},{"location":"reference/library-reference/#class-reportgenerator","title":"class <code>ReportGenerator</code>","text":"<p>Generates comprehensive Markdown reports from aggregated analysis data.</p> Method Parameters Description <code>generate_report(aggregated_data, analysis_data, repos_data)</code> see params Generate a full Markdown report with sections: Header, Executive Summary, Methodology, Results, Discussion, Limitations, Conclusion. <code>save_report(report_content, output_file)</code> <code>report_content: str, output_file: Path</code> Write report to Markdown file. <code>_generate_header()</code> internal Report title and metadata. <code>_generate_executive_summary(data)</code> internal Key findings, percentage summaries, implications. <code>_generate_methodology(repos_data, analysis_data)</code> internal Repository selection criteria, data extraction approach, analysis methodology (Q1/Q2/Q3). <code>_generate_results(data)</code> internal Green awareness section, known patterns table, emergent patterns, per-repo analysis, statistics. <code>_generate_discussion(data)</code> internal Interpretation, developer approaches, gap analysis, implications. <code>_generate_limitations()</code> internal Sample bias, commit message limitations, scope limitations. <code>_generate_conclusion(data)</code> internal Key findings, research question answers, recommendations."},{"location":"reference/library-reference/#analyzers","title":"Analyzers","text":""},{"location":"reference/library-reference/#greenmininganalyzersstatistical_analyzerpy","title":"<code>greenmining/analyzers/statistical_analyzer.py</code>","text":""},{"location":"reference/library-reference/#class-statisticalanalyzer","title":"class <code>StatisticalAnalyzer</code>","text":"<p>Advanced statistical analysis using scipy and numpy.</p> Method Parameters Description <code>analyze_pattern_correlations(commit_data)</code> <code>commit_data: DataFrame</code> Compute Pearson correlation matrix between pattern columns. Identifies significant pairs ( <code>temporal_trend_analysis(commits_df)</code> <code>commits_df: DataFrame</code> Monthly aggregation, Mann-Kendall trend test, optional seasonal decomposition (requires 24+ months), change point detection via rolling variance. Handles timezone-aware datetimes. <code>effect_size_analysis(group1, group2)</code> <code>group1: List[float], group2: List[float]</code> Cohen's d effect size with magnitude classification (negligible/small/medium/large). Includes independent t-test for significance. <code>pattern_adoption_rate_analysis(commits_df)</code> <code>commits_df: DataFrame</code> Analyze time-to-first-adoption, monthly adoption frequency, and pattern stickiness per pattern."},{"location":"reference/library-reference/#greenmininganalyzerstemporal_analyzerpy","title":"<code>greenmining/analyzers/temporal_analyzer.py</code>","text":""},{"location":"reference/library-reference/#class-temporalmetrics-dataclass","title":"class <code>TemporalMetrics</code> (dataclass)","text":"<p>Metrics for a specific time period: commit count, green count, green rate, unique patterns, dominant pattern, velocity.</p>"},{"location":"reference/library-reference/#class-trendanalysis-dataclass","title":"class <code>TrendAnalysis</code> (dataclass)","text":"<p>Trend analysis results: direction (increasing/decreasing/stable), slope, R-squared, start/end rates, change percentage.</p>"},{"location":"reference/library-reference/#class-temporalanalyzer","title":"class <code>TemporalAnalyzer</code>","text":"<p>Analyze temporal patterns in green software adoption over configurable time granularities.</p> Method Parameters Description <code>__init__(granularity)</code> <code>granularity: str</code> Initialize with time granularity: \"day\", \"week\", \"month\", \"quarter\", or \"year\". <code>group_commits_by_period(commits, date_field)</code> <code>commits: List[Dict], date_field: str</code> Group commits into time periods based on granularity. Handles both datetime objects and ISO strings. <code>calculate_period_metrics(period_key, commits, analysis_results)</code> see params Calculate <code>TemporalMetrics</code> for a single period. <code>analyze_trends(commits, analysis_results)</code> <code>commits: List[Dict], analysis_results: List[Dict]</code> Full temporal analysis: period metrics, linear trend (least squares), cumulative adoption curve, velocity trend, and pattern evolution timeline."},{"location":"reference/library-reference/#greenmininganalyzersqualitative_analyzerpy","title":"<code>greenmining/analyzers/qualitative_analyzer.py</code>","text":""},{"location":"reference/library-reference/#class-validationsample-dataclass","title":"class <code>ValidationSample</code> (dataclass)","text":"<p>Represents a single validation sample with commit data, detected patterns, and manual review fields.</p>"},{"location":"reference/library-reference/#class-validationmetrics-dataclass","title":"class <code>ValidationMetrics</code> (dataclass)","text":"<p>Precision, recall, F1 score, and accuracy metrics.</p>"},{"location":"reference/library-reference/#class-qualitativeanalyzer","title":"class <code>QualitativeAnalyzer</code>","text":"<p>Framework for manual validation and inter-rater reliability assessment.</p> Method Parameters Description <code>__init__(sample_size, stratify_by)</code> <code>sample_size: int, stratify_by: str</code> Initialize with sample size (default 30) and stratification method (\"pattern\" or \"repository\"). <code>generate_validation_samples(commits, analysis_results, include_negatives)</code> see params Generate stratified validation samples. 80% positive / 20% negative split for false-negative detection. <code>export_samples_for_review(output_path)</code> <code>output_path: str</code> Export samples to JSON for manual review. Includes instructions for reviewers. <code>import_validated_samples(input_path)</code> <code>input_path: str</code> Import manually validated samples from JSON. Updates sample statuses. <code>calculate_metrics()</code> none Calculate precision, recall, F1, and accuracy from validated samples. <code>get_validation_report()</code> none Generate comprehensive report: sampling info, metrics, error analysis (false positives/negatives), per-pattern accuracy. <code>get_inter_rater_reliability(samples_a, samples_b)</code> two lists of <code>ValidationSample</code> Calculate Cohen's Kappa for inter-rater reliability with interpretation (slight/fair/moderate/substantial/almost perfect)."},{"location":"reference/library-reference/#greenmininganalyzerscode_diff_analyzerpy","title":"<code>greenmining/analyzers/code_diff_analyzer.py</code>","text":""},{"location":"reference/library-reference/#class-codediffanalyzer","title":"class <code>CodeDiffAnalyzer</code>","text":"<p>Analyze code diffs to detect green software patterns in actual code changes. Contains regex-based pattern signatures for 13 categories:</p> <ul> <li>caching -- imports, annotations (<code>@cache</code>, <code>@lru_cache</code>), function calls, variable names</li> <li>resource_optimization -- Kubernetes resource limits, Docker optimization</li> <li>database_optimization -- indexes, query optimization, connection pooling</li> <li>async_processing -- async/await, ThreadPoolExecutor, Celery</li> <li>lazy_loading -- lazy, defer, dynamic import</li> <li>serverless_computing -- AWS Lambda, Azure Functions, serverless frameworks</li> <li>cdn_edge -- CloudFront, Cloudflare, edge caching</li> <li>compression -- gzip, brotli, zstd, lz4</li> <li>model_optimization -- quantization, pruning, ONNX, TensorRT</li> <li>efficient_protocols -- HTTP/2, gRPC, protobuf</li> <li>container_optimization -- Alpine, distroless, multi-stage builds</li> <li>green_regions -- renewable energy regions</li> <li>auto_scaling -- HPA, KEDA, scale-to-zero</li> <li>code_splitting -- React.lazy, Suspense, dynamic import</li> <li>green_ml_training -- early stopping, mixed precision, gradient checkpointing</li> </ul> Method Parameters Description <code>analyze_commit_diff(commit)</code> <code>commit: Commit</code> (PyDriller) Analyze all modified files in a commit. Returns patterns detected, evidence (file:line), confidence score, and code metrics. <code>_detect_patterns_in_line(code_line)</code> internal Match a single line against all pattern signatures. <code>_calculate_metrics(commit)</code> internal Calculate lines added/removed, files changed, net lines, complexity change. <code>_calculate_diff_confidence(patterns, evidence, metrics)</code> internal Confidence scoring: high (3+ patterns, 5+ evidence), medium (2+ patterns, 3+ evidence), low. <code>_is_code_file(modified_file)</code> internal Check if file is code (.py, .java, .go, etc.) or Kubernetes manifest."},{"location":"reference/library-reference/#greenmininganalyzersmetrics_power_correlatorpy","title":"<code>greenmining/analyzers/metrics_power_correlator.py</code>","text":""},{"location":"reference/library-reference/#class-correlationresult-dataclass","title":"class <code>CorrelationResult</code> (dataclass)","text":"<p>Result of a metrics-to-power correlation: Pearson r/p, Spearman r/p, significance, strength classification.</p>"},{"location":"reference/library-reference/#class-metricspowercorrelator","title":"class <code>MetricsPowerCorrelator</code>","text":"<p>Correlate code metrics (complexity, NLOC, churn) with power consumption measurements.</p> Method Parameters Description <code>__init__(significance_level)</code> <code>significance_level: float</code> Initialize with p-value threshold (default 0.05). <code>fit(metrics, metrics_values, power_measurements)</code> <code>metrics: List[str], metrics_values: Dict, power_measurements: List[float]</code> Compute Pearson and Spearman correlations for each metric against power data. Requires at least 3 data points. Computes feature importance (normalized absolute Spearman). <code>pearson</code> (property) none Get Pearson correlation values for all metrics. <code>spearman</code> (property) none Get Spearman correlation values for all metrics. <code>feature_importance</code> (property) none Get normalized feature importance scores. <code>get_results()</code> none Get all <code>CorrelationResult</code> objects. <code>get_significant_correlations()</code> none Filter to only statistically significant results. <code>summary()</code> none Generate summary with counts, correlations, feature importance, strongest positive/negative."},{"location":"reference/library-reference/#greenmininganalyzerspower_regressionpy","title":"<code>greenmining/analyzers/power_regression.py</code>","text":""},{"location":"reference/library-reference/#class-powerregression-dataclass","title":"class <code>PowerRegression</code> (dataclass)","text":"<p>A detected power regression: commit SHA, message, author, date, power before/after (watts), energy before/after (joules), percentage increase.</p>"},{"location":"reference/library-reference/#class-powerregressiondetector","title":"class <code>PowerRegressionDetector</code>","text":"<p>Detect commits that caused power consumption regressions by running a test command at each commit.</p> Method Parameters Description <code>__init__(test_command, energy_backend, threshold_percent, iterations, warmup_iterations)</code> see params Initialize detector. Default: <code>pytest tests/ -x</code>, RAPL backend, 5% threshold, 5 iterations, 1 warmup. <code>detect(repo_path, baseline_commit, target_commit, max_commits)</code> see params Iterate through commits from baseline to target. At each commit: checkout, run test command, measure energy. Flag commits where energy increased above threshold. Returns list of <code>PowerRegression</code> objects."},{"location":"reference/library-reference/#greenmininganalyzersversion_power_analyzerpy","title":"<code>greenmining/analyzers/version_power_analyzer.py</code>","text":""},{"location":"reference/library-reference/#class-versionpowerprofile-dataclass","title":"class <code>VersionPowerProfile</code> (dataclass)","text":"<p>Power profile for a single version: version tag, commit SHA, energy (joules), power (watts avg), duration, iterations, energy standard deviation.</p>"},{"location":"reference/library-reference/#class-versionpowerreport-dataclass","title":"class <code>VersionPowerReport</code> (dataclass)","text":"<p>Complete power analysis report across versions: list of profiles, trend direction, total change %, most/least efficient versions.</p> Method Description <code>to_dict()</code> Convert to dictionary. <code>summary()</code> Generate human-readable summary string."},{"location":"reference/library-reference/#class-versionpoweranalyzer","title":"class <code>VersionPowerAnalyzer</code>","text":"<p>Measure and compare power consumption across software versions/tags.</p> Method Parameters Description <code>__init__(test_command, energy_backend, iterations, warmup_iterations)</code> see params Initialize with test command and measurement settings. Default: 10 iterations, 2 warmup. <code>analyze_versions(repo_path, versions)</code> <code>repo_path: str, versions: List[str]</code> Measure energy for each version (checkout, warmup, measure N iterations). Returns <code>VersionPowerReport</code> with trend analysis (increasing/decreasing/stable based on 5% threshold)."},{"location":"reference/library-reference/#energy","title":"Energy","text":""},{"location":"reference/library-reference/#greenminingenergybasepy","title":"<code>greenmining/energy/base.py</code>","text":"<p>Core abstractions for energy measurement.</p>"},{"location":"reference/library-reference/#class-energybackend-enum","title":"class <code>EnergyBackend</code> (Enum)","text":"<p>Supported backends: <code>RAPL</code>, <code>CODECARBON</code>, <code>CPU_METER</code>.</p>"},{"location":"reference/library-reference/#class-energymetrics-dataclass","title":"class <code>EnergyMetrics</code> (dataclass)","text":"<p>Energy measurement results.</p> Field Type Description <code>joules</code> <code>float</code> Total energy consumed <code>watts_avg</code> <code>float</code> Average power draw <code>watts_peak</code> <code>float</code> Peak power draw <code>duration_seconds</code> <code>float</code> Measurement duration <code>cpu_energy_joules</code> <code>float</code> CPU-specific energy <code>dram_energy_joules</code> <code>float</code> Memory energy <code>gpu_energy_joules</code> <code>Optional[float]</code> GPU energy <code>carbon_grams</code> <code>Optional[float]</code> CO2 equivalent in grams <code>carbon_intensity</code> <code>Optional[float]</code> Grid carbon intensity (gCO2/kWh) <code>backend</code> <code>str</code> Backend name <code>start_time</code> / <code>end_time</code> <code>Optional[datetime]</code> Measurement timestamps Property Description <code>energy_joules</code> Alias for <code>joules</code>. <code>average_power_watts</code> Alias for <code>watts_avg</code>."},{"location":"reference/library-reference/#class-commitenergyprofile-dataclass","title":"class <code>CommitEnergyProfile</code> (dataclass)","text":"<p>Energy profile comparing a commit to its parent: <code>energy_before</code>, <code>energy_after</code>, <code>energy_delta</code>, <code>energy_regression</code>, <code>regression_percentage</code>.</p>"},{"location":"reference/library-reference/#class-energymeter-abc","title":"class <code>EnergyMeter</code> (ABC)","text":"<p>Abstract base class for all energy measurement backends.</p> Method Description <code>is_available()</code> Check if this backend works on the current system. <code>start()</code> Begin energy measurement. <code>stop()</code> Stop measurement, return <code>EnergyMetrics</code>. <code>measure(func, *args, **kwargs)</code> Measure energy of a function call. Returns <code>(result, EnergyMetrics)</code>. <code>measure_command(command, timeout)</code> Measure energy of a shell command. <code>__enter__</code> / <code>__exit__</code> Context manager support."},{"location":"reference/library-reference/#get_energy_meterbackend","title":"<code>get_energy_meter(backend)</code>","text":"<p>Factory function. Supported values: <code>\"rapl\"</code>, <code>\"codecarbon\"</code>, <code>\"cpu_meter\"</code>, <code>\"cpu\"</code>, <code>\"auto\"</code>. Auto mode tries RAPL first (most accurate), falls back to CPU meter.</p>"},{"location":"reference/library-reference/#greenminingenergyraplpy","title":"<code>greenmining/energy/rapl.py</code>","text":""},{"location":"reference/library-reference/#class-raplenergymeter","title":"class <code>RAPLEnergyMeter</code>","text":"<p>Intel RAPL (Running Average Power Limit) energy measurement for Linux. Reads directly from <code>/sys/class/powercap/intel-rapl</code>.</p> Method Description <code>__init__()</code> Discover available RAPL domains (package, core, dram, uncore). <code>is_available()</code> Check if RAPL sysfs interface exists and is readable. <code>start()</code> Record starting energy values for all domains. <code>stop()</code> Calculate energy delta per domain. Handles 32-bit counter wrap-around. Returns <code>EnergyMetrics</code> with CPU, DRAM, and GPU (uncore) breakdowns. <code>get_available_domains()</code> List discovered RAPL domain names."},{"location":"reference/library-reference/#greenminingenergycpu_meterpy","title":"<code>greenmining/energy/cpu_meter.py</code>","text":""},{"location":"reference/library-reference/#class-cpuenergymeter","title":"class <code>CPUEnergyMeter</code>","text":"<p>Cross-platform CPU energy estimation. Works on Linux, macOS, and Windows. Estimates power from CPU utilization percentage and TDP.</p> <p>Power model: <code>P = P_idle + (P_max - P_idle) * utilization</code> where idle power is 30% of TDP.</p> Method Parameters Description <code>__init__(tdp_watts, sample_interval)</code> <code>tdp_watts: Optional[float], sample_interval: float</code> Initialize. Auto-detects TDP from RAPL sysfs on Linux, or uses platform defaults (Linux: 65W, macOS: 30W, Windows: 65W). <code>is_available()</code> none Always returns True (universal fallback). <code>start()</code> none Begin measurement, prime psutil. <code>stop()</code> none Calculate estimated energy from CPU utilization samples."},{"location":"reference/library-reference/#greenminingenergycodecarbon_meterpy","title":"<code>greenmining/energy/codecarbon_meter.py</code>","text":""},{"location":"reference/library-reference/#class-codecarbonmeter","title":"class <code>CodeCarbonMeter</code>","text":"<p>Energy measurement with CO2 tracking via the CodeCarbon library. Provides carbon emissions in addition to energy data.</p> Method Parameters Description <code>__init__(project_name, output_dir, save_to_file)</code> see params Initialize CodeCarbon tracker. <code>is_available()</code> none Check if <code>codecarbon</code> package is installed. <code>start()</code> none Create and start <code>EmissionsTracker</code>. <code>stop()</code> none Stop tracker, extract energy (kWh to joules), emissions (kg to grams), and carbon intensity. Handles CodeCarbon v3.x Energy objects. <code>get_carbon_intensity()</code> none Query current grid carbon intensity for the configured region."},{"location":"reference/library-reference/#greenminingenergycarbon_reporterpy","title":"<code>greenmining/energy/carbon_reporter.py</code>","text":""},{"location":"reference/library-reference/#carbon_intensity_by_country","title":"<code>CARBON_INTENSITY_BY_COUNTRY</code>","text":"<p>Dictionary of average carbon intensity (gCO2/kWh) for 20 countries. Source: Electricity Maps, IEA.</p>"},{"location":"reference/library-reference/#cloud_region_intensity","title":"<code>CLOUD_REGION_INTENSITY</code>","text":"<p>Dictionary of carbon intensity by cloud provider region for AWS (14 regions), GCP (9 regions), and Azure (8 regions).</p>"},{"location":"reference/library-reference/#class-carbonreport-dataclass","title":"class <code>CarbonReport</code> (dataclass)","text":"<p>Carbon emissions report: total energy (kWh), emissions (kg), carbon intensity, equivalents (tree-months, smartphone charges, km driven).</p>"},{"location":"reference/library-reference/#class-carbonreporter","title":"class <code>CarbonReporter</code>","text":"<p>Generate carbon footprint reports from energy measurements.</p> Method Parameters Description <code>__init__(country_iso, cloud_provider, region)</code> see params Initialize with location. Cloud region intensity takes priority over country average. <code>generate_report(energy_metrics, analysis_results, total_joules)</code> see params Generate <code>CarbonReport</code> from energy data. Converts joules to kWh, applies carbon intensity, calculates equivalents. <code>get_carbon_intensity()</code> none Get the configured carbon intensity value. <code>get_supported_countries()</code> static List supported country ISO codes. <code>get_supported_cloud_regions(provider)</code> static List supported regions for a cloud provider."},{"location":"reference/library-reference/#controllers","title":"Controllers","text":""},{"location":"reference/library-reference/#greenminingcontrollersrepository_controllerpy","title":"<code>greenmining/controllers/repository_controller.py</code>","text":""},{"location":"reference/library-reference/#class-repositorycontroller","title":"class <code>RepositoryController</code>","text":"<p>Orchestrates repository fetching operations using the GraphQL fetcher and configuration.</p> Method Parameters Description <code>__init__(config)</code> <code>config: Config</code> Initialize with Config object. Creates <code>GitHubGraphQLFetcher</code>. <code>fetch_repositories(max_repos, min_stars, languages, keywords, created_after, created_before, pushed_after, pushed_before)</code> see params Fetch repositories via GraphQL, save to JSON file. Parameters default to Config values. <code>load_repositories()</code> none Load previously fetched repositories from JSON file. <code>get_repository_stats(repositories)</code> <code>repositories: list[Repository]</code> Compute statistics: total count, by-language breakdown, total/avg stars, top repository."},{"location":"reference/library-reference/#presenters","title":"Presenters","text":""},{"location":"reference/library-reference/#greenminingpresentersconsole_presenterpy","title":"<code>greenmining/presenters/console_presenter.py</code>","text":""},{"location":"reference/library-reference/#class-consolepresenter","title":"class <code>ConsolePresenter</code>","text":"<p>Handles formatted console output using tabulate and colorama.</p> Method Parameters Description <code>show_banner()</code> static Display application banner. <code>show_repositories(repositories, limit)</code> static Display repository table (name, language, stars, description). <code>show_commit_stats(stats)</code> static Display commit statistics table. <code>show_analysis_results(results)</code> static Display analysis results summary. <code>show_pattern_distribution(patterns, limit)</code> static Display top N green patterns with counts and percentages. <code>show_pipeline_status(status)</code> static Display pipeline phase status. <code>show_progress_message(phase, current, total)</code> static Display progress percentage. <code>show_error(message)</code> static Print error in red. <code>show_success(message)</code> static Print success in green. <code>show_warning(message)</code> static Print warning in yellow."},{"location":"reference/models/","title":"Data Models Reference","text":"<p>Reference for GreenMining data models and structures.</p>"},{"location":"reference/models/#core-models","title":"Core Models","text":""},{"location":"reference/models/#repository","title":"Repository","text":"<p>Represents a GitHub repository.</p> <pre><code>from greenmining.models import Repository\n\nrepo = Repository(\n    repo_id=12345,\n    name=\"flask\",\n    full_name=\"pallets/flask\",\n    owner=\"pallets\",\n    url=\"https://github.com/pallets/flask\",\n    clone_url=\"https://github.com/pallets/flask.git\",\n    stars=65000,\n    forks=16000,\n    watchers=2500,\n    open_issues=50,\n    language=\"Python\",\n    last_updated=\"2024-01-15T10:00:00Z\",\n    created_at=\"2010-04-06T12:00:00Z\",\n    description=\"The Python micro framework for building web applications.\",\n    main_branch=\"main\"\n)\n</code></pre>"},{"location":"reference/models/#fields","title":"Fields","text":"Field Type Description <code>repo_id</code> int GitHub repository ID <code>name</code> str Repository name <code>full_name</code> str Full name (owner/repo) <code>owner</code> str Repository owner <code>url</code> str GitHub URL <code>clone_url</code> str Git clone URL <code>stars</code> int Star count <code>forks</code> int Fork count <code>watchers</code> int Watcher count <code>open_issues</code> int Open issue count <code>language</code> str Primary language <code>last_updated</code> str Last update timestamp <code>created_at</code> str Creation timestamp <code>description</code> str Repository description <code>main_branch</code> str Default branch name"},{"location":"reference/models/#commit","title":"Commit","text":"<p>Represents a Git commit.</p> <pre><code>from greenmining.models import Commit\n\ncommit = Commit(\n    sha=\"abc123def456789\",\n    message=\"Implement Redis caching for user sessions\",\n    author=\"developer\",\n    date=\"2024-01-15T10:30:00Z\",\n    repository=\"pallets/flask\"\n)\n</code></pre>"},{"location":"reference/models/#fields_1","title":"Fields","text":"Field Type Description <code>sha</code> str Commit SHA hash <code>message</code> str Commit message <code>author</code> str Author username <code>date</code> str Commit timestamp (ISO 8601) <code>repository</code> str Repository full name"},{"location":"reference/models/#analysisresult","title":"AnalysisResult","text":"<p>Represents the analysis result for a commit.</p> <pre><code>from greenmining.models import AnalysisResult\n\nresult = AnalysisResult(\n    commit=commit,\n    green_aware=True,\n    patterns=[\"Cache Static Data\"],\n    confidence=0.85\n)\n</code></pre>"},{"location":"reference/models/#fields_2","title":"Fields","text":"Field Type Description <code>commit</code> Commit The analyzed commit <code>green_aware</code> bool Whether commit is green-aware <code>patterns</code> list[str] Matched pattern names <code>confidence</code> float Detection confidence (0-1)"},{"location":"reference/models/#analysis-output-structures","title":"Analysis Output Structures","text":""},{"location":"reference/models/#commit-analysis-dictionary","title":"Commit Analysis Dictionary","text":"<p>When commits are analyzed, they're returned as dictionaries:</p> <pre><code>{\n    \"sha\": \"abc123def456789\",\n    \"message\": \"Implement Redis caching for user sessions\",\n    \"author\": \"developer\",\n    \"date\": \"2024-01-15T10:30:00Z\",\n    \"repository\": \"pallets/flask\",\n    \"green_aware\": True,\n    \"patterns\": [\"Cache Static Data\"],\n    \"category\": \"caching\",\n    \"keywords_matched\": [\"redis\", \"caching\"],\n    \"confidence\": 0.85,\n\n    # If diff analysis enabled\n    \"diff_patterns\": [\"caching\"],\n    \"files_modified\": 3,\n\n    # If process metrics enabled\n    \"insertions\": 45,\n    \"deletions\": 12,\n    \"dmm_unit_size\": 0.85,\n    \"dmm_unit_complexity\": 0.72,\n    \"dmm_unit_interfacing\": 0.90\n}\n</code></pre>"},{"location":"reference/models/#aggregated-statistics-structure","title":"Aggregated Statistics Structure","text":"<p>Output from <code>DataAggregator.aggregate()</code>:</p> <pre><code>{\n    \"summary\": {\n        \"total_commits\": 5000,\n        \"green_aware_count\": 1250,\n        \"green_aware_percentage\": 25.0,\n        \"total_repos\": 50,\n        \"repos_with_green_commits\": 42,\n        \"analysis_date\": \"2024-01-15T10:00:00Z\"\n    },\n\n    \"pattern_distribution\": {\n        \"Cache Static Data\": 320,\n        \"Use Async Instead of Sync\": 180,\n        \"Compress Transmitted Data\": 150,\n        \"Lazy Loading\": 120,\n        \"Optimize Database Queries\": 95\n    },\n\n    \"category_distribution\": {\n        \"cloud\": 450,\n        \"caching\": 320,\n        \"async\": 210,\n        \"web\": 180,\n        \"database\": 95\n    },\n\n    \"per_repo_stats\": [\n        {\n            \"repository\": \"pallets/flask\",\n            \"total_commits\": 200,\n            \"green_aware_count\": 47,\n            \"green_aware_percentage\": 23.5,\n            \"top_patterns\": [\"Cache Static Data\", \"Lazy Loading\"]\n        }\n    ],\n\n    \"per_language_stats\": {\n        \"Python\": {\n            \"total_commits\": 2000,\n            \"green_aware_count\": 520,\n            \"green_aware_percentage\": 26.0\n        }\n    },\n\n    # If enable_temporal=True\n    \"temporal_analysis\": {\n        \"periods\": [\n            {\n                \"period\": \"2024-Q1\",\n                \"commit_count\": 500,\n                \"green_count\": 125,\n                \"green_awareness_rate\": 0.25\n            }\n        ],\n        \"overall_trend\": {\n            \"direction\": \"increasing\",\n            \"significant\": True\n        }\n    },\n\n    # If enable_stats=True\n    \"statistics\": {\n        \"pattern_correlations\": {\n            \"top_positive_correlations\": [\n                {\n                    \"pattern1\": \"caching\",\n                    \"pattern2\": \"performance\",\n                    \"correlation\": 0.75\n                }\n            ]\n        },\n        \"effect_size\": {\n            \"green_vs_nongreen_patterns\": {\n                \"cohens_d\": 0.65,\n                \"magnitude\": \"medium\"\n            }\n        },\n        \"descriptive\": {\n            \"patterns_per_commit\": {\n                \"mean\": 2.3,\n                \"median\": 2.0,\n                \"std\": 1.1\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"reference/models/#url-analysis-result-structure","title":"URL Analysis Result Structure","text":"<p>Output from <code>LocalRepoAnalyzer.analyze_repository()</code>:</p> <pre><code>{\n    \"repository\": {\n        \"name\": \"flask\",\n        \"url\": \"https://github.com/pallets/flask\",\n        \"owner\": \"pallets\",\n        \"clone_path\": \"/tmp/greenmining_repos/flask\"\n    },\n\n    \"total_commits\": 200,\n    \"green_aware_count\": 47,\n    \"green_aware_percentage\": 23.5,\n\n    \"commits\": [\n        {\n            \"sha\": \"abc123...\",\n            \"message\": \"Optimize template caching\",\n            \"author\": \"developer\",\n            \"date\": \"2024-03-15T10:30:00\",\n            \"green_aware\": True,\n            \"patterns\": [\"Cache Static Data\"],\n\n            # Process metrics\n            \"modified_files\": 3,\n            \"insertions\": 45,\n            \"deletions\": 12,\n            \"files\": [\"app.py\", \"cache.py\", \"config.py\"],\n\n            # DMM metrics\n            \"dmm_unit_size\": 0.85,\n            \"dmm_unit_complexity\": 0.72,\n            \"dmm_unit_interfacing\": 0.90\n        }\n    ],\n\n    \"pattern_distribution\": {\n        \"Cache Static Data\": 15,\n        \"Use Async Instead of Sync\": 12,\n        \"Lazy Loading\": 8\n    },\n\n    \"process_metrics\": {\n        \"change_set\": {\n            \"max\": 25,\n            \"avg\": 5.2\n        },\n        \"code_churn\": {\n            \"added\": 5000,\n            \"removed\": 2000\n        },\n        \"contributors_count\": 45,\n        \"commits_count\": 200\n    }\n}\n</code></pre>"},{"location":"reference/models/#energy-metrics-structures","title":"Energy Metrics Structures","text":""},{"location":"reference/models/#energymetrics","title":"EnergyMetrics","text":"<pre><code>from greenmining.energy.base import EnergyMetrics\n\n@dataclass\nclass EnergyMetrics:\n    energy_joules: float       # Total energy consumed\n    duration_seconds: float    # Measurement duration\n    average_power_watts: float # Average power draw\n    start_time: datetime       # Start timestamp\n    end_time: datetime         # End timestamp\n\n    # CodeCarbon specific\n    energy_kwh: float = 0.0    # Energy in kWh\n    emissions_kg: float = 0.0  # CO2 emissions\n</code></pre>"},{"location":"reference/models/#commitenergyprofile","title":"CommitEnergyProfile","text":"<pre><code>from greenmining.energy.base import CommitEnergyProfile\n\n@dataclass\nclass CommitEnergyProfile:\n    commit_sha: str            # Commit identifier\n    energy_joules: float       # Energy for this commit\n    duration_seconds: float    # Analysis duration\n    patterns_detected: list    # Patterns found\n    files_analyzed: int        # Files in commit\n</code></pre>"},{"location":"reference/models/#pattern-structure","title":"Pattern Structure","text":"<p>GSF patterns are stored as dictionaries:</p> <pre><code>{\n    \"cache_static_data\": {\n        \"name\": \"Cache Static Data\",\n        \"category\": \"cloud\",\n        \"keywords\": [\"cache\", \"caching\", \"static\", \"cdn\", \"redis\", \"memcache\"],\n        \"description\": \"Cache static content to reduce server load and network transfers\",\n        \"sci_impact\": \"Reduces energy by minimizing redundant compute and network operations\"\n    }\n}\n</code></pre>"},{"location":"reference/models/#fields_3","title":"Fields","text":"Field Type Description <code>name</code> str Human-readable pattern name <code>category</code> str Pattern category <code>keywords</code> list[str] Detection keywords <code>description</code> str Pattern description <code>sci_impact</code> str Impact on software carbon intensity"},{"location":"reference/models/#working-with-models","title":"Working with Models","text":""},{"location":"reference/models/#serialization","title":"Serialization","text":"<pre><code>import json\n\n# Convert to JSON\nresult_json = json.dumps(analysis_result, default=str)\n\n# Load from JSON\nwith open(\"results.json\") as f:\n    results = json.load(f)\n</code></pre>"},{"location":"reference/models/#filtering-results","title":"Filtering Results","text":"<pre><code># Get only green-aware commits\ngreen_commits = [c for c in commits if c[\"green_aware\"]]\n\n# Group by pattern\nfrom collections import defaultdict\nby_pattern = defaultdict(list)\nfor commit in green_commits:\n    for pattern in commit[\"patterns\"]:\n        by_pattern[pattern].append(commit)\n\n# Get top patterns\ntop_patterns = sorted(\n    by_pattern.items(), \n    key=lambda x: len(x[1]), \n    reverse=True\n)[:10]\n</code></pre>"},{"location":"reference/models/#aggregating-custom-metrics","title":"Aggregating Custom Metrics","text":"<pre><code>import statistics\n\n# Calculate custom statistics\ngreen_counts = [r[\"green_aware_count\"] for r in per_repo_stats]\navg_green = statistics.mean(green_counts)\nmedian_green = statistics.median(green_counts)\nstd_green = statistics.stdev(green_counts) if len(green_counts) &gt; 1 else 0\n</code></pre>"},{"location":"reference/models/#next-steps","title":"Next Steps","text":"<ul> <li>Python API - Working with models</li> <li>GSF Patterns - Pattern reference</li> <li>Configuration - All options</li> </ul>"},{"location":"reference/patterns/","title":"GSF Patterns Reference","text":"<p>Complete reference for all 124 Green Software Foundation patterns supported by GreenMining.</p>"},{"location":"reference/patterns/#overview","title":"Overview","text":"<p>GreenMining detects patterns from the Green Software Foundation catalog, organized into 15 categories.</p> Statistic Value Total Patterns 124 Categories 15 Keywords 332"},{"location":"reference/patterns/#pattern-categories","title":"Pattern Categories","text":""},{"location":"reference/patterns/#cloud-patterns-40","title":"Cloud Patterns (40+)","text":"<p>Patterns for cloud-native and infrastructure optimization.</p> Pattern Keywords Description Cache Static Data cache, caching, static, cdn, redis, memcache Cache static content to reduce server load Choose Region Closest region, closest, proximity, latency Deploy in regions closest to users Compress Stored Data compress, storage, gzip, zstd Compress data at rest Compress Transmitted Data compress, transmission, gzip, brotli Compress data before network transfer Containerize Workload container, docker, kubernetes, pod Use containers for resource efficiency Delete Unused Storage delete, remove, unused, cleanup Remove unused storage resources Encrypt What Is Necessary encrypt, tls, ssl, crypto Only encrypt data that needs protection Evaluate CPU Architectures cpu, arm, graviton, processor Consider ARM and efficient CPUs Use Service Mesh service mesh, istio, linkerd, envoy Optimize service-to-service communication TLS Termination tls termination, ssl offload Terminate TLS at edge Implement Stateless Design stateless, session, horizontal Design without server-side state Match SLO Requirements slo, sla, service level Don't over-engineer beyond SLO Match VM Utilization vm, instance, size, utilization Right-size VMs to workload Move to Cloud cloud, migrate, migration Leverage cloud efficiency Optimize Average Utilization utilization, optimize, average Increase resource utilization Optimize High Utilization high utilization, maximize Target high utilization levels Optimize Network Traffic network, traffic, optimize Reduce unnecessary network traffic Scale Down Idle Resources scale down, idle, shutdown Reduce resources when idle Scale Infrastructure scaling, autoscaling, scale Scale based on demand Terminate Unused Resources terminate, unused, cleanup Remove unused resources Use Reserved Instances reserved, spot, savings Use cost-efficient instance types Use Serverless serverless, lambda, functions Use serverless for variable workloads Use Spot Instances spot, preemptible, interruption Use spot instances for cost savings"},{"location":"reference/patterns/#web-patterns-15","title":"Web Patterns (15+)","text":"<p>Patterns for web application optimization.</p> Pattern Keywords Description Enable Text Compression gzip, brotli, deflate Compress text responses Lazy Loading lazy, defer, on-demand Load content only when needed Minify CSS/JS minify, minification, uglify Reduce asset file sizes Optimize Images image, webp, avif, srcset Use modern image formats Use CDN cdn, content delivery, edge Serve from edge locations Cache HTTP Responses cache-control, etag, expires Enable browser caching Reduce DOM Size dom, elements, optimize Minimize DOM complexity Use Service Workers service worker, pwa, offline Enable offline caching Preconnect Resources preconnect, preload, prefetch Hint browser to connect early Remove Unused CSS unused, purge, tree-shake Remove dead CSS code Optimize Fonts font, woff2, subset Use efficient font loading Reduce JavaScript javascript, bundle, split Minimize JS payload Use HTTP/2 http2, multiplexing Use modern HTTP protocol Enable Keep-Alive keep-alive, persistent Reuse HTTP connections"},{"location":"reference/patterns/#aiml-patterns-10","title":"AI/ML Patterns (10+)","text":"<p>Patterns for machine learning optimization.</p> Pattern Keywords Description Model Optimization model, optimize, prune Optimize model architecture Quantization quantize, int8, fp16 Reduce model precision Knowledge Distillation distillation, student, teacher Train smaller models Efficient Training training, efficient, epoch Optimize training process Batch Inference batch, inference, throughput Process predictions in batches Model Caching model cache, warm, preload Cache loaded models Feature Selection feature, select, reduce Use fewer features Early Stopping early stop, convergence Stop training when converged Mixed Precision mixed precision, amp Use mixed precision training Gradient Checkpointing checkpoint, gradient, memory Trade compute for memory"},{"location":"reference/patterns/#caching-patterns-8","title":"Caching Patterns (8)","text":"<p>Patterns for caching strategies.</p> Pattern Keywords Description Redis Caching redis, cache, memory Use Redis for caching CDN Caching cdn, edge, cache Cache at CDN edge Database Query Cache query cache, mysql, postgres Cache database queries Application Cache app cache, memory, local In-memory application cache Distributed Cache distributed, memcached Multi-node caching Cache Invalidation invalidate, ttl, expire Proper cache expiration Write-Through Cache write-through, consistency Consistent caching Cache Warming warm, preload, prefetch Pre-populate caches"},{"location":"reference/patterns/#async-patterns-6","title":"Async Patterns (6)","text":"<p>Patterns for asynchronous processing.</p> Pattern Keywords Description Queue Non-Urgent Requests queue, async, defer Queue non-critical work Use Async Instead of Sync async, await, non-blocking Prefer async operations Batch Processing batch, bulk, aggregate Process in batches Event-Driven Architecture event, pub-sub, message Use event-driven design Background Jobs background, worker, job Process in background Stream Processing stream, reactive, flow Use streaming for large data"},{"location":"reference/patterns/#database-patterns-8","title":"Database Patterns (8)","text":"<p>Patterns for database optimization.</p> Pattern Keywords Description Optimize Database Queries query, optimize, explain Improve query performance Use Connection Pooling pool, connection, reuse Reuse database connections Index Optimization index, btree, covering Optimize database indexes Read Replicas replica, read, slave Scale reads with replicas Denormalization denormalize, join, embed Reduce join operations Partition Tables partition, shard, split Split large tables Use NoSQL nosql, document, key-value Use appropriate database type Lazy Loading Relations lazy, eager, n+1 Avoid N+1 query problems"},{"location":"reference/patterns/#network-patterns-6","title":"Network Patterns (6)","text":"<p>Patterns for network optimization.</p> Pattern Keywords Description HTTP Compression gzip, compress, transfer Compress HTTP responses Reduce API Calls batch, aggregate, graphql Minimize API requests Use WebSockets websocket, socket, realtime Use persistent connections Protocol Optimization http3, quic, protocol Use efficient protocols Edge Computing edge, close, proximity Process at the edge Connection Reuse keep-alive, persist, reuse Reuse network connections"},{"location":"reference/patterns/#resource-patterns-5","title":"Resource Patterns (5)","text":"<p>Patterns for resource management.</p> Pattern Keywords Description Memory Optimization memory, heap, gc Optimize memory usage CPU Optimization cpu, thread, parallel Optimize CPU usage I/O Optimization io, disk, buffer Optimize I/O operations Resource Pooling pool, reuse, recycle Pool expensive resources Garbage Collection Tuning gc, tuning, generational Tune GC parameters"},{"location":"reference/patterns/#code-patterns-4","title":"Code Patterns (4)","text":"<p>Patterns for code-level optimization.</p> Pattern Keywords Description Remove Dead Code dead code, unused, remove Eliminate unused code Algorithm Optimization algorithm, complexity, O(n) Use efficient algorithms Loop Optimization loop, iteration, vectorize Optimize loops Avoid Premature Optimization premature, profile, measure Profile before optimizing"},{"location":"reference/patterns/#infrastructure-patterns-4","title":"Infrastructure Patterns (4)","text":"<p>Patterns for infrastructure optimization.</p> Pattern Keywords Description Alpine Containers alpine, minimal, scratch Use minimal base images Infrastructure as Code iac, terraform, ansible Manage infrastructure as code Renewable Energy Regions renewable, green, carbon Use green energy regions Container Optimization container, layer, cache Optimize container builds"},{"location":"reference/patterns/#microservices-patterns-4","title":"Microservices Patterns (4)","text":"<p>Patterns for microservices architecture.</p> Pattern Keywords Description Service Decomposition decompose, microservice, split Right-size services Colocation Strategies colocate, affinity, proximity Place related services together Graceful Shutdown graceful, shutdown, sigterm Handle shutdown properly Service Mesh Optimization mesh, sidecar, istio Optimize service mesh overhead"},{"location":"reference/patterns/#monitoring-patterns-3","title":"Monitoring Patterns (3)","text":"<p>Patterns for observability optimization.</p> Pattern Keywords Description Efficient Logging logging, log level, structured Optimize log volume Metrics Aggregation metrics, aggregate, rollup Aggregate metrics efficiently Trace Sampling sampling, trace, opentelemetry Sample traces appropriately"},{"location":"reference/patterns/#general-patterns-8","title":"General Patterns (8)","text":"<p>General optimization patterns.</p> Pattern Keywords Description Feature Flags feature flag, toggle, switch Use feature flags Incremental Processing incremental, delta, diff Process only changes Precomputation precompute, materialize, cache Precompute expensive results Background Jobs background, async, worker Process in background Rate Limiting rate limit, throttle, backoff Limit request rates Circuit Breaker circuit, breaker, fallback Fail fast with fallbacks Retry with Backoff retry, backoff, exponential Retry with exponential backoff Timeout Configuration timeout, deadline, cancel Set appropriate timeouts"},{"location":"reference/patterns/#accessing-patterns-programmatically","title":"Accessing Patterns Programmatically","text":"<pre><code>from greenmining import GSF_PATTERNS\n\n# Count patterns\nprint(f\"Total patterns: {len(GSF_PATTERNS)}\")  # 124\n\n# Get all categories\ncategories = set(p[\"category\"] for p in GSF_PATTERNS.values())\nprint(f\"Categories: {sorted(categories)}\")\n\n# Find patterns by category\ncloud_patterns = [\n    p for p in GSF_PATTERNS.values() \n    if p[\"category\"] == \"cloud\"\n]\nprint(f\"Cloud patterns: {len(cloud_patterns)}\")\n\n# Get pattern details\npattern = GSF_PATTERNS[\"cache_static_data\"]\nprint(f\"Name: {pattern['name']}\")\nprint(f\"Category: {pattern['category']}\")\nprint(f\"Keywords: {pattern['keywords']}\")\nprint(f\"Description: {pattern['description']}\")\nprint(f\"SCI Impact: {pattern['sci_impact']}\")\n</code></pre>"},{"location":"reference/patterns/#green-keywords","title":"Green Keywords","text":"<p>The 332 green keywords used for detection:</p> <pre><code>from greenmining import GREEN_KEYWORDS\n\n# Categories of keywords\nkeyword_categories = {\n    \"energy\": [\"energy\", \"power\", \"watt\", \"joule\", \"consumption\"],\n    \"carbon\": [\"carbon\", \"emission\", \"co2\", \"greenhouse\", \"footprint\"],\n    \"efficiency\": [\"efficient\", \"efficiency\", \"optimize\", \"reduce\", \"minimize\"],\n    \"sustainability\": [\"sustainable\", \"green\", \"eco\", \"environmental\"],\n    \"performance\": [\"performance\", \"fast\", \"speed\", \"latency\", \"throughput\"],\n    \"resource\": [\"resource\", \"memory\", \"cpu\", \"disk\", \"network\"],\n    \"caching\": [\"cache\", \"cached\", \"caching\", \"redis\", \"memcache\"],\n    \"compression\": [\"compress\", \"gzip\", \"brotli\", \"minify\", \"compact\"],\n}\n\n# Sample keywords\nprint(GREEN_KEYWORDS[:20])\n# ['energy', 'power', 'carbon', 'emission', 'footprint', 'sustainability', ...]\n</code></pre>"},{"location":"reference/patterns/#pattern-detection-example","title":"Pattern Detection Example","text":"<pre><code>from greenmining import is_green_aware, get_pattern_by_keywords\n\n# Test messages\nmessages = [\n    \"Implement Redis caching for user sessions\",\n    \"Enable gzip compression on API responses\",\n    \"Migrate to serverless Lambda functions\",\n    \"Optimize database queries with proper indexing\",\n    \"Add lazy loading for images\",\n    \"Fix typo in documentation\",\n]\n\nfor msg in messages:\n    is_green = is_green_aware(msg)\n    patterns = get_pattern_by_keywords(msg) if is_green else []\n\n    if is_green:\n        print(f\"\ud83c\udf31 {msg}\")\n        print(f\"   Patterns: {patterns}\")\n    else:\n        print(f\"   {msg}\")\n</code></pre> <p>Output:</p> <pre><code>\ud83c\udf31 Implement Redis caching for user sessions\n   Patterns: ['Cache Static Data']\n\ud83c\udf31 Enable gzip compression on API responses\n   Patterns: ['Compress Transmitted Data', 'Enable Text Compression']\n\ud83c\udf31 Migrate to serverless Lambda functions\n   Patterns: ['Use Serverless']\n\ud83c\udf31 Optimize database queries with proper indexing\n   Patterns: ['Optimize Database Queries', 'Index Optimization']\n\ud83c\udf31 Add lazy loading for images\n   Patterns: ['Lazy Loading', 'Optimize Images']\n   Fix typo in documentation\n</code></pre>"},{"location":"reference/patterns/#contributing-patterns","title":"Contributing Patterns","text":"<p>To suggest new patterns or improvements:</p> <ol> <li>Check the GSF Patterns Catalog</li> <li>Open an issue on GitHub</li> <li>Submit a pull request with pattern additions</li> </ol>"},{"location":"reference/patterns/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - All configuration parameters</li> <li>Python API - Programmatic usage</li> <li>Data Models - Repository and Commit models</li> </ul>"},{"location":"user-guide/api/","title":"Python API Reference","text":"<p>Use GreenMining programmatically in your Python scripts.</p>"},{"location":"user-guide/api/#quick-import","title":"Quick Import","text":"<pre><code>from greenmining import (\n    GSF_PATTERNS,        # Dict of 124 GSF patterns\n    GREEN_KEYWORDS,      # List of 332 green keywords\n    is_green_aware,      # Check if message is green-aware\n    get_pattern_by_keywords,  # Get matched patterns\n    fetch_repositories,  # Fetch repos from GitHub\n    Config,              # Configuration class\n)\n</code></pre>"},{"location":"user-guide/api/#core-functions","title":"Core Functions","text":""},{"location":"user-guide/api/#is_green_aware","title":"is_green_aware()","text":"<p>Check if a commit message indicates green software awareness.</p> <pre><code>def is_green_aware(commit_message: str) -&gt; bool\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>commit_message</code> str The commit message to analyze <p>Returns: <code>bool</code> - True if message contains green keywords</p> <p>Example:</p> <pre><code>from greenmining import is_green_aware\n\n# Returns True\nis_green_aware(\"Optimize Redis caching for better performance\")\nis_green_aware(\"Enable gzip compression on API responses\")\nis_green_aware(\"Implement async batch processing\")\n\n# Returns False\nis_green_aware(\"Fix typo in README\")\nis_green_aware(\"Update dependencies\")\nis_green_aware(\"Refactor variable names\")\n</code></pre>"},{"location":"user-guide/api/#get_pattern_by_keywords","title":"get_pattern_by_keywords()","text":"<p>Find GSF patterns that match a commit message.</p> <pre><code>def get_pattern_by_keywords(commit_message: str) -&gt; list[str]\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>commit_message</code> str The commit message to analyze <p>Returns: <code>list[str]</code> - List of matched pattern names</p> <p>Example:</p> <pre><code>from greenmining import get_pattern_by_keywords\n\npatterns = get_pattern_by_keywords(\"Implement Redis caching layer\")\nprint(patterns)\n# Output: ['Cache Static Data']\n\npatterns = get_pattern_by_keywords(\"Enable gzip compression for API responses\")\nprint(patterns)\n# Output: ['Compress Transmitted Data', 'Enable Text Compression']\n\npatterns = get_pattern_by_keywords(\"Fix typo\")\nprint(patterns)\n# Output: []\n</code></pre>"},{"location":"user-guide/api/#fetch_repositories","title":"fetch_repositories()","text":"<p>Fetch repositories from GitHub matching search criteria.</p> <pre><code>def fetch_repositories(\n    github_token: str,\n    max_repos: int = 100,\n    min_stars: int = 100,\n    languages: list = None,\n    keywords: str = \"microservices\"\n) -&gt; list\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>github_token</code> str (required) GitHub personal access token <code>max_repos</code> int 100 Maximum repositories to fetch <code>min_stars</code> int 100 Minimum star count <code>languages</code> list None Filter by languages <code>keywords</code> str \"microservices\" Search keywords <p>Returns: <code>list</code> - List of Repository objects</p> <p>Example:</p> <pre><code>from greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=\"ghp_xxxx\",\n    max_repos=10,\n    min_stars=500,\n    keywords=\"kubernetes\",\n    languages=[\"Python\", \"Go\"]\n)\n\nfor repo in repos:\n    print(f\"{repo.full_name}: {repo.stars} stars\")\n</code></pre>"},{"location":"user-guide/api/#data-structures","title":"Data Structures","text":""},{"location":"user-guide/api/#gsf_patterns","title":"GSF_PATTERNS","text":"<p>Dictionary containing all 124 Green Software Foundation patterns.</p> <pre><code>from greenmining import GSF_PATTERNS\n\n# Structure\nGSF_PATTERNS = {\n    \"pattern_id\": {\n        \"name\": \"Pattern Name\",\n        \"category\": \"category_name\",\n        \"keywords\": [\"keyword1\", \"keyword2\"],\n        \"description\": \"Pattern description\",\n        \"sci_impact\": \"Impact on software carbon intensity\"\n    },\n    ...\n}\n</code></pre> <p>Example Usage:</p> <pre><code>from greenmining import GSF_PATTERNS\n\n# Get pattern count\nprint(f\"Total patterns: {len(GSF_PATTERNS)}\")  # 124\n\n# Get all categories\ncategories = set(p[\"category\"] for p in GSF_PATTERNS.values())\nprint(f\"Categories: {categories}\")\n# {'cloud', 'web', 'ai', 'caching', 'async', 'database', ...}\n\n# Find patterns by category\ncloud_patterns = [\n    p[\"name\"] for p in GSF_PATTERNS.values() \n    if p[\"category\"] == \"cloud\"\n]\nprint(f\"Cloud patterns: {len(cloud_patterns)}\")  # 40+\n\n# Get pattern details\ncache_pattern = GSF_PATTERNS[\"cache_static_data\"]\nprint(f\"Name: {cache_pattern['name']}\")\nprint(f\"Keywords: {cache_pattern['keywords']}\")\n</code></pre>"},{"location":"user-guide/api/#green_keywords","title":"GREEN_KEYWORDS","text":"<p>List of 332 keywords indicating green software practices.</p> <pre><code>from greenmining import GREEN_KEYWORDS\n\nprint(f\"Total keywords: {len(GREEN_KEYWORDS)}\")  # 332\n\n# Sample keywords\nprint(GREEN_KEYWORDS[:10])\n# ['energy', 'power', 'carbon', 'emission', 'footprint', \n#  'sustainability', 'sustainable', 'green', 'efficient', 'efficiency']\n</code></pre>"},{"location":"user-guide/api/#analyze_repositories","title":"analyze_repositories()","text":"<p>Analyze multiple repositories from URLs with optional parallel processing.</p> <pre><code>def analyze_repositories(\n    urls: list,\n    max_commits: int = 500,\n    parallel_workers: int = 1,\n    output_format: str = \"dict\",\n    energy_tracking: bool = False,\n    energy_backend: str = \"rapl\",\n    method_level_analysis: bool = False,\n    include_source_code: bool = False,\n    ssh_key_path: str = None,\n    github_token: str = None,\n) -&gt; list\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>urls</code> list (required) List of GitHub repository URLs <code>max_commits</code> int 500 Maximum commits per repository <code>parallel_workers</code> int 1 Concurrent analysis workers <code>energy_tracking</code> bool False Enable energy measurement <code>energy_backend</code> str \"rapl\" Energy backend (rapl, codecarbon, cpu_meter, auto) <code>method_level_analysis</code> bool False Include per-method metrics <code>include_source_code</code> bool False Include source code before/after <code>ssh_key_path</code> str None SSH key for private repos <code>github_token</code> str None GitHub token for private HTTPS repos <p>Example:</p> <pre><code>from greenmining import analyze_repositories\n\nresults = analyze_repositories(\n    urls=[\n        \"https://github.com/kubernetes/kubernetes\",\n        \"https://github.com/istio/istio\",\n    ],\n    max_commits=100,\n    parallel_workers=4,\n    energy_tracking=True,\n    energy_backend=\"auto\",\n)\n\nfor result in results:\n    print(f\"{result.name}: {result.green_commit_rate:.1%} green\")\n</code></pre>"},{"location":"user-guide/api/#service-classes","title":"Service Classes","text":""},{"location":"user-guide/api/#dataanalyzer","title":"DataAnalyzer","text":"<p>Analyze commits for green software patterns.</p> <pre><code>from greenmining.services.data_analyzer import DataAnalyzer\n\nanalyzer = DataAnalyzer(\n    enable_diff_analysis=False,  # Analyze code diffs\n    patterns=None,               # Custom patterns (default: GSF_PATTERNS)\n    batch_size=10                # Commits per batch\n)\n</code></pre> <p>Methods:</p> <pre><code># Analyze a single commit\nresult = analyzer.analyze_commit(commit_dict)\n\n# Analyze multiple commits\nresults = analyzer.analyze_commits(commits_list)\n\n# Save results to file\nanalyzer.save_results(results, \"output.json\")\n</code></pre> <p>Example:</p> <pre><code>from greenmining.services.data_analyzer import DataAnalyzer\n\nanalyzer = DataAnalyzer(enable_diff_analysis=True)\n\ncommit = {\n    \"sha\": \"abc123\",\n    \"message\": \"Implement Redis caching for user sessions\",\n    \"author\": \"developer\",\n    \"date\": \"2024-01-15T10:00:00Z\"\n}\n\nresult = analyzer.analyze_commit(commit)\nprint(f\"Green-aware: {result['green_aware']}\")\nprint(f\"Patterns: {result['patterns']}\")\n</code></pre>"},{"location":"user-guide/api/#dataaggregator","title":"DataAggregator","text":"<p>Aggregate analysis results with statistics.</p> <pre><code>from greenmining.services.data_aggregator import DataAggregator\n\naggregator = DataAggregator(\n    config=None,                  # Config object\n    enable_stats=True,            # Statistical analysis\n    enable_temporal=True,         # Temporal trends\n    temporal_granularity=\"quarter\"  # day/week/month/quarter/year\n)\n</code></pre> <p>Methods:</p> <pre><code># Aggregate results\naggregated = aggregator.aggregate(analysis_results, repositories)\n\n# Save to files\naggregator.save_results(aggregated, \"stats.json\", \"stats.csv\", analysis_results)\n\n# Print summary\naggregator.print_summary(aggregated)\n</code></pre> <p>Example:</p> <pre><code>from greenmining.services.data_aggregator import DataAggregator\n\naggregator = DataAggregator(\n    enable_stats=True,\n    enable_temporal=True,\n    temporal_granularity=\"month\"\n)\n\n# Assuming analysis_results and repositories are already populated\naggregated = aggregator.aggregate(analysis_results, repositories)\n\nprint(f\"Total commits: {aggregated['summary']['total_commits']}\")\nprint(f\"Green-aware: {aggregated['summary']['green_aware_percentage']}%\")\n</code></pre>"},{"location":"user-guide/api/#localrepoanalyzer","title":"LocalRepoAnalyzer","text":"<p>Analyze repositories directly from GitHub URLs.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/greenmining_repos\",  # Clone directory\n    max_commits=500,                       # Max commits per repo\n    days_back=730,                         # How far back to analyze\n    skip_merges=True,                      # Skip merge commits\n    compute_process_metrics=True,          # Compute process metrics\n    cleanup_after=True,                    # Delete after analysis\n    ssh_key_path=None,                     # SSH key for private repos\n    github_token=None,                     # GitHub token for private repos\n    energy_tracking=False,                 # Enable energy measurement\n    energy_backend=\"rapl\",                 # Energy backend\n    method_level_analysis=False,           # Per-method metrics\n    include_source_code=False,             # Source code before/after\n    process_metrics=\"standard\",            # \"standard\" or \"full\"\n)\n</code></pre> <p>Methods:</p> <pre><code># Analyze single repository\nresult = analyzer.analyze_repository(\"https://github.com/owner/repo\")\n\n# Analyze multiple repositories (with parallelism)\nresults = analyzer.analyze_repositories(\n    urls=[\"https://github.com/org/repo1\", \"https://github.com/org/repo2\"],\n    parallel_workers=4,\n)\n</code></pre> <p>Example: Basic analysis</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\nresult = analyzer.analyze_repository(\"https://github.com/pallets/flask\")\n\nprint(f\"Repository: {result.name}\")\nprint(f\"Commits analyzed: {result.total_commits}\")\nprint(f\"Green-aware: {result.green_commits} ({result.green_commit_rate:.1%})\")\n\nfor commit in result.commits[:5]:\n    if commit.green_aware:\n        print(f\"  {commit.message[:50]}...\")\n</code></pre> <p>Example: Private repository with energy tracking</p> <pre><code>analyzer = LocalRepoAnalyzer(\n    github_token=\"ghp_xxxx\",\n    energy_tracking=True,\n    energy_backend=\"auto\",\n    method_level_analysis=True,\n)\n\nresult = analyzer.analyze_repository(\"https://github.com/company/private-repo\")\nprint(f\"Energy consumed: {result.energy_metrics['joules']:.2f} J\")\n\nfor commit in result.commits:\n    for method in commit.methods:\n        print(f\"  {method.name}: complexity={method.complexity}\")\n</code></pre> <p>Example: Batch parallel analysis</p> <pre><code>analyzer = LocalRepoAnalyzer(max_commits=100)\nresults = analyzer.analyze_repositories(\n    urls=[\n        \"https://github.com/kubernetes/kubernetes\",\n        \"https://github.com/istio/istio\",\n        \"https://github.com/envoyproxy/envoy\",\n    ],\n    parallel_workers=3,\n)\n\nfor result in results:\n    print(f\"{result.name}: {result.green_commit_rate:.1%} green\")\n</code></pre>"},{"location":"user-guide/api/#reportgenerator","title":"ReportGenerator","text":"<p>Generate Markdown reports from analysis results.</p> <pre><code>from greenmining.services.reports import ReportGenerator\n\ngenerator = ReportGenerator()\n</code></pre> <p>Methods:</p> <pre><code># Generate full report\nreport = generator.generate_report(aggregated_data)\n\n# Save to file\ngenerator.save_report(report, \"report.md\")\n</code></pre>"},{"location":"user-guide/api/#analyzer-classes","title":"Analyzer Classes","text":""},{"location":"user-guide/api/#statisticalanalyzer","title":"StatisticalAnalyzer","text":"<p>Compute statistical metrics on analysis results.</p> <pre><code>from greenmining.analyzers.statistical_analyzer import StatisticalAnalyzer\n\nanalyzer = StatisticalAnalyzer()\n\n# Pattern correlations\ncorrelations = analyzer.analyze_pattern_correlations(analysis_results)\n\n# Effect sizes\neffect_sizes = analyzer.analyze_effect_sizes(analysis_results)\n\n# Descriptive statistics\ndescriptive = analyzer.get_descriptive_statistics(analysis_results)\n</code></pre>"},{"location":"user-guide/api/#temporalanalyzer","title":"TemporalAnalyzer","text":"<p>Analyze patterns over time.</p> <pre><code>from greenmining.analyzers.temporal_analyzer import TemporalAnalyzer\n\nanalyzer = TemporalAnalyzer(granularity=\"quarter\")\n\n# Group commits by period\nperiods = analyzer.group_commits_by_period(commits)\n\n# Analyze trends\ntrends = analyzer.analyze_trends(periods)\n\n# Pattern evolution\nevolution = analyzer.analyze_pattern_evolution(commits)\n</code></pre>"},{"location":"user-guide/api/#qualitativeanalyzer","title":"QualitativeAnalyzer","text":"<p>Generate validation samples for manual review.</p> <pre><code>from greenmining.analyzers.qualitative_analyzer import QualitativeAnalyzer\n\nanalyzer = QualitativeAnalyzer(\n    sample_size=30,\n    stratify_by=\"pattern\"  # pattern/repository/time/random\n)\n\n# Generate samples\nsamples = analyzer.generate_validation_samples(analysis_results)\n\n# Export for review\nanalyzer.export_samples_for_review(samples, \"validation_samples.csv\")\n</code></pre>"},{"location":"user-guide/api/#codediffanalyzer","title":"CodeDiffAnalyzer","text":"<p>Analyze code changes for green patterns.</p> <pre><code>from greenmining.analyzers.code_diff_analyzer import CodeDiffAnalyzer\n\nanalyzer = CodeDiffAnalyzer()\n\n# Check if file is analyzable\nis_code = analyzer.is_code_file(\"app.py\")  # True\n\n# Analyze diff content\npatterns = analyzer.detect_patterns_in_diff(diff_text)\n</code></pre>"},{"location":"user-guide/api/#powerregressiondetector","title":"PowerRegressionDetector","text":"<p>Identify commits that caused power consumption regressions by running a test command at each commit and measuring energy usage.</p> <pre><code>from greenmining.analyzers import PowerRegressionDetector\n\ndetector = PowerRegressionDetector(\n    test_command=\"pytest tests/ -x\",\n    energy_backend=\"rapl\",\n    threshold_percent=5.0,\n    iterations=5,\n    warmup_iterations=1,\n)\n\nregressions = detector.detect(\n    repo_path=\"/path/to/repo\",\n    baseline_commit=\"v1.0.0\",\n    target_commit=\"HEAD\",\n)\n\nfor regression in regressions:\n    print(f\"Commit {regression.sha[:8]}: +{regression.power_increase:.1f}%\")\n    print(f\"  Message: {regression.message}\")\n</code></pre> <p>Constructor Parameters:</p> Parameter Type Default Description <code>test_command</code> str <code>\"pytest tests/ -x\"</code> Shell command to run for measurement <code>energy_backend</code> str <code>\"rapl\"</code> Energy backend (rapl, codecarbon, cpu_meter, auto) <code>threshold_percent</code> float <code>5.0</code> Minimum % increase to flag as regression <code>iterations</code> int <code>5</code> Measurement iterations per commit <code>warmup_iterations</code> int <code>1</code> Warmup runs before measuring <p>PowerRegression Output:</p> Field Type Description <code>sha</code> str Commit SHA <code>message</code> str Commit message <code>author</code> str Commit author <code>date</code> str Commit date <code>power_before</code> float Average power before (watts) <code>power_after</code> float Average power after (watts) <code>power_increase</code> float Percentage increase <code>energy_before</code> float Energy before (joules) <code>energy_after</code> float Energy after (joules) <code>is_regression</code> bool True if above threshold"},{"location":"user-guide/api/#metricspowercorrelator","title":"MetricsPowerCorrelator","text":"<p>Correlate code metrics (complexity, NLOC, churn) with power consumption using Pearson and Spearman correlation coefficients.</p> <pre><code>from greenmining.analyzers import MetricsPowerCorrelator\n\ncorrelator = MetricsPowerCorrelator(significance_level=0.05)\ncorrelator.fit(\n    metrics=[\"complexity\", \"nloc\", \"code_churn\"],\n    metrics_values={\n        \"complexity\": [...],\n        \"nloc\": [...],\n        \"code_churn\": [...],\n    },\n    power_measurements=[...],\n)\n\n# Access results\nfor name, result in correlator.get_results().items():\n    print(f\"{name}: pearson={result.pearson_r:.3f}, spearman={result.spearman_r:.3f}\")\n    print(f\"  Significant: {result.significant}, Strength: {result.strength}\")\n\n# Feature importance ranking\nfor name, importance in correlator.feature_importance.items():\n    print(f\"{name}: {importance:.3f}\")\n</code></pre> <p>Constructor Parameters:</p> Parameter Type Default Description <code>significance_level</code> float <code>0.05</code> P-value threshold for significance <p>CorrelationResult Output:</p> Field Type Description <code>metric_name</code> str Name of the metric <code>pearson_r</code> float Pearson correlation coefficient <code>pearson_p</code> float Pearson p-value <code>spearman_r</code> float Spearman rank correlation <code>spearman_p</code> float Spearman p-value <code>significant</code> bool True if p &lt; significance_level <code>strength</code> str none, weak, moderate, strong"},{"location":"user-guide/api/#versionpoweranalyzer","title":"VersionPowerAnalyzer","text":"<p>Compare energy consumption across software versions by checking out tags/branches and running a test suite at each version.</p> <pre><code>from greenmining.analyzers import VersionPowerAnalyzer\n\nanalyzer = VersionPowerAnalyzer(\n    test_command=\"pytest tests/\",\n    energy_backend=\"rapl\",\n    iterations=10,\n    warmup_iterations=2,\n)\n\nreport = analyzer.analyze_versions(\n    repo_path=\"/path/to/repo\",\n    versions=[\"v1.0\", \"v1.1\", \"v1.2\", \"v2.0\"],\n)\n\nprint(report.summary())\nprint(f\"Trend: {report.trend}\")            # increasing, decreasing, stable\nprint(f\"Most efficient: {report.most_efficient}\")\nprint(f\"Total change: {report.total_change_percent:.1f}%\")\n\nfor v in report.versions:\n    print(f\"  {v.version}: {v.power_watts_avg:.2f}W (std: {v.energy_std:.4f})\")\n</code></pre> <p>Constructor Parameters:</p> Parameter Type Default Description <code>test_command</code> str <code>\"pytest tests/\"</code> Shell command to run per version <code>energy_backend</code> str <code>\"rapl\"</code> Energy backend <code>iterations</code> int <code>5</code> Measurement iterations per version <code>warmup_iterations</code> int <code>1</code> Warmup runs before measuring <p>VersionPowerReport Output:</p> Field Type Description <code>versions</code> list List of VersionPowerProfile objects <code>trend</code> str increasing, decreasing, or stable <code>total_change_percent</code> float % change from first to last version <code>most_efficient</code> str Version tag with lowest power <code>least_efficient</code> str Version tag with highest power <p>VersionPowerProfile Fields:</p> Field Type Description <code>version</code> str Version tag or branch name <code>commit_sha</code> str Resolved commit SHA <code>energy_joules</code> float Average energy per run <code>power_watts_avg</code> float Average power draw <code>duration_seconds</code> float Average test duration <code>iterations</code> int Number of measurement iterations <code>energy_std</code> float Standard deviation across iterations"},{"location":"user-guide/api/#carbonreporter","title":"CarbonReporter","text":"<p>Generate carbon footprint reports from energy measurements. Supports 20+ countries and major cloud providers (AWS, GCP, Azure).</p> <pre><code>from greenmining.energy import CarbonReporter\n\nreporter = CarbonReporter(\n    country_iso=\"USA\",\n    cloud_provider=\"aws\",\n    region=\"us-east-1\",\n)\n\nreport = reporter.generate_report(total_joules=1000.0)\nprint(f\"CO2 emissions: {report.total_emissions_kg:.4f} kg\")\nprint(f\"Equivalent: {report.tree_months:.1f} tree-months\")\nprint(report.summary())\n</code></pre>"},{"location":"user-guide/api/#configuration-class","title":"Configuration Class","text":"<pre><code>from greenmining.config import Config\n\nconfig = Config()\n\n# Access configuration values\nprint(config.MAX_REPOS)           # 100\nprint(config.COMMITS_PER_REPO)    # 1000\nprint(config.SUPPORTED_LANGUAGES) # ['Python', 'Java', ...]\nprint(config.OUTPUT_DIR)          # 'data'\n\n# URL Analysis options\nprint(config.REPOSITORY_URLS)     # []\nprint(config.CLONE_PATH)          # '/tmp/greenmining_repos'\n\n# Energy options\nprint(config.ENERGY_ENABLED)      # False\nprint(config.ENERGY_BACKEND)      # 'rapl'\n\n# Process metric options\nprint(config.PROCESS_METRICS_ENABLED)  # True\nprint(config.DMM_ENABLED)              # True\n</code></pre>"},{"location":"user-guide/api/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Complete GreenMining analysis workflow.\"\"\"\n\nfrom greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom greenmining.services.data_aggregator import DataAggregator\nfrom greenmining.services.reports import ReportGenerator\n\n# 1. Analyze repository\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=200\n)\n\nprint(f\"Analyzed {result['total_commits']} commits\")\nprint(f\"Green-aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\n# 2. Aggregate results\naggregator = DataAggregator(enable_stats=True, enable_temporal=True)\naggregated = aggregator.aggregate(\n    analysis_results=result['commits'],\n    repositories=[result['repository']]\n)\n\n# 3. Generate report\ngenerator = ReportGenerator()\nreport = generator.generate_report(aggregated)\ngenerator.save_report(report, \"flask_analysis.md\")\n\nprint(\"Report saved to flask_analysis.md\")\n</code></pre>"},{"location":"user-guide/api/#next-steps","title":"Next Steps","text":"<ul> <li>URL Analysis - Deep dive into URL-based analysis</li> <li>Energy Measurement - Power profiling with RAPL/CodeCarbon</li> <li>GSF Patterns Reference - All 124 patterns</li> </ul>"},{"location":"user-guide/energy/","title":"Energy Measurement","text":"<p>Measure energy consumption of your analysis workloads with RAPL and CodeCarbon.</p>"},{"location":"user-guide/energy/#overview","title":"Overview","text":"<p>GreenMining includes energy measurement capabilities to profile the power consumption of analysis operations. This is useful for:</p> <ul> <li>Research - Quantify energy cost of mining operations</li> <li>Optimization - Identify energy-intensive analysis steps</li> <li>Reporting - Include energy metrics in analysis reports</li> </ul>"},{"location":"user-guide/energy/#supported-backends","title":"Supported Backends","text":"Backend Platform Features RAPL Linux (Intel/AMD) Direct CPU/DRAM power reading CodeCarbon Cross-platform Emissions tracking, cloud support CPU Energy Meter All platforms Utilization-based estimation Auto All platforms RAPL if available, else CPU Meter"},{"location":"user-guide/energy/#rapl-backend","title":"RAPL Backend","text":"<p>Intel's Running Average Power Limit (RAPL) provides direct power measurements on Linux systems with Intel or AMD processors.</p>"},{"location":"user-guide/energy/#requirements","title":"Requirements","text":"<ul> <li>Linux operating system</li> <li>Intel Core 2nd generation+ or AMD Ryzen</li> <li>Read access to <code>/sys/class/powercap/intel-rapl/</code></li> </ul>"},{"location":"user-guide/energy/#checking-availability","title":"Checking Availability","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\nif meter.is_available():\n    print(\"RAPL is available on this system\")\nelse:\n    print(\"RAPL not available - try running as root\")\n</code></pre>"},{"location":"user-guide/energy/#basic-usage","title":"Basic Usage","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\n\n# Start measurement\nmeter.start()\n\n# Your workload here\nresult = expensive_computation()\n\n# Stop and get metrics\nmetrics = meter.stop()\n\nprint(f\"Energy consumed: {metrics.energy_joules:.2f} J\")\nprint(f\"Duration: {metrics.duration_seconds:.2f} s\")\nprint(f\"Average power: {metrics.average_power_watts:.2f} W\")\n</code></pre>"},{"location":"user-guide/energy/#with-context-manager","title":"With Context Manager","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\n\nwith meter.measure() as measurement:\n    # Your analysis code\n    analyzer.analyze_repository(repo_url)\n\nprint(f\"Analysis consumed {measurement.energy_joules:.2f} J\")\n</code></pre>"},{"location":"user-guide/energy/#permission-setup","title":"Permission Setup","text":"<p>RAPL typically requires root access. To allow non-root users:</p> <pre><code># Grant read access to RAPL files\nsudo chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\nsudo chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/intel-rapl:0:*/energy_uj\n\n# Or create a udev rule for persistent access\necho 'SUBSYSTEM==\"powercap\", ACTION==\"add\", RUN+=\"/bin/chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\"' | sudo tee /etc/udev/rules.d/99-rapl.rules\n</code></pre>"},{"location":"user-guide/energy/#codecarbon-backend","title":"CodeCarbon Backend","text":"<p>CodeCarbon tracks energy consumption and CO2 emissions across platforms.</p>"},{"location":"user-guide/energy/#installation","title":"Installation","text":"<pre><code>pip install codecarbon\n</code></pre>"},{"location":"user-guide/energy/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from greenmining.energy.codecarbon_meter import CodeCarbonMeter\n\nmeter = CodeCarbonMeter(\n    country_iso_code=\"USA\",  # For carbon intensity\n    project_name=\"greenmining-analysis\",\n    tracking_mode=\"process\"  # or \"machine\"\n)\n\n# Start tracking\nmeter.start()\n\n# Your workload\nresult = analyzer.analyze_repository(repo_url)\n\n# Stop and get metrics\nmetrics = meter.stop()\n\nprint(f\"Energy: {metrics.energy_kwh:.6f} kWh\")\nprint(f\"CO2 emissions: {metrics.emissions_kg:.6f} kg\")\nprint(f\"Duration: {metrics.duration_seconds:.2f} s\")\n</code></pre>"},{"location":"user-guide/energy/#configuration-options","title":"Configuration Options","text":"<pre><code>meter = CodeCarbonMeter(\n    country_iso_code=\"FRA\",      # France\n    region=\"ile-de-france\",       # Optional region\n    project_name=\"my-analysis\",\n    output_dir=\"./energy_logs\",   # Where to save logs\n    save_to_file=True,            # Save detailed logs\n    tracking_mode=\"process\"       # process or machine\n)\n</code></pre>"},{"location":"user-guide/energy/#carbon-tracking","title":"Carbon Tracking","text":"<pre><code>from greenmining.energy.codecarbon_meter import CodeCarbonMeter\n\nmeter = CodeCarbonMeter(project_name=\"greenmining\")\n\nmeter.start()\n# ... analysis ...\nmetrics = meter.stop()\n\nprint(f\"Energy: {metrics.energy_joules:.2f} J\")\nprint(f\"Carbon footprint: {metrics.carbon_grams:.4f} g CO2\")\n</code></pre>"},{"location":"user-guide/energy/#cpu-energy-meter","title":"CPU Energy Meter","text":"<p>Cross-platform energy estimation using CPU utilization and TDP modeling.</p>"},{"location":"user-guide/energy/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from greenmining.energy import CPUEnergyMeter\n\nmeter = CPUEnergyMeter()\nmeter.start()\n\n# Your workload\nresult = expensive_computation()\n\nmetrics = meter.stop()\nprint(f\"Estimated energy: {metrics.joules:.2f} J\")\nprint(f\"Average power: {metrics.watts_avg:.2f} W\")\n</code></pre>"},{"location":"user-guide/energy/#custom-tdp","title":"Custom TDP","text":"<pre><code>from greenmining.energy import CPUEnergyMeter\n\n# Specify your CPU's TDP for more accurate estimation\nmeter = CPUEnergyMeter(tdp_watts=45.0)  # e.g., laptop CPU\n</code></pre>"},{"location":"user-guide/energy/#auto-detection","title":"Auto-Detection","text":"<pre><code>from greenmining.energy import get_energy_meter\n\n# Automatically selects RAPL (if available) or CPU Meter\nmeter = get_energy_meter(\"auto\")\nmeter.start()\n# ... workload ...\nmetrics = meter.stop()\n</code></pre>"},{"location":"user-guide/energy/#integrated-energy-tracking","title":"Integrated Energy Tracking","text":"<p>Automatically measure energy during repository analysis.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    energy_tracking=True,\n    energy_backend=\"auto\",  # rapl, codecarbon, cpu_meter, auto\n)\n\nresult = analyzer.analyze_repository(\"https://github.com/pallets/flask\")\nprint(f\"Energy consumed: {result.energy_metrics['joules']:.2f} J\")\nprint(f\"Average power: {result.energy_metrics['watts_avg']:.2f} W\")\nprint(f\"Duration: {result.energy_metrics['duration_seconds']:.2f} s\")\n</code></pre>"},{"location":"user-guide/energy/#carbon-footprint-reporting","title":"Carbon Footprint Reporting","text":"<p>Convert energy measurements to CO2 emissions using regional carbon intensity data.</p>"},{"location":"user-guide/energy/#basic-usage_3","title":"Basic Usage","text":"<pre><code>from greenmining.energy import CarbonReporter\n\nreporter = CarbonReporter(country_iso=\"USA\")\nreport = reporter.generate_report(total_joules=3600.0)\n\nprint(f\"CO2: {report.total_emissions_kg * 1000:.4f} grams\")\nprint(f\"Equivalent to {report.tree_months:.2f} tree-months\")\nprint(f\"Or {report.smartphone_charges:.1f} smartphone charges\")\n</code></pre>"},{"location":"user-guide/energy/#cloud-region-support","title":"Cloud Region Support","text":"<pre><code>from greenmining.energy import CarbonReporter\n\n# Use cloud provider region for accurate carbon intensity\nreporter = CarbonReporter(\n    country_iso=\"USA\",\n    cloud_provider=\"aws\",\n    region=\"eu-north-1\",  # Stockholm - low carbon (renewable energy)\n)\n\nreport = reporter.generate_report(total_joules=1000.0)\nprint(report.summary())\n</code></pre>"},{"location":"user-guide/energy/#supported-regions","title":"Supported Regions","text":"<pre><code>from greenmining.energy import CarbonReporter\n\n# 20+ country profiles\ncountries = CarbonReporter.get_supported_countries()\n\n# AWS, GCP, Azure regions\naws_regions = CarbonReporter.get_supported_cloud_regions(\"aws\")\ngcp_regions = CarbonReporter.get_supported_cloud_regions(\"gcp\")\n</code></pre>"},{"location":"user-guide/energy/#energy-metrics","title":"Energy Metrics","text":""},{"location":"user-guide/energy/#energymetrics-class","title":"EnergyMetrics Class","text":"<pre><code>from greenmining.energy.base import EnergyResult\n\n@dataclass\nclass EnergyResult:\n    energy_joules: float       # Total energy in Joules\n    duration_seconds: float    # Measurement duration\n    average_power_watts: float # Average power draw\n    start_time: datetime       # Measurement start\n    end_time: datetime         # Measurement end\n\n    # CodeCarbon specific\n    energy_kwh: float = 0.0    # Energy in kilowatt-hours\n    emissions_kg: float = 0.0  # CO2 emissions in kg\n</code></pre>"},{"location":"user-guide/energy/#commitenergyprofile","title":"CommitEnergyProfile","text":"<p>Track energy per commit analysis:</p> <pre><code>from greenmining.energy.base import CommitEnergyProfile\n\n@dataclass\nclass CommitEnergyProfile:\n    commit_sha: str\n    energy_joules: float\n    duration_seconds: float\n    patterns_detected: list\n    files_analyzed: int\n</code></pre>"},{"location":"user-guide/energy/#research-applications","title":"Research Applications","text":""},{"location":"user-guide/energy/#measuring-analysis-efficiency","title":"Measuring Analysis Efficiency","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nmeter = RAPLEnergyMeter()\nanalyzer = LocalRepoAnalyzer()\n\nrepos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n]\n\nresults = []\nfor url in repos:\n    meter.start()\n    analysis = analyzer.analyze_repository(url, max_commits=100)\n    energy = meter.stop()\n\n    results.append({\n        \"repo\": url,\n        \"commits\": analysis[\"total_commits\"],\n        \"energy_joules\": energy.energy_joules,\n        \"joules_per_commit\": energy.energy_joules / analysis[\"total_commits\"]\n    })\n\n# Compare efficiency\nfor r in results:\n    print(f\"{r['repo']}: {r['joules_per_commit']:.3f} J/commit\")\n</code></pre>"},{"location":"user-guide/energy/#energy-aware-batch-processing","title":"Energy-Aware Batch Processing","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\nenergy_budget_joules = 1000  # Set energy budget\n\ntotal_energy = 0\nanalyzed_repos = []\n\nfor repo in repositories:\n    meter.start()\n    result = analyze(repo)\n    metrics = meter.stop()\n\n    total_energy += metrics.energy_joules\n    analyzed_repos.append(repo)\n\n    if total_energy &gt;= energy_budget_joules:\n        print(f\"Energy budget reached after {len(analyzed_repos)} repos\")\n        break\n</code></pre>"},{"location":"user-guide/energy/#comparing-backends","title":"Comparing Backends","text":"Feature RAPL CodeCarbon CPU Meter Accuracy High (hardware) Medium (estimation) Low (model) Platform Linux only Cross-platform All platforms Granularity Microseconds Seconds Seconds CO2 tracking No Yes Via CarbonReporter Cloud support No Yes Via CarbonReporter Setup May need root pip install No setup"},{"location":"user-guide/energy/#recommendation","title":"Recommendation","text":"<ul> <li>Use RAPL for precise measurements on Linux</li> <li>Use CodeCarbon for cross-platform and carbon tracking</li> <li>Use CPU Meter when neither RAPL nor CodeCarbon is available</li> <li>Use auto to let GreenMining pick the best available backend</li> </ul>"},{"location":"user-guide/energy/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/energy/#rapl-not-available","title":"RAPL Not Available","text":"<pre><code># Check if RAPL files exist\nimport os\nrapl_path = \"/sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\"\nprint(f\"RAPL exists: {os.path.exists(rapl_path)}\")\n\n# Check permissions\nif os.path.exists(rapl_path):\n    print(f\"Readable: {os.access(rapl_path, os.R_OK)}\")\n</code></pre>"},{"location":"user-guide/energy/#codecarbon-import-error","title":"CodeCarbon Import Error","text":"<pre><code># Install with all dependencies\npip install codecarbon[viz]\n\n# Or minimal install\npip install codecarbon\n</code></pre>"},{"location":"user-guide/energy/#virtual-machine-limitations","title":"Virtual Machine Limitations","text":"<p>RAPL typically doesn't work in VMs. Use CodeCarbon instead:</p> <pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\nfrom greenmining.energy.codecarbon_meter import CodeCarbonMeter\n\n# Try RAPL first, fall back to CodeCarbon\nrapl = RAPLEnergyMeter()\nif rapl.is_available():\n    meter = rapl\nelse:\n    print(\"RAPL not available, using CodeCarbon\")\n    meter = CodeCarbonMeter()\n</code></pre>"},{"location":"user-guide/energy/#next-steps","title":"Next Steps","text":"<ul> <li>Python API - Full API reference</li> <li>URL Analysis - Analyze repositories by URL</li> <li>Configuration - Energy settings</li> </ul>"},{"location":"user-guide/url-analysis/","title":"URL Analysis","text":"<p>Analyze GitHub repositories directly by URL.</p>"},{"location":"user-guide/url-analysis/#overview","title":"Overview","text":"<p>URL analysis allows you to analyze any GitHub repository without using the GitHub API rate limits. GreenMining clones repositories locally and extracts commit data with full diff information.</p>"},{"location":"user-guide/url-analysis/#benefits","title":"Benefits","text":"<ul> <li>No GitHub API limits - Clone and analyze directly</li> <li>Full commit data - Access diffs, modified files, metrics</li> <li>Process metrics - Code churn, change set size, contributor count</li> <li>DMM metrics - Delta Maintainability Model scores</li> <li>Method-level analysis - Per-function complexity via Lizard</li> <li>Historical analysis - Analyze any date range</li> </ul>"},{"location":"user-guide/url-analysis/#python-api","title":"Python API","text":""},{"location":"user-guide/url-analysis/#localrepoanalyzer","title":"LocalRepoAnalyzer","text":"<p>The main class for URL-based analysis.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/greenmining_repos\",  # Where to clone\n    cleanup_after=True                     # Delete after analysis\n)\n</code></pre>"},{"location":"user-guide/url-analysis/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>clone_path</code> str /tmp/greenmining_repos Directory for cloning <code>cleanup_after</code> bool True Delete cloned repo after analysis"},{"location":"user-guide/url-analysis/#single-repository-analysis","title":"Single Repository Analysis","text":"<p>Analyze a single repository.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom datetime import datetime\n\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=200,\n    since_date=datetime(2024, 1, 1),\n    to_date=datetime(2024, 12, 31)\n)\n\nprint(f\"Total commits: {result['total_commits']}\")\nprint(f\"Green-aware: {result['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"user-guide/url-analysis/#multiple-repositories","title":"Multiple Repositories","text":"<pre><code>repos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n    \"https://github.com/fastapi/fastapi\"\n]\n\nfor repo_url in repos:\n    result = analyzer.analyze_repository(repo_url, max_commits=100)\n    print(f\"{result['repository']['name']}: {result['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"user-guide/url-analysis/#analyze_repository-parameters","title":"analyze_repository() Parameters","text":"Parameter Type Default Description <code>repo_url</code> str (required) GitHub repository URL <code>max_commits</code> int 1000 Maximum commits to analyze <code>since_date</code> datetime None Start date filter <code>to_date</code> datetime None End date filter"},{"location":"user-guide/url-analysis/#return-value","title":"Return Value","text":"<pre><code>{\n    \"repository\": {\n        \"name\": \"flask\",\n        \"url\": \"https://github.com/pallets/flask\",\n        \"owner\": \"pallets\",\n        \"clone_path\": \"/tmp/greenmining_repos/flask\"\n    },\n    \"total_commits\": 200,\n    \"green_aware_count\": 47,\n    \"green_aware_percentage\": 23.5,\n    \"commits\": [\n        {\n            \"sha\": \"abc123...\",\n            \"message\": \"Optimize template caching\",\n            \"author\": \"developer\",\n            \"date\": \"2024-03-15T10:30:00\",\n            \"green_aware\": True,\n            \"patterns\": [\"Cache Static Data\"],\n            \"modified_files\": 3,\n            \"insertions\": 45,\n            \"deletions\": 12,\n            \"dmm_unit_size\": 0.85,\n            \"dmm_unit_complexity\": 0.72,\n            \"dmm_unit_interfacing\": 0.90\n        },\n        ...\n    ],\n    \"pattern_distribution\": {\n        \"Cache Static Data\": 15,\n        \"Use Async Instead of Sync\": 12,\n        ...\n    },\n    \"process_metrics\": {\n        \"change_set\": {\"max\": 25, \"avg\": 5.2},\n        \"code_churn\": {\"added\": 5000, \"removed\": 2000},\n        \"contributors_count\": 45\n    }\n}\n</code></pre>"},{"location":"user-guide/url-analysis/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Analyze Flask repository for green patterns.\"\"\"\n\nfrom datetime import datetime\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\n# Initialize analyzer\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/flask_analysis\",\n    cleanup_after=True\n)\n\n# Analyze repository\nprint(\"Analyzing Flask repository...\")\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=100,\n    since_date=datetime(2024, 1, 1)\n)\n\n# Print summary\nprint(f\"\\n{'='*60}\")\nprint(\"ANALYSIS RESULTS\")\nprint(f\"{'='*60}\")\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Total commits: {result['total_commits']}\")\nprint(f\"Green-aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\n# Top patterns\nprint(f\"\\nTop Patterns:\")\nfor pattern, count in sorted(\n    result['pattern_distribution'].items(), \n    key=lambda x: x[1], \n    reverse=True\n)[:5]:\n    print(f\"  {pattern}: {count}\")\n\n# Sample green commits\nprint(f\"\\nSample Green Commits:\")\ngreen_commits = [c for c in result['commits'] if c['green_aware']]\nfor commit in green_commits[:5]:\n    print(f\"  \ud83c\udf31 {commit['message'][:60]}...\")\n    print(f\"     Patterns: {commit['patterns']}\")\n</code></pre> <p>Output:</p> <pre><code>Analyzing Flask repository...\n\n============================================================\nANALYSIS RESULTS\n============================================================\nRepository: flask\nTotal commits: 100\nGreen-aware: 23 (23.0%)\n\nTop Patterns:\n  Cache Static Data: 8\n  Use Async Instead of Sync: 5\n  Lazy Loading: 4\n  Compress Transmitted Data: 3\n  Optimize Database Queries: 3\n\nSample Green Commits:\n  \ud83c\udf31 Implement response caching for static assets...\n     Patterns: ['Cache Static Data']\n  \ud83c\udf31 Add async support for request handling...\n     Patterns: ['Use Async Instead of Sync']\n</code></pre>"},{"location":"user-guide/url-analysis/#supported-url-formats","title":"Supported URL Formats","text":"<pre><code># HTTPS (recommended)\n\"https://github.com/owner/repo\"\n\"https://github.com/owner/repo.git\"\n\n# SSH\n\"git@github.com:owner/repo.git\"\n\n# With branch (coming soon)\n\"https://github.com/owner/repo/tree/branch-name\"\n</code></pre>"},{"location":"user-guide/url-analysis/#commit-metrics","title":"Commit Metrics","text":"<p>GreenMining extracts the following metrics for each commit:</p>"},{"location":"user-guide/url-analysis/#basic-commit-metrics","title":"Basic Commit Metrics","text":"Metric Description <code>modified_files</code> Number of files changed <code>insertions</code> Lines added <code>deletions</code> Lines removed <code>files</code> List of modified file paths"},{"location":"user-guide/url-analysis/#dmm-metrics-delta-maintainability-model","title":"DMM Metrics (Delta Maintainability Model)","text":"<p>Measures how a commit impacts code maintainability on a 0-1 scale (higher is better).</p> Metric Range Description <code>dmm_unit_size</code> 0-1 Unit size maintainability \u2014 proportion of changed code units that remain within acceptable size thresholds <code>dmm_unit_complexity</code> 0-1 Cyclomatic complexity impact \u2014 proportion of changed code units with acceptable complexity <code>dmm_unit_interfacing</code> 0-1 Interface complexity \u2014 proportion of changed code units with manageable parameter counts"},{"location":"user-guide/url-analysis/#process-metrics","title":"Process Metrics","text":"<p>All 8 process metrics tracked per repository:</p> Metric Description <code>change_set</code> Number of files changed per commit (max, avg) <code>code_churn</code> Lines added/removed over time <code>contributors_count</code> Unique contributors in the analysis period <code>commits_count</code> Total commits in the analysis period <code>contributors_experience</code> Average experience of contributors (commits to repo) <code>history_complexity</code> Normalized entropy of file change history <code>hunks_count</code> Number of contiguous changed blocks per file <code>lines_count</code> Total lines of code modified across all commits"},{"location":"user-guide/url-analysis/#method-level-metrics","title":"Method-Level Metrics","text":"<p>When <code>method_level_analysis=True</code>, GreenMining uses Lizard to extract per-function metrics:</p> Metric Description <code>methods_count</code> Number of methods analyzed in a commit <code>total_nloc</code> Total non-comment lines of code <code>total_complexity</code> Sum of cyclomatic complexity across all methods <code>max_complexity</code> Highest single-function complexity <p>Each method entry includes:</p> Field Description <code>name</code> Function/method name <code>nloc</code> Non-comment lines of code <code>complexity</code> Cyclomatic complexity <code>token_count</code> Number of tokens <code>parameters</code> Number of parameters"},{"location":"user-guide/url-analysis/#configuration-options","title":"Configuration Options","text":"<p>Configure URL analysis via environment variables or Config:</p> <pre><code># Environment variables\nexport CLONE_PATH=/custom/path\nexport CLEANUP_AFTER_ANALYSIS=false\nexport PROCESS_METRICS_ENABLED=true\nexport DMM_ENABLED=true\n</code></pre> <pre><code># Python configuration\nfrom greenmining.config import Config\n\nconfig = Config()\nprint(config.CLONE_PATH)               # /tmp/greenmining_repos\nprint(config.CLEANUP_AFTER_ANALYSIS)   # True\nprint(config.PROCESS_METRICS_ENABLED)  # True\nprint(config.DMM_ENABLED)              # True\n</code></pre>"},{"location":"user-guide/url-analysis/#batch-analysis","title":"Batch Analysis","text":"<p>Analyze multiple repositories efficiently:</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nimport json\n\nrepos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n    \"https://github.com/fastapi/fastapi\",\n]\n\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\nall_results = []\n\nfor url in repos:\n    print(f\"Analyzing {url}...\")\n    result = analyzer.analyze_repository(url, max_commits=100)\n    all_results.append(result)\n    print(f\"  \u2713 {result['green_aware_count']}/{result['total_commits']} green-aware\")\n\n# Save combined results\nwith open(\"batch_results.json\", \"w\") as f:\n    json.dump(all_results, f, indent=2, default=str)\n\n# Summary\nprint(f\"\\nTotal repositories: {len(all_results)}\")\ntotal_commits = sum(r['total_commits'] for r in all_results)\ntotal_green = sum(r['green_aware_count'] for r in all_results)\nprint(f\"Total commits: {total_commits}\")\nprint(f\"Total green-aware: {total_green} ({total_green/total_commits*100:.1f}%)\")\n</code></pre>"},{"location":"user-guide/url-analysis/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/url-analysis/#clone-failures","title":"Clone Failures","text":"<pre><code># Increase timeout\nanalyzer = LocalRepoAnalyzer(clone_timeout=300)  # 5 minutes\n\n# Use SSH for private repos\nresult = analyzer.analyze_repository(\"git@github.com:org/private-repo.git\")\n</code></pre>"},{"location":"user-guide/url-analysis/#large-repositories","title":"Large Repositories","text":"<pre><code># Limit commits for large repos\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/kubernetes/kubernetes\",\n    max_commits=500  # Limit for faster analysis\n)\n</code></pre>"},{"location":"user-guide/url-analysis/#disk-space","title":"Disk Space","text":"<pre><code># Always cleanup\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\n\n# Or manual cleanup\nimport shutil\nshutil.rmtree(\"/tmp/greenmining_repos\")\n</code></pre>"},{"location":"user-guide/url-analysis/#next-steps","title":"Next Steps","text":"<ul> <li>Energy Measurement - Measure energy during analysis</li> <li>Python API - Full API reference</li> <li>Configuration - All settings</li> </ul>"}]}