{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GreenMining Documentation","text":"<p> An empirical Python library for Mining Software Repositories (MSR) in Green IT research </p> <p> </p>"},{"location":"#what-is-greenmining","title":"What is GreenMining?","text":"<p>GreenMining is an empirical research tool for Mining Software Repositories (MSR) focused on Green IT and sustainable software practices. It provides researchers and practitioners with a comprehensive toolkit for:</p> <ul> <li>Mining repositories at scale to study green software evolution</li> <li>Classifying commits using the Green Software Foundation (GSF) pattern catalog</li> <li>Measuring energy consumption of software systems and analysis workloads</li> <li>Analyzing temporal trends and adoption patterns across projects</li> <li>Generating research-ready datasets with statistical analysis</li> </ul> <p>The library performs deep repository analysis with full commit extraction and supports multiple energy measurement backends (RAPL, CodeCarbon) for empirical Green IT research.</p>"},{"location":"#key-capabilities","title":"Key Capabilities","text":"Feature Description 122 GSF Patterns Detect patterns across 15 categories (cloud, web, AI, caching, etc.) 321 Green Keywords Comprehensive keyword matching for green-aware commits GitHub Mining Fetch repositories by keywords, stars, language filters URL Analysis Analyze any GitHub repo directly via URL with full commit extraction Statistical Analysis Pattern correlations, temporal trends, effect sizes Energy Measurement RAPL and CodeCarbon backends for power profiling Multiple Outputs JSON, CSV, and Markdown reports"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install greenmining\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Python API:</p> <pre><code>from greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\n\n# Check if a commit message is green-aware\nmessage = \"Optimize Redis caching to reduce energy consumption\"\nprint(is_green_aware(message))  # True\n\n# Get matched patterns\npatterns = get_pattern_by_keywords(message)\nprint(patterns)  # ['Cache Static Data']\n</code></pre> <p>Analyze a Repository:</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\"https://github.com/pallets/flask\")\n\nprint(f\"Green-aware: {result['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"#pattern-categories","title":"Pattern Categories","text":"<p>GreenMining detects 122 patterns across 15 categories:</p> Category Patterns Examples Cloud 40+ Caching, compression, auto-scaling, serverless Web 15+ Lazy loading, image optimization, minification AI/ML 10+ Model optimization, quantization, efficient training Caching 8 Redis, CDN, static data caching Async 6 Batch processing, queue-based architecture Database 8 Query optimization, connection pooling Network 6 Compression, CDN, efficient protocols Resource 5 Memory management, CPU optimization Code 4 Dead code removal, algorithm efficiency Infrastructure 4 Container optimization, IaC Microservices 4 Service decomposition, graceful shutdown Monitoring 3 Efficient logging, metrics collection General 8 Feature flags, incremental processing"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Install GreenMining via pip, source, or Docker</li> <li>Quick Start - Run your first analysis in 5 minutes</li> <li>Configuration - Configure via environment variables or config files</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Python API - Use GreenMining programmatically</li> <li>URL Analysis - Analyze repositories directly by URL</li> <li>Energy Measurement - Measure energy consumption with RAPL/CodeCarbon</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>GSF Patterns - All 122 patterns with categories and keywords</li> <li>Configuration Options - All configuration parameters</li> <li>Data Models - Repository, Commit, and AnalysisResult models</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Complete Pipeline - Full experiment workflow example</li> </ul>"},{"location":"#project","title":"Project","text":"<ul> <li>Contributing - How to contribute to GreenMining</li> <li>Changelog - Version history and release notes</li> </ul>"},{"location":"#example-output","title":"Example Output","text":"<p>When analyzing a repository, GreenMining produces reports like:</p> <pre><code>============================================================\nGREENMINING ANALYSIS RESULTS\n============================================================\n\nRepository: kubernetes/kubernetes\nCommits analyzed: 1000\nGreen-aware commits: 247 (24.7%)\n\nTop Patterns Detected:\n  1. Cache Static Data (89 commits)\n  2. Use Async Instead of Sync (67 commits)\n  3. Containerize Workload (45 commits)\n  4. Compress Transmitted Data (31 commits)\n\nCategories Distribution:\n  cloud: 45.2%\n  caching: 23.1%\n  async: 18.5%\n  infrastructure: 13.2%\n</code></pre>"},{"location":"#research-applications","title":"Research Applications","text":"<p>GreenMining is designed for empirical MSR research in Green IT:</p>"},{"location":"#mining-software-repositories-msr","title":"Mining Software Repositories (MSR)","text":"<ul> <li>Large-scale repository mining with GitHub API and GraphQL</li> <li>Configurable filters (stars, languages, dates, keywords)</li> <li>Batch processing with rate limit handling</li> </ul>"},{"location":"#green-it-pattern-analysis","title":"Green IT Pattern Analysis","text":"<ul> <li>122 GSF patterns across 15 sustainability categories</li> <li>Keyword-based commit classification with confidence scoring</li> <li>Pattern co-occurrence and correlation analysis</li> </ul>"},{"location":"#temporal-statistical-analysis","title":"Temporal &amp; Statistical Analysis","text":"<ul> <li>Trend analysis at configurable granularity (day/week/month/quarter/year)</li> <li>Effect size calculations (Cohen's d, Cliff's delta)</li> <li>Cross-repository comparative studies</li> </ul>"},{"location":"#energy-measurement","title":"Energy Measurement","text":"<ul> <li>RAPL backend for direct CPU/DRAM power measurement (Linux)</li> <li>CodeCarbon integration for cross-platform emissions tracking</li> <li>Energy profiling of analysis workloads</li> </ul>"},{"location":"#research-outputs","title":"Research Outputs","text":"<ul> <li>JSON, CSV, and Markdown report generation</li> <li>Publication-ready statistical summaries</li> <li>Reproducible analysis pipelines</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use GreenMining in your research, please cite:</p> <pre><code>@software{greenmining2024,\n  author = {Bouafia, Adam},\n  title = {GreenMining: Mining Green Software Patterns},\n  year = {2024},\n  url = {https://github.com/adam-bouafia/greenmining}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>GreenMining is released under the MIT License.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to GreenMining are documented here.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#113-2026-01-30","title":"[1.1.3] - 2026-01-30","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>GraphQL <code>_parse_repository</code> now maps correctly to Repository dataclass fields (fixes <code>languages</code> keyword error)</li> <li>Removed all emojis from library output</li> <li>Removed marketing comments from codebase</li> <li>GraphQL search skips language filter when &gt;5 languages to avoid query complexity limits</li> <li>Cleaned up deadcode blocks in repository_controller.py and github_fetcher.py</li> </ul>"},{"location":"changelog/#108-2026-01-30","title":"[1.0.8] - 2026-01-30","text":""},{"location":"changelog/#enhanced-repository-support-phase-1","title":"Enhanced Repository Support (Phase 1)","text":"<ul> <li>Batch URL Analysis API: Analyze multiple repositories with configurable parallel workers</li> <li><code>analyze_repositories(urls, parallel_workers=4)</code> top-level function</li> <li><code>LocalRepoAnalyzer.analyze_repositories(urls, parallel_workers)</code> method</li> <li>Thread pool-based parallelism for concurrent repository processing</li> <li>Private Repository Support: Authenticate with SSH keys or GitHub tokens</li> <li><code>LocalRepoAnalyzer(ssh_key_path=\"~/.ssh/id_rsa\")</code> for SSH auth</li> <li><code>LocalRepoAnalyzer(github_token=\"ghp_xxx\")</code> for HTTPS private repo access</li> <li>Transparent URL token injection for private repository cloning</li> </ul>"},{"location":"changelog/#extended-energy-measurement-phase-2","title":"Extended Energy Measurement (Phase 2)","text":"<ul> <li>CPU Energy Meter Backend: Cross-platform energy estimation</li> <li>Utilization-based power modeling using CPU TDP</li> <li>Auto-detects platform (Linux, macOS, Windows)</li> <li>Supports psutil for accurate CPU utilization tracking</li> <li><code>get_energy_meter(\"auto\")</code> selects optimal backend</li> <li>Integrated Energy Tracking: Automatic energy measurement during analysis</li> <li><code>LocalRepoAnalyzer(energy_tracking=True, energy_backend=\"rapl\")</code></li> <li>Energy metrics included in <code>RepositoryAnalysis.energy_metrics</code></li> <li>Carbon Footprint Reporting: CO2 emissions calculation from energy data</li> <li><code>CarbonReporter</code> with 20+ country carbon intensity profiles</li> <li>Cloud provider region support (AWS, GCP, Azure)</li> <li>Equivalence calculations (tree-months, smartphone charges, km driven)</li> </ul>"},{"location":"changelog/#full-process-metrics-integration-phase-3","title":"Full Process Metrics Integration (Phase 3)","text":"<ul> <li>Complete Process Metrics: All 8 process metrics (ChangeSet, CodeChurn, CommitsCount, ContributorsCount, ContributorsExperience, HistoryComplexity, HunksCount, LinesCount)</li> <li>Method-Level Analysis: Per-method metrics via Lizard integration</li> <li><code>LocalRepoAnalyzer(method_level_analysis=True)</code></li> <li>Extracts name, complexity, NLOC, token count, parameters per method</li> <li>Source Code Access: Before/after source code for refactoring detection</li> <li><code>LocalRepoAnalyzer(include_source_code=True)</code></li> <li>Full diff, change type, added/deleted lines per file</li> </ul>"},{"location":"changelog/#green-msr-techniques-phase-4","title":"Green MSR Techniques (Phase 4)","text":"<ul> <li>Power Regression Detection: Identify commits that increased power consumption</li> <li><code>PowerRegressionDetector</code> with configurable threshold and test commands</li> <li>Automated bisect-style regression detection across commit ranges</li> <li>Metrics-to-Power Correlation: Correlate code metrics with power consumption</li> <li><code>MetricsPowerCorrelator</code> with Pearson and Spearman correlation analysis</li> <li>Feature importance scoring and significance testing</li> <li>Version-by-Version Power Analysis: Compare power across software versions</li> <li><code>VersionPowerAnalyzer</code> with multi-iteration measurement and warmup</li> <li>Trend detection (increasing/decreasing/stable) with statistical reporting</li> </ul>"},{"location":"changelog/#web-dashboard-phase-5","title":"Web Dashboard (Phase 5)","text":"<ul> <li>Flask-based Dashboard: Interactive visualization of analysis results</li> <li>Repository listing, summary statistics, green rate metrics</li> <li>REST API endpoints for programmatic access</li> <li><code>pip install greenmining[dashboard]</code> for dashboard support</li> </ul>"},{"location":"changelog/#dependencies","title":"Dependencies","text":"<ul> <li>Added optional <code>psutil</code> for cross-platform CPU energy measurement</li> <li>Added optional <code>flask</code> for web dashboard</li> <li>Added optional <code>codecarbon</code> for carbon tracking</li> </ul>"},{"location":"changelog/#104-2026-01-29","title":"[1.0.4] - 2026-01-29","text":""},{"location":"changelog/#major-performance-improvements","title":"\ud83d\ude80 Major Performance Improvements","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>GraphQL API Migration (5-10x faster repository fetching)</li> <li>Replaced REST API with GitHub GraphQL API v4</li> <li>Fetch 100 repos in 15 seconds (vs 2 minutes with REST)</li> <li>Single request for repos + commits (vs 100+ REST calls)</li> <li>Better rate limit efficiency: 5000 points/hour vs 5000 requests/hour</li> <li>New <code>GitHubGraphQLFetcher</code> class replaces <code>GitHubFetcher</code></li> <li> <p>Old REST implementation deadcoded for reference</p> </li> <li> <p>Enhanced Code Pattern Detection (13+ pattern types)</p> </li> <li>Expanded from 5 to 15 code-level detection patterns</li> <li>NEW patterns: Serverless computing, CDN/edge, Compression, ML optimization,     HTTP/2, gRPC, Container optimization, Green cloud regions, Auto-scaling,     Code splitting, Green ML training</li> <li>More accurate detection in code diffs</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>CLI Interface - Library is now Python API-only</li> <li>Removed <code>cli.py</code> and <code>main.py</code></li> <li>Removed CLI entry points</li> <li>Use Python API instead (simpler for researchers and developers)</li> </ul>"},{"location":"changelog/#performance-metrics","title":"Performance Metrics","text":"Metric Before (REST) After (GraphQL) Improvement Fetch 100 repos 2 minutes 15 seconds 8x faster API requests 100+ 1-2 50x fewer Rate limit cost ~100 requests ~2 points 50x better Pattern detection 5 types 15 types 3x more"},{"location":"changelog/#migration-from-103","title":"Migration from 1.0.3","text":"<p>No code changes needed! The GraphQL migration is transparent:</p> <pre><code># This still works exactly the same - just 10x faster now!\nfrom greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=\"token\",\n    max_repos=100\n)\n# Now uses GraphQL internally (10x faster)\n</code></pre> <p>If you were using CLI: - CLI has been removed - Use the Python API instead - See updated documentation for API examples</p> <p>New features available: <pre><code># Use GraphQL directly for more control\nfrom greenmining.services.github_graphql_fetcher import GitHubGraphQLFetcher\n\nfetcher = GitHubGraphQLFetcher(token=\"token\")\nrepos = fetcher.search_repositories(keywords=\"kubernetes\", max_repos=100)\n</code></pre></p>"},{"location":"changelog/#020-2024-xx-xx","title":"[0.2.0] - 2024-XX-XX","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>URL-based Analysis: Analyze repositories directly by URL with full commit extraction</li> <li><code>greenmining analyze &lt;url&gt;</code> command (unified single/batch support)</li> <li><code>LocalRepoAnalyzer</code> service for Python API</li> <li>Energy Measurement: Track energy consumption during analysis</li> <li>RAPL backend for Linux (Intel CPUs)</li> <li>CodeCarbon backend for cross-platform carbon tracking</li> <li><code>--energy</code> and <code>--energy-backend</code> CLI options</li> <li>Commit Extraction: Rich commit metrics and process metrics</li> <li>Delta Maintainability Model (DMM) metrics</li> <li>Process metrics (code churn, change set, contributor count)</li> <li>Structural metrics (complexity, lines of code)</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Renamed \"Enhanced\" terminology to standard naming throughout codebase</li> <li>Simplified analyzer architecture</li> <li>Consolidated CLI: unified <code>analyze</code> command handles both local data and URL analysis</li> <li>Previous: <code>analyze</code>, <code>analyze-url</code>, <code>analyze-urls</code> (3 separate commands)</li> <li>Now: <code>analyze [SOURCES...]</code> - auto-detects URLs vs local mode</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>NLP-based semantic analysis (simplified to keyword matching)</li> <li>ML-based classification (not needed for pattern detection)</li> <li>External ML model dependencies</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Pattern detection accuracy improvements</li> <li>Configuration loading from .env files</li> <li>Report generation formatting</li> </ul>"},{"location":"changelog/#010-2024-xx-xx","title":"[0.1.0] - 2024-XX-XX","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial release</li> <li>GSF pattern detection (122 patterns, 15 categories)</li> <li>GitHub repository fetching</li> <li>Commit extraction and analysis</li> <li>Statistical analysis with effect sizes</li> <li>Temporal trend analysis</li> <li>Docker support</li> <li>Configuration via environment variables</li> </ul>"},{"location":"changelog/#dependencies_1","title":"Dependencies","text":"<ul> <li>Python 3.9+</li> <li>Built-in repository mining and commit extraction</li> <li>python-dotenv for configuration</li> </ul>"},{"location":"changelog/#migration-guide","title":"Migration Guide","text":""},{"location":"changelog/#from-01x-to-10x","title":"From 0.1.x to 1.0.x","text":""},{"location":"changelog/#api-changes","title":"API Changes","text":"<ol> <li> <p>Import paths unchanged:    <pre><code># Still works\nfrom greenmining import is_green_aware, GSF_PATTERNS\n</code></pre></p> </li> <li> <p>New services:    <pre><code># New in 0.2.0+\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom greenmining.energy.base import EnergyMeasurer\n</code></pre></p> </li> <li> <p>Removed NLP/ML features:    <pre><code># These no longer exist - remove from your code\n# from greenmining.analyzers import SemanticAnalyzer  # Removed\n# from greenmining.analyzers import MLClassifier  # Removed\n</code></pre></p> </li> </ol>"},{"location":"changelog/#from-103-to-104-cli-removal","title":"From 1.0.3 to 1.0.4 (CLI Removal)","text":"<p>CLI has been removed in 1.0.4. Use the Python API:</p> <pre><code># Old CLI: greenmining analyze https://github.com/org/repo\n# New Python API:\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\"https://github.com/org/repo\")\n</code></pre>"},{"location":"changelog/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>Documentation</li> <li>PyPI Package</li> </ul>"},{"location":"contributing/","title":"Contributing to GreenMining","text":"<p>Thank you for your interest in contributing to GreenMining! </p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code># Fork on GitHub, then:\ngit clone https://github.com/YOUR-USERNAME/greenmining.git\ncd greenmining\n</code></pre>"},{"location":"contributing/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<pre><code># Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\n# or: venv\\Scripts\\activate  # Windows\n\n# Install in development mode\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\npytest\n\n# With coverage\npytest --cov=greenmining --cov-report=html\n\n# Specific test file\npytest tests/test_gsf_patterns.py -v\n</code></pre>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We follow PEP 8 with these tools:</p> <pre><code># Format code\nblack greenmining/ tests/\n\n# Check types\nmypy greenmining/\n\n# Lint\nruff check greenmining/\n</code></pre>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Configuration (<code>.pre-commit-config.yaml</code>):</p> <pre><code>repos:\n  - repo: https://github.com/psf/black\n    rev: 23.12.1\n    hooks:\n      - id: black\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.1.9\n    hooks:\n      - id: ruff\n</code></pre>"},{"location":"contributing/#contribution-types","title":"Contribution Types","text":""},{"location":"contributing/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<ol> <li>Search existing issues first</li> <li>Create a new issue with:</li> <li>GreenMining version (<code>pip show greenmining</code>)</li> <li>Python version</li> <li>Operating system</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Error messages/tracebacks</li> </ol>"},{"location":"contributing/#feature-requests","title":"\u2728 Feature Requests","text":"<ol> <li>Check existing issues and roadmap</li> <li>Create an issue describing:</li> <li>Use case</li> <li>Proposed solution</li> <li>Alternatives considered</li> </ol>"},{"location":"contributing/#documentation","title":"\ud83d\udcdd Documentation","text":"<ul> <li>Fix typos and clarify explanations</li> <li>Add examples</li> <li>Improve docstrings</li> <li>Update README</li> </ul>"},{"location":"contributing/#code-contributions","title":"\ud83d\udd27 Code Contributions","text":"<ol> <li>Open an issue to discuss major changes</li> <li>Fork and create a feature branch</li> <li>Write tests for new functionality</li> <li>Ensure all tests pass</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#adding-new-gsf-patterns","title":"Adding New GSF Patterns","text":""},{"location":"contributing/#1-update-pattern-definition","title":"1. Update Pattern Definition","text":"<p>Edit <code>greenmining/gsf_patterns.py</code>:</p> <pre><code>GSF_PATTERNS = {\n    # ... existing patterns ...\n\n    \"new_pattern_key\": {\n        \"name\": \"New Pattern Name\",\n        \"category\": \"category_name\",  # cloud, web, ai, caching, etc.\n        \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n        \"description\": \"Brief description of the pattern\",\n        \"sci_impact\": \"How this pattern reduces software carbon intensity\"\n    },\n}\n</code></pre>"},{"location":"contributing/#2-add-keywords-to-green_keywords","title":"2. Add Keywords to GREEN_KEYWORDS","text":"<pre><code>GREEN_KEYWORDS = [\n    # ... existing keywords ...\n    \"keyword1\",\n    \"keyword2\", \n    \"keyword3\",\n]\n</code></pre>"},{"location":"contributing/#3-write-tests","title":"3. Write Tests","text":"<pre><code># tests/test_gsf_patterns.py\n\ndef test_new_pattern_detection():\n    \"\"\"Test that new pattern is detected correctly.\"\"\"\n    from greenmining import is_green_aware, get_pattern_by_keywords\n\n    # Should detect\n    assert is_green_aware(\"message with keyword1\")\n    patterns = get_pattern_by_keywords(\"message with keyword1\")\n    assert \"New Pattern Name\" in patterns\n\n    # Should not detect\n    assert not is_green_aware(\"unrelated message\")\n</code></pre>"},{"location":"contributing/#4-update-documentation","title":"4. Update Documentation","text":"<p>Add pattern to <code>docs/reference/patterns.md</code></p>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>greenmining/\n\u251c\u2500\u2500 __init__.py          # Package exports\n\u251c\u2500\u2500 __main__.py          # Entry point\n\u251c\u2500\u2500 config.py            # Configuration\n\u251c\u2500\u2500 gsf_patterns.py      # Pattern definitions\n\u251c\u2500\u2500 utils.py             # Utilities\n\u251c\u2500\u2500 analyzers/           # Analysis modules\n\u2502   \u251c\u2500\u2500 qualitative_analyzer.py\n\u2502   \u251c\u2500\u2500 statistical_analyzer.py\n\u2502   \u2514\u2500\u2500 temporal_analyzer.py\n\u251c\u2500\u2500 controllers/         # Controllers\n\u2502   \u2514\u2500\u2500 repository_controller.py\n\u251c\u2500\u2500 energy/              # Energy measurement\n\u2502   \u251c\u2500\u2500 base.py\n\u2502   \u251c\u2500\u2500 rapl_backend.py\n\u2502   \u2514\u2500\u2500 codecarbon_backend.py\n\u251c\u2500\u2500 models/              # Data models\n\u251c\u2500\u2500 presenters/          # Output formatting\n\u2502   \u2514\u2500\u2500 console_presenter.py\n\u2514\u2500\u2500 services/            # Core services\n    \u251c\u2500\u2500 commit_extractor.py\n    \u251c\u2500\u2500 data_aggregator.py\n    \u251c\u2500\u2500 data_analyzer.py\n    \u251c\u2500\u2500 github_fetcher.py\n    \u251c\u2500\u2500 local_repo_analyzer.py\n    \u2514\u2500\u2500 reports.py\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#1-create-branch","title":"1. Create Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/issue-description\n</code></pre>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write clean, documented code</li> <li>Add tests for new functionality</li> <li>Update documentation if needed</li> </ul>"},{"location":"contributing/#3-commit-with-clear-messages","title":"3. Commit with Clear Messages","text":"<pre><code>git add .\ngit commit -m \"feat: add new pattern detection for X\"\n# or\ngit commit -m \"fix: resolve issue with Y\"\n# or\ngit commit -m \"docs: update installation instructions\"\n</code></pre> <p>Follow Conventional Commits: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation - <code>test:</code> - Tests - <code>refactor:</code> - Code refactoring - <code>chore:</code> - Maintenance</p>"},{"location":"contributing/#4-push-and-create-pr","title":"4. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub with: - Clear title - Description of changes - Link to related issue - Screenshots if UI changes</p>"},{"location":"contributing/#5-address-review-feedback","title":"5. Address Review Feedback","text":"<p>Respond to reviewer comments and make requested changes.</p>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#test-structure","title":"Test Structure","text":"<pre><code># tests/test_module.py\n\nimport pytest\nfrom greenmining import function_to_test\n\nclass TestFunctionName:\n    \"\"\"Tests for function_to_test.\"\"\"\n\n    def test_basic_functionality(self):\n        \"\"\"Test basic happy path.\"\"\"\n        result = function_to_test(\"input\")\n        assert result == \"expected\"\n\n    def test_edge_case(self):\n        \"\"\"Test edge case handling.\"\"\"\n        result = function_to_test(\"\")\n        assert result is None\n\n    def test_error_handling(self):\n        \"\"\"Test error conditions.\"\"\"\n        with pytest.raises(ValueError):\n            function_to_test(None)\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest\n\n# Verbose output\npytest -v\n\n# Specific test\npytest tests/test_gsf_patterns.py::test_pattern_detection -v\n\n# With coverage\npytest --cov=greenmining --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help newcomers</li> <li>Acknowledge contributions</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b Issue Tracker</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"examples/experiment/","title":"Experiment: Unified Repository Analysis Pipeline","text":"<p>This page describes the reference experiment included with GreenMining. The Jupyter notebook (<code>experiment/beta/experiment.ipynb</code>) demonstrates every feature of the library in a single, end-to-end pipeline applied to 13 repositories.</p>"},{"location":"examples/experiment/#overview","title":"Overview","text":"<p>The experiment follows a four-part structure:</p> Part Purpose Library Features Used A: Setup Install, import, configure <code>greenmining</code> package, <code>GSF_PATTERNS</code>, <code>GREEN_KEYWORDS</code> B: Data Gathering Fetch and analyze repositories <code>fetch_repositories</code>, <code>analyze_repositories</code> C: Unified Analysis Apply every analyzer to the combined dataset All analyzers, energy, visualization, export D: Summary Feature coverage table and output files --"},{"location":"examples/experiment/#part-a-setup","title":"Part A: Setup","text":""},{"location":"examples/experiment/#step-1-install","title":"Step 1 -- Install","text":"<pre><code>!pip install greenmining[energy,dashboard] --upgrade --quiet\n</code></pre> <p>The <code>[energy,dashboard]</code> extras install optional dependencies for energy measurement (CodeCarbon) and the Flask web dashboard.</p>"},{"location":"examples/experiment/#step-2-imports","title":"Step 2 -- Imports","text":"<p>All modules are imported from the library:</p> <pre><code>from greenmining import (\n    fetch_repositories,\n    analyze_repositories,\n    GSF_PATTERNS,\n    GREEN_KEYWORDS,\n    is_green_aware,\n    get_pattern_by_keywords,\n)\nfrom greenmining.analyzers import (\n    StatisticalAnalyzer,\n    TemporalAnalyzer,\n    CodeDiffAnalyzer,\n    PowerRegressionDetector,\n    MetricsPowerCorrelator,\n    VersionPowerAnalyzer,\n)\nfrom greenmining.energy import get_energy_meter, CPUEnergyMeter\nfrom greenmining.dashboard import create_app\nimport tracemalloc\n</code></pre>"},{"location":"examples/experiment/#step-3-configuration","title":"Step 3 -- Configuration","text":"<p>Shared parameters used across the entire pipeline:</p> Parameter Value Description <code>MAX_COMMITS</code> 20 Commits analyzed per repository <code>MIN_STARS</code> 3 Minimum GitHub stars for search <code>PARALLEL_WORKERS</code> 2 Concurrent repository analysis <code>LANGUAGES</code> 20 Top programming languages <code>CREATED_AFTER</code> <code>2020-01-01</code> Repository creation lower bound <code>CREATED_BEFORE</code> <code>2023-12-31</code> Repository creation upper bound <code>PUSHED_AFTER</code> <code>2023-01-01</code> Recent activity filter <code>COMMIT_DATE_FROM</code> <code>2023-01-01</code> Commit analysis start date <code>COMMIT_DATE_TO</code> <code>2025-12-31</code> Commit analysis end date <p>The GitHub token is loaded from the environment or a <code>.env</code> file.</p>"},{"location":"examples/experiment/#part-b-data-gathering","title":"Part B: Data Gathering","text":""},{"location":"examples/experiment/#step-4-search-blockchain-repositories","title":"Step 4 -- Search Blockchain Repositories","text":"<p>Uses <code>fetch_repositories()</code> with the GraphQL API v4 to find 10 blockchain repositories:</p> <pre><code>search_repos = fetch_repositories(\n    github_token=GITHUB_TOKEN,\n    max_repos=10,\n    min_stars=MIN_STARS,\n    languages=LANGUAGES,\n    keywords=\"blockchain\",\n    created_after=CREATED_AFTER,\n    created_before=CREATED_BEFORE,\n    pushed_after=PUSHED_AFTER,\n)\n</code></pre> <p>Library feature: <code>greenmining.fetch_repositories</code> wraps <code>GitHubGraphQLFetcher.search_repositories</code>. When more than 5 languages are provided, the language filter is skipped to stay within GitHub query complexity limits. Date filters (<code>created_after</code>, <code>created_before</code>, <code>pushed_after</code>) narrow the search to repositories within specific time windows.</p>"},{"location":"examples/experiment/#step-5-analyze-all-13-repositories","title":"Step 5 -- Analyze All 13 Repositories","text":"<p>Combines the 10 searched repositories with 3 manually selected ones (Flask, Requests, FastAPI), then runs the full analysis pipeline on all of them at once:</p> <pre><code>raw_results = analyze_repositories(\n    urls=all_urls,\n    max_commits=MAX_COMMITS,\n    parallel_workers=PARALLEL_WORKERS,\n    output_format=\"dict\",\n    energy_tracking=True,\n    energy_backend=\"auto\",\n    method_level_analysis=True,\n    include_source_code=True,\n    github_token=GITHUB_TOKEN,\n    since_date=COMMIT_DATE_FROM,\n    to_date=COMMIT_DATE_TO,\n)\nresults = [r.to_dict() for r in raw_results]\n</code></pre> <p><code>analyze_repositories()</code> returns a list of <code>RepositoryAnalysis</code> dataclass objects. These are converted to dictionaries with <code>.to_dict()</code> for downstream processing.</p> <p>Library features applied per repository:</p> <ul> <li>GSF pattern detection (122 patterns, 15 categories, 321 keywords)</li> <li>Process metrics (DMM unit size, complexity, interfacing)</li> <li>Method-level analysis (per-function complexity via Lizard)</li> <li>Source code capture (before/after for each modified file)</li> <li>Energy measurement (auto-detected backend)</li> <li>Date-bounded commit analysis (<code>since_date</code> / <code>to_date</code>)</li> </ul>"},{"location":"examples/experiment/#step-6-results-overview","title":"Step 6 -- Results Overview","text":"<p>Prints a summary table and builds a flat <code>all_commits</code> list used by every subsequent analysis step.</p>"},{"location":"examples/experiment/#part-c-unified-analysis","title":"Part C: Unified Analysis","text":"<p>Every analyzer operates on the combined dataset of all 13 repositories.</p>"},{"location":"examples/experiment/#step-7-gsf-pattern-analysis","title":"Step 7 -- GSF Pattern Analysis","text":"<p>Counts pattern frequency across all commits and groups patterns by category.</p> <p>Library features: <code>GSF_PATTERNS</code> dictionary (122 patterns), category grouping.</p>"},{"location":"examples/experiment/#step-8-green-awareness-detection","title":"Step 8 -- Green Awareness Detection","text":"<p>Demonstrates <code>is_green_aware()</code> on sample commit messages and <code>get_pattern_by_keywords()</code> for pattern lookup.</p> <p>Library features: <code>greenmining.is_green_aware</code>, <code>greenmining.get_pattern_by_keywords</code>.</p>"},{"location":"examples/experiment/#step-9-process-metrics","title":"Step 9 -- Process Metrics","text":"<p>Inspects DMM scores, structural complexity, and method-level data collected during analysis.</p> <p>Library features: Process metrics from <code>analyze_repositories</code> with <code>method_level_analysis=True</code>.</p> Metric Description <code>dmm_unit_size</code> Delta Maintainability Model -- size (0-1) <code>dmm_unit_complexity</code> Delta Maintainability Model -- complexity (0-1) <code>dmm_unit_interfacing</code> Delta Maintainability Model -- interfacing (0-1) <code>total_nloc</code> Total non-comment lines of code <code>total_complexity</code> Cyclomatic complexity sum <code>max_complexity</code> Highest single-function complexity <code>methods_count</code> Number of methods analyzed"},{"location":"examples/experiment/#step-10-statistical-analysis","title":"Step 10 -- Statistical Analysis","text":"<p>Applies three statistical methods to the combined dataset:</p> <pre><code>stat_analyzer = StatisticalAnalyzer()\nstat_analyzer.analyze_pattern_correlations(commits_df)\nstat_analyzer.temporal_trend_analysis(commits_df)\nstat_analyzer.effect_size_analysis(green_cx, non_green_cx)\n</code></pre> <p>Library feature: <code>greenmining.analyzers.StatisticalAnalyzer</code></p> Method Output <code>analyze_pattern_correlations()</code> Significant co-occurrence pairs <code>temporal_trend_analysis()</code> Trend direction, significance, correlation <code>effect_size_analysis()</code> Cohen's d, magnitude, mean difference"},{"location":"examples/experiment/#step-11-temporal-analysis","title":"Step 11 -- Temporal Analysis","text":"<p>Groups commits by quarter and tracks green awareness evolution over time:</p> <pre><code>temporal = TemporalAnalyzer(granularity=\"quarter\")\ntemporal_results = temporal.analyze_trends(all_commits, analysis_results_fmt)\n</code></pre> <p>Library feature: <code>greenmining.analyzers.TemporalAnalyzer</code></p> <p>Output includes period-by-period green commit rates, unique pattern counts, overall trend direction, and peak period identification.</p>"},{"location":"examples/experiment/#step-12-code-diff-pattern-signatures","title":"Step 12 -- Code Diff Pattern Signatures","text":"<p>Inspects the 15+ green pattern categories that <code>CodeDiffAnalyzer</code> detects directly in code diffs:</p> <pre><code>diff_analyzer = CodeDiffAnalyzer()\nprint(diff_analyzer.PATTERN_SIGNATURES)\n</code></pre> <p>Library feature: <code>greenmining.analyzers.CodeDiffAnalyzer</code></p> <p>Pattern signatures include caching, lazy loading, compression, connection pooling, batch processing, async I/O, and more.</p>"},{"location":"examples/experiment/#step-13-energy-measurement","title":"Step 13 -- Energy Measurement","text":"<p>Demonstrates all four energy measurement approaches:</p> <p>RAPL (Running Average Power Limit):</p> <pre><code>from greenmining.energy import RAPLEnergyMeter\nrapl = RAPLEnergyMeter()\nresult, energy = rapl.measure(workload_function)\n</code></pre> <p>CPU Meter (universal fallback):</p> <pre><code>meter = CPUEnergyMeter()\nresult, energy = meter.measure(workload_function)\n</code></pre> <p>Memory Profiling (tracemalloc):</p> <pre><code>tracemalloc.start()\n# ... workload ...\ncurrent, peak = tracemalloc.get_traced_memory()\ntracemalloc.stop()\n</code></pre> <p>CO2 Emissions (CodeCarbon):</p> <pre><code>from codecarbon import EmissionsTracker\ntracker = EmissionsTracker(project_name=\"greenmining-experiment\")\ntracker.start()\n# ... workload ...\nemissions = tracker.stop()\n</code></pre> Backend Platform What It Measures RAPL Linux (Intel/AMD) Hardware energy counters (Joules) CPU Meter Universal CPU utilization + TDP estimate (Joules) tracemalloc Universal Peak memory allocation (bytes) CodeCarbon Cross-platform CO2 emissions (kg CO2eq)"},{"location":"examples/experiment/#step-14-power-regression-detection","title":"Step 14 -- Power Regression Detection","text":"<p>Identifies commits that caused energy consumption increases:</p> <pre><code>detector = PowerRegressionDetector(\n    test_command=\"python -c 'sum(range(100000))'\",\n    energy_backend=\"cpu_meter\",\n    threshold_percent=5.0,\n    iterations=3,\n)\nregressions = detector.detect(\n    repo_path=\"./my-repo\",\n    baseline_commit=\"HEAD~10\",\n    target_commit=\"HEAD\",\n)\n</code></pre> <p>Library feature: <code>greenmining.analyzers.PowerRegressionDetector</code></p> <p>Runs the test command at each commit, measures energy, and flags regressions above the threshold.</p>"},{"location":"examples/experiment/#step-15-metrics-to-power-correlation","title":"Step 15 -- Metrics-to-Power Correlation","text":"<p>Correlates code metrics (complexity, NLOC, churn) with energy consumption:</p> <pre><code>correlator = MetricsPowerCorrelator(significance_level=0.05)\ncorrelator.fit(metric_names, metrics_values, power_measurements)\nsummary = correlator.summary()\n</code></pre> <p>Library feature: <code>greenmining.analyzers.MetricsPowerCorrelator</code></p> <p>Computes Pearson and Spearman coefficients with significance testing, plus feature importance ranking across all metrics.</p>"},{"location":"examples/experiment/#step-16-version-power-analysis","title":"Step 16 -- Version Power Analysis","text":"<p>Compares energy consumption across software versions:</p> <pre><code>version_analyzer = VersionPowerAnalyzer(\n    test_command=\"python -c 'sum(range(100000))'\",\n    energy_backend=\"cpu_meter\",\n    iterations=5,\n)\nreport = version_analyzer.analyze_versions(\n    repo_path=\"./my-repo\",\n    versions=[\"v1.0\", \"v2.0\", \"v3.0\"],\n)\n</code></pre> <p>Library feature: <code>greenmining.analyzers.VersionPowerAnalyzer</code></p> <p>Reports per-version power consumption, overall trend, most/least efficient versions, and total change percentage.</p>"},{"location":"examples/experiment/#step-17-visualization-matplotlib","title":"Step 17 -- Visualization (matplotlib)","text":"<p>Static 2x2 chart grid:</p> <ol> <li>Green commit rate per repository (horizontal bar)</li> <li>Top 10 GSF patterns (horizontal bar)</li> <li>Total vs green commit breakdown (grouped bar)</li> <li>Complexity distribution (histogram)</li> </ol> <p>Saved to <code>data/analysis_plots.png</code>.</p>"},{"location":"examples/experiment/#step-18-interactive-visualization-plotly","title":"Step 18 -- Interactive Visualization (Plotly)","text":"<p>Interactive charts:</p> <ul> <li>Sunburst: Repository &gt; Green/Non-Green &gt; Pattern</li> <li>Scatter: Complexity vs Lines of Code, colored by green awareness</li> </ul>"},{"location":"examples/experiment/#step-19-export-results","title":"Step 19 -- Export Results","text":"<p>Exports the combined dataset in three formats:</p> Format File Content JSON <code>data/analysis_results.json</code> Full analysis data (all fields) CSV <code>data/analysis_results.csv</code> Flattened commit-level rows DataFrame In-memory pandas DataFrame for further analysis"},{"location":"examples/experiment/#step-20-web-dashboard","title":"Step 20 -- Web Dashboard","text":"<p>Creates a Flask-based dashboard with REST API endpoints:</p> <pre><code>app = create_app(data_dir=\"./data\")\n</code></pre> <p>Library feature: <code>greenmining.dashboard.create_app</code>, <code>greenmining.dashboard.run_dashboard</code></p> Endpoint Description <code>GET /</code> Dashboard UI <code>GET /api/repositories</code> Repository data <code>GET /api/analysis</code> Analysis results <code>GET /api/statistics</code> Aggregated statistics <code>GET /api/energy</code> Energy report <code>GET /api/summary</code> Summary metrics"},{"location":"examples/experiment/#part-d-summary","title":"Part D: Summary","text":""},{"location":"examples/experiment/#feature-coverage","title":"Feature Coverage","text":"<p>The experiment applies every library feature to every repository equally:</p> Feature Module Applied GSF Pattern Detection (122 patterns, 15 categories) <code>greenmining</code> Yes Process Metrics (DMM size, complexity, interfacing) <code>greenmining</code> Yes Method-Level Analysis (per-function complexity) <code>greenmining</code> Yes Source Code Capture (before/after) <code>greenmining</code> Yes Energy Measurement (RAPL + CPU Meter) <code>greenmining.energy</code> Yes Memory Profiling (tracemalloc) <code>tracemalloc</code> Yes CO2 Emissions (CodeCarbon) <code>codecarbon</code> Yes Statistical Analysis (correlations, effect sizes) <code>greenmining.analyzers</code> Yes Temporal Analysis (quarterly trends) <code>greenmining.analyzers</code> Yes Code Diff Pattern Signatures <code>greenmining.analyzers</code> Yes Power Regression Detection <code>greenmining.analyzers</code> Demonstrated Metrics-to-Power Correlation (Pearson/Spearman) <code>greenmining.analyzers</code> Yes Version Power Comparison <code>greenmining.analyzers</code> Demonstrated Visualization (matplotlib + Plotly) External Yes Export (JSON, CSV, DataFrame) Built-in Yes Web Dashboard (Flask REST API) <code>greenmining.dashboard</code> Yes"},{"location":"examples/experiment/#output-files","title":"Output Files","text":"File Description <code>data/analysis_results.json</code> Full analysis data <code>data/analysis_results.csv</code> Flattened commit-level data <code>data/analysis_plots.png</code> Static visualizations"},{"location":"examples/experiment/#running-the-experiment","title":"Running the Experiment","text":"<pre><code># Install the library\npip install greenmining[energy,dashboard]\n\n# Set your GitHub token\nexport GITHUB_TOKEN=ghp_your_token_here\n\n# Open the notebook\njupyter notebook experiment/beta/experiment.ipynb\n</code></pre> <p>Run all cells sequentially. The data gathering steps require a valid GitHub token and internet access. Analysis steps operate on the collected data.</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>GreenMining can be configured via environment variables, <code>.env</code> files, or programmatically.</p>"},{"location":"getting-started/configuration/#configuration-methods","title":"Configuration Methods","text":""},{"location":"getting-started/configuration/#method-1-environment-variables","title":"Method 1: Environment Variables","text":"<p>Set variables directly in your shell:</p> <pre><code>export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\nexport MAX_REPOS=50\nexport MIN_STARS=500\nexport ENABLE_TEMPORAL=true\n</code></pre>"},{"location":"getting-started/configuration/#method-2-env-file","title":"Method 2: .env File","text":"<p>Create a <code>.env</code> file in your project directory:</p> <pre><code># Required\nGITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n\n# Repository Fetching\nMAX_REPOS=100\nMIN_STARS=100\nSUPPORTED_LANGUAGES=Python,Java,Go,JavaScript,TypeScript\nSEARCH_KEYWORDS=microservices\n\n# Commit Extraction\nCOMMITS_PER_REPO=1000\nEXCLUDE_MERGE_COMMITS=true\nEXCLUDE_BOT_COMMITS=true\n\n# Analysis Features\nENABLE_DIFF_ANALYSIS=false\nBATCH_SIZE=10\n\n# Temporal Analysis\nENABLE_TEMPORAL=true\nTEMPORAL_GRANULARITY=quarter\nENABLE_STATS=true\n\n# Output\nOUTPUT_DIR=./data\nREPORT_FORMAT=markdown\n</code></pre>"},{"location":"getting-started/configuration/#method-3-python-config-object","title":"Method 3: Python Config Object","text":"<pre><code>from greenmining.config import Config\n\nconfig = Config()\n\n# Override settings\nconfig.MAX_REPOS = 50\nconfig.COMMITS_PER_REPO = 200\nconfig.MIN_STARS = 500\n</code></pre>"},{"location":"getting-started/configuration/#configuration-options-reference","title":"Configuration Options Reference","text":""},{"location":"getting-started/configuration/#github-api","title":"GitHub API","text":"Option Type Default Description <code>GITHUB_TOKEN</code> string (required) GitHub personal access token"},{"location":"getting-started/configuration/#repository-fetching","title":"Repository Fetching","text":"Option Type Default Description <code>MAX_REPOS</code> int 100 Maximum repositories to fetch <code>MIN_STARS</code> int 100 Minimum GitHub stars required <code>SUPPORTED_LANGUAGES</code> list Python,Java,Go,... Language filter (comma-separated) <code>SEARCH_KEYWORDS</code> string microservices Search keywords for GitHub API"},{"location":"getting-started/configuration/#url-analysis","title":"URL Analysis","text":"Option Type Default Description <code>REPOSITORY_URLS</code> list [] List of repository URLs to analyze <code>CLONE_PATH</code> string /tmp/greenmining_repos Directory for cloning repositories <code>CLEANUP_AFTER_ANALYSIS</code> bool true Delete cloned repos after analysis"},{"location":"getting-started/configuration/#commit-extraction","title":"Commit Extraction","text":"Option Type Default Description <code>COMMITS_PER_REPO</code> int 1000 Maximum commits per repository <code>EXCLUDE_MERGE_COMMITS</code> bool true Skip merge commits <code>EXCLUDE_BOT_COMMITS</code> bool true Skip bot commits <code>MIN_MESSAGE_LENGTH</code> int 10 Minimum commit message length"},{"location":"getting-started/configuration/#analysis-features","title":"Analysis Features","text":"Option Type Default Description <code>ENABLE_DIFF_ANALYSIS</code> bool false Analyze code diffs (slower) <code>BATCH_SIZE</code> int 10 Commits per batch"},{"location":"getting-started/configuration/#process-metrics-options","title":"Process Metrics Options","text":"Option Type Default Description <code>PROCESS_METRICS_ENABLED</code> bool true Compute process metrics <code>STRUCTURAL_METRICS_ENABLED</code> bool true Compute structural metrics <code>DMM_ENABLED</code> bool true Delta Maintainability Model"},{"location":"getting-started/configuration/#statistical-analysis","title":"Statistical Analysis","text":"Option Type Default Description <code>ENABLE_STATS</code> bool false Enable statistical analysis <code>ENABLE_TEMPORAL</code> bool false Enable temporal trend analysis <code>TEMPORAL_GRANULARITY</code> string quarter day/week/month/quarter/year"},{"location":"getting-started/configuration/#energy-measurement","title":"Energy Measurement","text":"Option Type Default Description <code>ENERGY_ENABLED</code> bool false Enable energy measurement <code>ENERGY_BACKEND</code> string rapl rapl, codecarbon, or cpu_meter <code>CARBON_TRACKING</code> bool false Track carbon emissions <code>COUNTRY_ISO</code> string USA Country for carbon calculations"},{"location":"getting-started/configuration/#output-configuration","title":"Output Configuration","text":"Option Type Default Description <code>OUTPUT_DIR</code> string ./data Output directory path <code>REPOS_FILE</code> string repositories.json Repository data filename <code>COMMITS_FILE</code> string commits.json Commits data filename <code>ANALYSIS_FILE</code> string analysis_results.json Analysis results filename <code>STATS_FILE</code> string aggregated_statistics.json Statistics filename <code>REPORT_FILE</code> string green_analysis.md Report filename"},{"location":"getting-started/configuration/#yaml-configuration","title":"YAML Configuration","text":"<p>For advanced configuration, use a YAML file:</p> <p>config/config.yaml:</p> <pre><code># GreenMining Configuration\nversion: \"2.0\"\n\nsearch:\n  keywords: [\"microservices\", \"kubernetes\"]\n  languages: [\"Python\", \"Go\", \"Java\"]\n  min_stars: 100\n  max_repos: 100\n\n  temporal:\n    created_after: \"2020-01-01\"\n    created_before: \"2025-12-31\"\n    pushed_after: \"2023-01-01\"\n\ndetection:\n  methods:\n    keyword:\n      enabled: true\n      confidence_weight: 1.0\n\n    diff_analysis:\n      enabled: true\n      confidence_weight: 0.8\n\nanalysis:\n  statistical:\n    enabled: true\n    methods:\n      - chi_square\n      - correlation_analysis\n      - temporal_trends\n      - effect_sizes\n</code></pre>"},{"location":"getting-started/configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>Configuration values are loaded in this order (later overrides earlier):</p> <ol> <li>Default values in <code>Config</code> class</li> <li>YAML config file if specified</li> <li><code>.env</code> file in current directory</li> <li>Environment variables</li> <li>Python Config overrides</li> </ol>"},{"location":"getting-started/configuration/#validating-configuration","title":"Validating Configuration","text":"<p>Check current configuration in Python:</p> <pre><code>from greenmining.config import Config\n\nconfig = Config()\n\n# Check settings\nprint(f\"GITHUB_TOKEN: {'***configured***' if config.GITHUB_TOKEN else 'Not set'}\")\nprint(f\"MAX_REPOS: {config.MAX_REPOS}\")\nprint(f\"COMMITS_PER_REPO: {config.COMMITS_PER_REPO}\")\nprint(f\"OUTPUT_DIR: {config.OUTPUT_DIR}\")\nprint(f\"ENABLE_TEMPORAL: {config.ENABLE_TEMPORAL}\")\nprint(f\"TEMPORAL_GRANULARITY: {config.TEMPORAL_GRANULARITY}\")\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Get started with examples</li> <li>Python API - Programmatic configuration</li> <li>Config Options Reference - Full reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers all methods to install GreenMining.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.9 or higher</li> <li>Operating System: Linux, macOS, or Windows</li> <li>GitHub Token: Required for repository fetching (optional for URL analysis)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-pip-recommended","title":"Method 1: pip (Recommended)","text":"<p>Install from PyPI:</p> <pre><code>pip install greenmining\n</code></pre> <p>Verify installation:</p> <pre><code>python -c \"import greenmining; print(f'greenmining v{greenmining.__version__}')\"\n# Output: greenmining v1.0.4\n</code></pre>"},{"location":"getting-started/installation/#method-2-from-source","title":"Method 2: From Source","text":"<p>Clone and install for development:</p> <pre><code># Clone repository\ngit clone https://github.com/adam-bouafia/greenmining.git\ncd greenmining\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#method-3-docker","title":"Method 3: Docker","text":"<p>Run using Docker:</p> <pre><code># Pull image\ndocker pull adambouafia/greenmining:latest\n\n# Interactive Python shell\ndocker run -it -v $(pwd)/data:/app/data \\\n           adambouafia/greenmining:latest python\n\n# Run a Python script\ndocker run -v $(pwd)/data:/app/data \\\n           -e GITHUB_TOKEN=your_token \\\n           adambouafia/greenmining:latest python your_script.py\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>GreenMining automatically installs these dependencies:</p> Package Purpose <code>PyGithub&gt;=2.1.1</code> GitHub API access <code>gitpython&gt;=3.1.0</code> Git repository access and commit extraction <code>lizard&gt;=1.17.0</code> Method-level code complexity analysis <code>pandas&gt;=2.2.0</code> Data manipulation <code>scipy&gt;=1.10.0</code> Statistical analysis <code>numpy&gt;=1.24.0</code> Numerical operations <code>python-dotenv</code> Environment variable loading <code>tqdm</code> Progress bars"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For energy measurement features:</p> <pre><code># CodeCarbon support\npip install codecarbon\n\n# Full development environment\npip install greenmining[dev]\n</code></pre>"},{"location":"getting-started/installation/#github-token-setup","title":"GitHub Token Setup","text":"<p>A GitHub personal access token is required for the <code>fetch</code> and <code>pipeline</code> commands.</p>"},{"location":"getting-started/installation/#creating-a-token","title":"Creating a Token","text":"<ol> <li>Go to GitHub Settings \u2192 Developer settings \u2192 Personal access tokens</li> <li>Click \"Generate new token (classic)\"</li> <li>Select scopes:<ul> <li><code>repo</code> (for private repositories)</li> <li><code>public_repo</code> (for public repositories only)</li> </ul> </li> <li>Copy the generated token</li> </ol>"},{"location":"getting-started/installation/#configuring-the-token","title":"Configuring the Token","text":"<p>Option 1: Environment Variable</p> <pre><code>export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre> <p>Option 2: .env File</p> <p>Create a <code>.env</code> file in your project directory:</p> <pre><code>GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre> <p>Option 3: Pass to Functions</p> <p>Pass directly to API functions:</p> <pre><code>from greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=\"ghp_xxxxxxxxxxxxxxxxxxxx\",\n    max_repos=10\n)\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>Run the following to verify everything works:</p> <pre><code># Check version\nimport greenmining\nprint(f\"greenmining v{greenmining.__version__}\")\n\n# Check available exports\nfrom greenmining import GSF_PATTERNS, GREEN_KEYWORDS, is_green_aware\nprint(f\"{len(GSF_PATTERNS)} patterns loaded\")\nprint(f\"{len(GREEN_KEYWORDS)} keywords loaded\")\n\n# Test pattern detection\nprint(is_green_aware(\"Enable caching\"))  # True\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>Issue: <code>ModuleNotFoundError: No module named 'greenmining'</code></p> <p>Solution: Ensure you've activated your virtual environment and installed the package:</p> <pre><code>source venv/bin/activate\npip install greenmining\n</code></pre> <p>Issue: <code>GITHUB_TOKEN not set</code></p> <p>Solution: Set the environment variable or create a <code>.env</code> file:</p> <pre><code>export GITHUB_TOKEN=your_token_here\n</code></pre> <p>Issue: <code>Rate limit exceeded</code></p> <p>Solution: GitHub API has rate limits. Either:</p> <ul> <li>Wait for the rate limit to reset (1 hour)</li> <li>Use an authenticated token for higher limits</li> <li>Reduce <code>--max-repos</code> parameter</li> </ul> <p>Issue: <code>Permission denied</code> on RAPL energy measurement</p> <p>Solution: RAPL requires root access or specific permissions:</p> <pre><code># Grant read access to energy files\nsudo chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\n</code></pre> <p>Or use CodeCarbon instead (no root needed):</p> <pre><code>from greenmining.energy import CodeCarbonMeter\nmeter = CodeCarbonMeter()\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Run your first analysis</li> <li>Configuration - Customize GreenMining settings</li> <li>Python API - Complete API reference</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with GreenMining in 5 minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>GitHub token (for fetching repositories)</li> <li>GreenMining installed (<code>pip install greenmining</code>)</li> </ul>"},{"location":"getting-started/quickstart/#option-1-analyze-a-repository-by-url","title":"Option 1: Analyze a Repository by URL","text":"<p>Analyze any public GitHub repository directly:</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\n    \"https://github.com/pallets/flask\",\n    max_commits=100\n)\n\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Total commits: {result['total_commits']}\")\nprint(f\"Green-aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\nprint(\"\\nTop Patterns:\")\nfor pattern, count in sorted(\n    result['pattern_distribution'].items(),\n    key=lambda x: x[1],\n    reverse=True\n)[:5]:\n    print(f\"  - {pattern}: {count}\")\n</code></pre> <p>Output:</p> <pre><code>Repository: flask\nTotal commits: 100\nGreen-aware: 23 (23.0%)\n\nTop Patterns:\n  - Cache Static Data: 8\n  - Use Async Instead of Sync: 5\n  - Lazy Loading: 4\n</code></pre>"},{"location":"getting-started/quickstart/#option-2-full-pipeline-comprehensive","title":"Option 2: Full Pipeline (Comprehensive)","text":"<p>Run the complete analysis pipeline programmatically:</p> <pre><code>import os\nfrom greenmining.services import (\n    GitHubFetcher,\n    CommitExtractor,\n    DataAnalyzer,\n    DataAggregator,\n)\n\n# Set your GitHub token\ngithub_token = os.environ.get(\"GITHUB_TOKEN\")\n\n# 1. Fetch repositories\nfetcher = GitHubFetcher(token=github_token, max_repos=5, min_stars=500)\nrepos = fetcher.search_repositories()\n\n# 2. Extract commits\nextractor = CommitExtractor(max_commits=50)\ncommits = extractor.extract_from_repositories(repos)\n\n# 3. Analyze commits\nanalyzer = DataAnalyzer()\nresults = analyzer.analyze_commits(commits)\n\n# 4. Aggregate statistics\naggregator = DataAggregator(enable_stats=True, enable_temporal=True)\nstats = aggregator.aggregate(results, repos)\n\n# Print results\nprint(f\"Repositories analyzed: {len(repos)}\")\nprint(f\"Total commits: {len(commits)}\")\nprint(f\"Green-aware: {stats['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"getting-started/quickstart/#option-3-quick-pattern-detection","title":"Option 3: Quick Pattern Detection","text":"<p>Use GreenMining programmatically:</p> <pre><code>from greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\n\n# Check pattern count\nprint(f\"Loaded {len(GSF_PATTERNS)} GSF patterns\")\n# Output: Loaded 122 GSF patterns\n\n# Test green awareness detection\nmessages = [\n    \"Optimize Redis caching for better performance\",\n    \"Fix typo in README\",\n    \"Enable gzip compression for API responses\",\n    \"Update dependencies to latest versions\",\n]\n\nfor msg in messages:\n    is_green = is_green_aware(msg)\n    patterns = get_pattern_by_keywords(msg) if is_green else []\n    status = \"\ud83c\udf31\" if is_green else \"  \"\n    print(f\"{status} {msg[:50]}\")\n    if patterns:\n        print(f\"   \u2192 Patterns: {patterns}\")\n</code></pre> <p>Output:</p> <pre><code>\ud83c\udf31 Optimize Redis caching for better performance\n   \u2192 Patterns: ['Cache Static Data']\n   Fix typo in README\n\ud83c\udf31 Enable gzip compression for API responses\n   \u2192 Patterns: ['Compress Transmitted Data', 'Enable Text Compression']\n   Update dependencies to latest versions\n</code></pre>"},{"location":"getting-started/quickstart/#output-files","title":"Output Files","text":"<p>When running the pipeline, outputs are saved to the <code>data/</code> directory:</p> File Description <code>repositories.json</code> Fetched repository metadata <code>commits.json</code> Extracted commit data <code>analysis_results.json</code> Pattern detection results <code>aggregated_statistics.json</code> Summary statistics <code>aggregated_data.json</code> Full aggregated data"},{"location":"getting-started/quickstart/#quick-test-script","title":"Quick Test Script","text":"<p>Create <code>test_greenmining.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Quick test of GreenMining functionality.\"\"\"\n\nfrom greenmining import GSF_PATTERNS, GREEN_KEYWORDS, is_green_aware, get_pattern_by_keywords\n\n# Test 1: Check patterns loaded\nprint(f\"\u2713 Loaded {len(GSF_PATTERNS)} GSF patterns\")\nprint(f\"\u2713 Loaded {len(GREEN_KEYWORDS)} green keywords\")\n\n# Test 2: Get categories\ncategories = set(p[\"category\"] for p in GSF_PATTERNS.values())\nprint(f\"\u2713 Categories: {', '.join(sorted(categories))}\")\n\n# Test 3: Pattern detection\ntest_msg = \"Implement Redis caching to reduce database load\"\nis_green = is_green_aware(test_msg)\npatterns = get_pattern_by_keywords(test_msg)\n\nprint(f\"\u2713 Test message: '{test_msg}'\")\nprint(f\"  Green-aware: {is_green}\")\nprint(f\"  Patterns: {patterns}\")\n\nprint(\"\\n\u2705 All tests passed!\")\n</code></pre> <p>Run it:</p> <pre><code>python test_greenmining.py\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Customize settings</li> <li>Python API - Programmatic usage</li> <li>GSF Patterns - All 122 patterns</li> <li>URL Analysis - Analyze by URL</li> </ul>"},{"location":"reference/config-options/","title":"Configuration Options Reference","text":"<p>Complete reference for all GreenMining configuration options.</p>"},{"location":"reference/config-options/#configuration-methods","title":"Configuration Methods","text":"<p>GreenMining supports multiple configuration methods:</p> <ol> <li>Environment variables - <code>export OPTION=value</code></li> <li><code>.env</code> file - Key-value pairs in project root</li> <li>Python Config object - Programmatic configuration</li> </ol>"},{"location":"reference/config-options/#precedence-order","title":"Precedence Order","text":"<p>Later sources override earlier ones:</p> <ol> <li>Default values (lowest priority)</li> <li><code>.env</code> file</li> <li>Environment variables</li> <li>Python Config overrides (highest priority)</li> </ol>"},{"location":"reference/config-options/#github-api-options","title":"GitHub API Options","text":""},{"location":"reference/config-options/#github_token","title":"GITHUB_TOKEN","text":"<p>GitHub personal access token for API access.</p> Property Value Type string Default (none - required) Environment <code>GITHUB_TOKEN</code> Required Yes (for fetch operations) <pre><code>export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n</code></pre>"},{"location":"reference/config-options/#repository-fetching-options","title":"Repository Fetching Options","text":""},{"location":"reference/config-options/#max_repos","title":"MAX_REPOS","text":"<p>Maximum number of repositories to fetch from GitHub.</p> Property Value Type integer Default 100 Environment <code>MAX_REPOS</code> <pre><code>from greenmining import fetch_repositories\n\nrepos = fetch_repositories(github_token=token, max_repos=50)\n</code></pre>"},{"location":"reference/config-options/#min_stars","title":"MIN_STARS","text":"<p>Minimum GitHub star count required for repositories.</p> Property Value Type integer Default 100 Environment <code>MIN_STARS</code> <pre><code>repos = fetch_repositories(github_token=token, min_stars=500)\n</code></pre>"},{"location":"reference/config-options/#supported_languages","title":"SUPPORTED_LANGUAGES","text":"<p>Programming languages to filter repositories.</p> Property Value Type list (comma-separated) Default Python,Java,Go,JavaScript,TypeScript,C#,Rust Environment <code>SUPPORTED_LANGUAGES</code> <pre><code>repos = fetch_repositories(github_token=token, languages=[\"Python\", \"Go\", \"Rust\"])\n</code></pre>"},{"location":"reference/config-options/#search_keywords","title":"SEARCH_KEYWORDS","text":"<p>Keywords for GitHub repository search.</p> Property Value Type string Default microservices Environment <code>SEARCH_KEYWORDS</code> <pre><code>repos = fetch_repositories(github_token=token, keywords=\"kubernetes cloud-native\")\n</code></pre>"},{"location":"reference/config-options/#url-analysis-options","title":"URL Analysis Options","text":""},{"location":"reference/config-options/#repository_urls","title":"REPOSITORY_URLS","text":"<p>List of repository URLs to analyze directly.</p> Property Value Type list Default [] Environment <code>REPOSITORY_URLS</code> (comma-separated) <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(max_commits=100)\nresult = analyzer.analyze_repository(\"https://github.com/org/repo\")\n</code></pre>"},{"location":"reference/config-options/#clone_path","title":"CLONE_PATH","text":"<p>Directory where repositories are cloned for analysis.</p> Property Value Type string (path) Default /tmp/greenmining_repos Environment <code>CLONE_PATH</code> <pre><code>export CLONE_PATH=/var/data/greenmining\n</code></pre>"},{"location":"reference/config-options/#cleanup_after_analysis","title":"CLEANUP_AFTER_ANALYSIS","text":"<p>Whether to delete cloned repositories after analysis.</p> Property Value Type boolean Default true Environment <code>CLEANUP_AFTER_ANALYSIS</code> <pre><code>export CLEANUP_AFTER_ANALYSIS=false\n</code></pre>"},{"location":"reference/config-options/#commit-extraction-options","title":"Commit Extraction Options","text":""},{"location":"reference/config-options/#commits_per_repo","title":"COMMITS_PER_REPO","text":"<p>Maximum commits to extract per repository.</p> Property Value Type integer Default 1000 Environment <code>COMMITS_PER_REPO</code> <pre><code>from greenmining.services.commit_extractor import CommitExtractor\n\nextractor = CommitExtractor()\nextractor.max_commits = 500\n</code></pre>"},{"location":"reference/config-options/#exclude_merge_commits","title":"EXCLUDE_MERGE_COMMITS","text":"<p>Skip merge commits during extraction.</p> Property Value Type boolean Default true Environment <code>EXCLUDE_MERGE_COMMITS</code> <pre><code>export EXCLUDE_MERGE_COMMITS=false\n</code></pre>"},{"location":"reference/config-options/#exclude_bot_commits","title":"EXCLUDE_BOT_COMMITS","text":"<p>Skip commits from bot accounts.</p> Property Value Type boolean Default true Environment <code>EXCLUDE_BOT_COMMITS</code> <pre><code>export EXCLUDE_BOT_COMMITS=false\n</code></pre>"},{"location":"reference/config-options/#min_message_length","title":"MIN_MESSAGE_LENGTH","text":"<p>Minimum commit message length to include.</p> Property Value Type integer Default 10 Environment <code>MIN_MESSAGE_LENGTH</code> <pre><code>export MIN_MESSAGE_LENGTH=20\n</code></pre>"},{"location":"reference/config-options/#analysis-options","title":"Analysis Options","text":""},{"location":"reference/config-options/#enable_diff_analysis","title":"ENABLE_DIFF_ANALYSIS","text":"<p>Enable code diff analysis for pattern detection.</p> Property Value Type boolean Default false Environment <code>ENABLE_DIFF_ANALYSIS</code> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(enable_diff_analysis=True)\n</code></pre>"},{"location":"reference/config-options/#batch_size","title":"BATCH_SIZE","text":"<p>Number of commits to process per batch.</p> Property Value Type integer Default 10 Environment <code>BATCH_SIZE</code> <pre><code>from greenmining.services.data_analyzer import DataAnalyzer\n\nanalyzer = DataAnalyzer()\nanalyzer.batch_size = 50\n</code></pre>"},{"location":"reference/config-options/#process-metrics-options","title":"Process Metrics Options","text":""},{"location":"reference/config-options/#process_metrics_enabled","title":"PROCESS_METRICS_ENABLED","text":"<p>Compute process metrics (code churn, change set, etc.).</p> Property Value Type boolean Default true Environment <code>PROCESS_METRICS_ENABLED</code> <pre><code>export PROCESS_METRICS_ENABLED=true\n</code></pre>"},{"location":"reference/config-options/#structural_metrics_enabled","title":"STRUCTURAL_METRICS_ENABLED","text":"<p>Compute structural metrics (complexity, lines of code).</p> Property Value Type boolean Default true Environment <code>STRUCTURAL_METRICS_ENABLED</code> <pre><code>export STRUCTURAL_METRICS_ENABLED=true\n</code></pre>"},{"location":"reference/config-options/#dmm_enabled","title":"DMM_ENABLED","text":"<p>Enable Delta Maintainability Model metrics.</p> Property Value Type boolean Default true Environment <code>DMM_ENABLED</code> <pre><code>export DMM_ENABLED=true\n</code></pre>"},{"location":"reference/config-options/#statistical-analysis-options","title":"Statistical Analysis Options","text":""},{"location":"reference/config-options/#enable_stats","title":"ENABLE_STATS","text":"<p>Enable statistical analysis (correlations, effect sizes).</p> Property Value Type boolean Default false Environment <code>ENABLE_STATS</code> <pre><code>from greenmining.services.data_aggregator import DataAggregator\n\naggregator = DataAggregator(enable_stats=True)\n</code></pre>"},{"location":"reference/config-options/#enable_temporal","title":"ENABLE_TEMPORAL","text":"<p>Enable temporal trend analysis.</p> Property Value Type boolean Default false Environment <code>ENABLE_TEMPORAL</code> <pre><code>aggregator = DataAggregator(enable_temporal=True)\n</code></pre>"},{"location":"reference/config-options/#temporal_granularity","title":"TEMPORAL_GRANULARITY","text":"<p>Time period granularity for temporal analysis.</p> Property Value Type string Default quarter Options day, week, month, quarter, year Environment <code>TEMPORAL_GRANULARITY</code> <pre><code>aggregator = DataAggregator(temporal_granularity=\"month\")\n</code></pre>"},{"location":"reference/config-options/#energy-measurement-options","title":"Energy Measurement Options","text":""},{"location":"reference/config-options/#energy_enabled","title":"ENERGY_ENABLED","text":"<p>Enable energy measurement during analysis.</p> Property Value Type boolean Default false Environment <code>ENERGY_ENABLED</code> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    energy_enabled=True,\n    energy_backend=\"rapl\"\n)\n</code></pre>"},{"location":"reference/config-options/#energy_backend","title":"ENERGY_BACKEND","text":"<p>Energy measurement backend to use.</p> Property Value Type string Default rapl Options rapl, codecarbon Environment <code>ENERGY_BACKEND</code> <pre><code>analyzer = LocalRepoAnalyzer(\n    energy_enabled=True,\n    energy_backend=\"codecarbon\"\n)\n</code></pre>"},{"location":"reference/config-options/#carbon_tracking","title":"CARBON_TRACKING","text":"<p>Enable carbon emissions tracking (CodeCarbon only).</p> Property Value Type boolean Default false Environment <code>CARBON_TRACKING</code> <pre><code>export CARBON_TRACKING=true\n</code></pre>"},{"location":"reference/config-options/#country_iso","title":"COUNTRY_ISO","text":"<p>ISO country code for carbon intensity calculations.</p> Property Value Type string Default USA Environment <code>COUNTRY_ISO</code> <pre><code>export COUNTRY_ISO=DEU  # Germany\n</code></pre>"},{"location":"reference/config-options/#output-options","title":"Output Options","text":""},{"location":"reference/config-options/#output_dir","title":"OUTPUT_DIR","text":"<p>Directory for output files.</p> Property Value Type string (path) Default ./data Environment <code>OUTPUT_DIR</code> <pre><code>export OUTPUT_DIR=/var/results/greenmining\n</code></pre>"},{"location":"reference/config-options/#repos_file","title":"REPOS_FILE","text":"<p>Filename for repository data.</p> Property Value Type string Default repositories.json Environment <code>REPOS_FILE</code>"},{"location":"reference/config-options/#commits_file","title":"COMMITS_FILE","text":"<p>Filename for commit data.</p> Property Value Type string Default commits.json Environment <code>COMMITS_FILE</code>"},{"location":"reference/config-options/#analysis_file","title":"ANALYSIS_FILE","text":"<p>Filename for analysis results.</p> Property Value Type string Default analysis_results.json Environment <code>ANALYSIS_FILE</code>"},{"location":"reference/config-options/#stats_file","title":"STATS_FILE","text":"<p>Filename for aggregated statistics.</p> Property Value Type string Default aggregated_statistics.json Environment <code>STATS_FILE</code>"},{"location":"reference/config-options/#report_file","title":"REPORT_FILE","text":"<p>Filename for Markdown report.</p> Property Value Type string Default green_analysis.md Environment <code>REPORT_FILE</code>"},{"location":"reference/config-options/#sample-env-file","title":"Sample .env File","text":"<p>Complete <code>.env</code> configuration:</p> <pre><code># Required\nGITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx\n\n# Repository Fetching\nMAX_REPOS=100\nMIN_STARS=100\nSUPPORTED_LANGUAGES=Python,Java,Go,JavaScript,TypeScript\nSEARCH_KEYWORDS=microservices\n\n# URL Analysis\nCLONE_PATH=/tmp/greenmining_repos\nCLEANUP_AFTER_ANALYSIS=true\n\n# Commit Extraction\nCOMMITS_PER_REPO=1000\nEXCLUDE_MERGE_COMMITS=true\nEXCLUDE_BOT_COMMITS=true\nMIN_MESSAGE_LENGTH=10\n\n# Analysis\nENABLE_DIFF_ANALYSIS=false\nBATCH_SIZE=10\n\n# Process Metrics\nPROCESS_METRICS_ENABLED=true\nSTRUCTURAL_METRICS_ENABLED=true\nDMM_ENABLED=true\n\n# Statistical Analysis\nENABLE_STATS=true\nENABLE_TEMPORAL=true\nTEMPORAL_GRANULARITY=quarter\n\n# Energy Measurement\nENERGY_ENABLED=false\nENERGY_BACKEND=rapl\nCARBON_TRACKING=false\nCOUNTRY_ISO=USA\n\n# Output\nOUTPUT_DIR=./data\nREPORT_FILE=green_analysis.md\n</code></pre>"},{"location":"reference/config-options/#python-config-class","title":"Python Config Class","text":"<pre><code>from greenmining.config import Config\n\nconfig = Config()\n\n# Access all options\nprint(f\"MAX_REPOS: {config.MAX_REPOS}\")\nprint(f\"COMMITS_PER_REPO: {config.COMMITS_PER_REPO}\")\nprint(f\"SUPPORTED_LANGUAGES: {config.SUPPORTED_LANGUAGES}\")\nprint(f\"CLONE_PATH: {config.CLONE_PATH}\")\nprint(f\"ENERGY_ENABLED: {config.ENERGY_ENABLED}\")\nprint(f\"ENERGY_BACKEND: {config.ENERGY_BACKEND}\")\n\n# Override at runtime\nconfig.MAX_REPOS = 50\nconfig.ENABLE_TEMPORAL = True\n</code></pre>"},{"location":"reference/config-options/#next-steps","title":"Next Steps","text":"<ul> <li>GSF Patterns - Pattern reference</li> <li>Python API - Programmatic configuration</li> <li>Energy Measurement - Energy tracking guide</li> </ul>"},{"location":"reference/graphql/","title":"GraphQL API Reference","text":"<p>GreenMining uses GitHub's GraphQL API v4 for repository search and metadata retrieval.</p>"},{"location":"reference/graphql/#overview","title":"Overview","text":"<p>The <code>GitHubGraphQLFetcher</code> class provides efficient repository discovery using a single GraphQL query per page of results, compared to multiple REST API calls.</p> <pre><code>from greenmining.services.github_graphql_fetcher import GitHubGraphQLFetcher\n\nfetcher = GitHubGraphQLFetcher(token=\"ghp_your_token\")\n</code></pre> <p>Or use the top-level wrapper:</p> <pre><code>from greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=\"ghp_your_token\",\n    max_repos=10,\n    keywords=\"blockchain\",\n)\n</code></pre>"},{"location":"reference/graphql/#githubgraphqlfetcher","title":"GitHubGraphQLFetcher","text":""},{"location":"reference/graphql/#constructor","title":"Constructor","text":"<pre><code>GitHubGraphQLFetcher(token: str)\n</code></pre> Parameter Type Description <code>token</code> str GitHub personal access token with <code>repo</code> scope"},{"location":"reference/graphql/#search_repositories","title":"search_repositories()","text":"<p>Search for repositories matching criteria.</p> <pre><code>def search_repositories(\n    keywords: str = \"microservices\",\n    max_repos: int = 100,\n    min_stars: int = 100,\n    languages: list[str] | None = None,\n    created_after: str | None = None,\n    created_before: str | None = None,\n    pushed_after: str | None = None,\n    pushed_before: str | None = None,\n) -&gt; list[Repository]\n</code></pre> Parameter Type Default Description <code>keywords</code> str <code>\"microservices\"</code> Search keywords <code>max_repos</code> int <code>100</code> Maximum repositories to return <code>min_stars</code> int <code>100</code> Minimum GitHub stars <code>languages</code> list <code>None</code> Filter by programming languages <code>created_after</code> str <code>None</code> Created after date (YYYY-MM-DD) <code>created_before</code> str <code>None</code> Created before date (YYYY-MM-DD) <code>pushed_after</code> str <code>None</code> Last pushed after date (YYYY-MM-DD) <code>pushed_before</code> str <code>None</code> Last pushed before date (YYYY-MM-DD) <p>Returns: List of <code>Repository</code> objects.</p> <p>Example:</p> <pre><code>fetcher = GitHubGraphQLFetcher(token)\n\nrepos = fetcher.search_repositories(\n    keywords=\"blockchain\",\n    max_repos=10,\n    min_stars=3,\n    languages=[\"Python\", \"Go\", \"Rust\"],\n    created_after=\"2020-01-01\",\n)\n\nfor repo in repos:\n    print(f\"{repo.full_name} ({repo.stars} stars, {repo.language})\")\n</code></pre>"},{"location":"reference/graphql/#get_repository_commits","title":"get_repository_commits()","text":"<p>Fetch commits for a specific repository.</p> <pre><code>def get_repository_commits(\n    owner: str,\n    name: str,\n    max_commits: int = 100,\n) -&gt; list[dict]\n</code></pre> Parameter Type Default Description <code>owner</code> str (required) Repository owner <code>name</code> str (required) Repository name <code>max_commits</code> int <code>100</code> Maximum commits to fetch <p>Returns: List of commit dictionaries with keys:</p> Key Type Description <code>sha</code> str Commit SHA (oid) <code>message</code> str Commit message <code>date</code> str Committed date (ISO 8601) <code>author</code> str Author name <code>author_email</code> str Author email <code>additions</code> int Lines added <code>deletions</code> int Lines deleted <code>changed_files</code> int Number of files changed <p>Example:</p> <pre><code>commits = fetcher.get_repository_commits(\"pallets\", \"flask\", max_commits=50)\nfor c in commits:\n    print(f\"{c['sha'][:8]} | {c['author']} | {c['message'][:60]}\")\n</code></pre>"},{"location":"reference/graphql/#save_results","title":"save_results()","text":"<p>Save repositories to a JSON file.</p> <pre><code>def save_results(repositories: list[Repository], output_file: str) -&gt; None\n</code></pre>"},{"location":"reference/graphql/#search-query-building","title":"Search Query Building","text":"<p>The <code>_build_search_query()</code> method constructs a GitHub search query string from the parameters. Understanding how queries are built helps debug search results.</p>"},{"location":"reference/graphql/#query-format","title":"Query Format","text":"<pre><code>{keywords} stars:&gt;={min_stars} [language:{lang}]... [created:&gt;={date}] [pushed:&gt;={date}]\n</code></pre>"},{"location":"reference/graphql/#language-filter-limit","title":"Language Filter Limit","text":"<p>GitHub's search API has a complexity limit on queries. When more than 5 languages are specified, the language filter is skipped entirely to avoid empty results:</p> <pre><code># 3 languages -&gt; filter applied\n# Query: \"blockchain stars:&gt;=3 language:Python language:Go language:Rust\"\nrepos = fetch_repositories(languages=[\"Python\", \"Go\", \"Rust\"], ...)\n\n# 20 languages -&gt; filter skipped (returns repos in any language)\n# Query: \"blockchain stars:&gt;=3\"\nrepos = fetch_repositories(languages=LANGUAGES_20, ...)\n</code></pre> <p>This is by design. When using many languages, the search returns results in any language, which is more useful than returning zero results from an overly complex query.</p>"},{"location":"reference/graphql/#date-filters","title":"Date Filters","text":"<p>All date filters use the <code>YYYY-MM-DD</code> format:</p> <pre><code># Repos created in 2023\nrepos = fetcher.search_repositories(\n    created_after=\"2023-01-01\",\n    created_before=\"2023-12-31\",\n)\n\n# Repos with recent activity\nrepos = fetcher.search_repositories(\n    pushed_after=\"2024-01-01\",\n)\n</code></pre>"},{"location":"reference/graphql/#rate-limiting","title":"Rate Limiting","text":"<p>The GraphQL API has a point-based rate limit (5,000 points/hour for authenticated users). Each search query costs 1 point.</p> <p>GreenMining handles rate limits automatically:</p> <ul> <li>Prints remaining quota after each query</li> <li>Sleeps for 60 seconds when remaining points drop below 100</li> <li>Supports pagination with cursor-based <code>after</code> parameter</li> </ul> <pre><code>Rate Limit: 4998/5000 (cost: 1)\n</code></pre>"},{"location":"reference/graphql/#checking-your-rate-limit","title":"Checking Your Rate Limit","text":"<pre><code>fetcher = GitHubGraphQLFetcher(token)\n# Rate limit info is printed with every query\nrepos = fetcher.search_repositories(max_repos=1)\n</code></pre>"},{"location":"reference/graphql/#pagination","title":"Pagination","text":"<p>Results are paginated using GitHub's cursor-based pagination. The fetcher handles this automatically:</p> <ul> <li>Fetches up to 100 results per page (GitHub's maximum)</li> <li>Follows <code>pageInfo.hasNextPage</code> and <code>endCursor</code> cursors</li> <li>Stops when <code>max_repos</code> is reached or no more results exist</li> </ul> <p>For large result sets:</p> <pre><code># Fetches 3 pages of 100 results each\nrepos = fetcher.search_repositories(max_repos=300)\n</code></pre>"},{"location":"reference/graphql/#repository-model","title":"Repository Model","text":"<p>Each search result is parsed into a <code>Repository</code> dataclass:</p> Field Type Source (GraphQL) <code>repo_id</code> int Sequential index <code>name</code> str <code>name</code> <code>owner</code> str Parsed from <code>nameWithOwner</code> <code>full_name</code> str <code>nameWithOwner</code> <code>url</code> str <code>url</code> <code>clone_url</code> str <code>url</code> + <code>.git</code> <code>language</code> str <code>primaryLanguage.name</code> <code>stars</code> int <code>stargazerCount</code> <code>forks</code> int <code>forkCount</code> <code>watchers</code> int <code>watchers.totalCount</code> <code>open_issues</code> int 0 (not queried) <code>last_updated</code> str <code>updatedAt</code> <code>created_at</code> str <code>createdAt</code> <code>description</code> str <code>description</code> <code>main_branch</code> str <code>defaultBranchRef.name</code> <code>archived</code> bool <code>isArchived</code> <code>license</code> str <code>licenseInfo.name</code>"},{"location":"reference/graphql/#graphql-query-schema","title":"GraphQL Query Schema","text":"<p>The search query fetches these fields per repository:</p> <pre><code>query($searchQuery: String!, $first: Int!) {\n  search(query: $searchQuery, type: REPOSITORY, first: $first) {\n    repositoryCount\n    pageInfo { hasNextPage, endCursor }\n    nodes {\n      ... on Repository {\n        id, name, nameWithOwner, description, url\n        createdAt, updatedAt, pushedAt\n        stargazerCount, forkCount\n        watchers { totalCount }\n        primaryLanguage { name }\n        languages(first: 5) { nodes { name } }\n        licenseInfo { name }\n        isArchived, isFork\n        defaultBranchRef { name }\n      }\n    }\n  }\n  rateLimit { limit, cost, remaining, resetAt }\n}\n</code></pre>"},{"location":"reference/graphql/#next-steps","title":"Next Steps","text":"<ul> <li>Python API - <code>fetch_repositories</code> wrapper</li> <li>Data Models - Repository dataclass fields</li> <li>Experiment - Full pipeline example</li> </ul>"},{"location":"reference/models/","title":"Data Models Reference","text":"<p>Reference for GreenMining data models and structures.</p>"},{"location":"reference/models/#core-models","title":"Core Models","text":""},{"location":"reference/models/#repository","title":"Repository","text":"<p>Represents a GitHub repository.</p> <pre><code>from greenmining.models import Repository\n\nrepo = Repository(\n    repo_id=12345,\n    name=\"flask\",\n    full_name=\"pallets/flask\",\n    owner=\"pallets\",\n    url=\"https://github.com/pallets/flask\",\n    clone_url=\"https://github.com/pallets/flask.git\",\n    stars=65000,\n    forks=16000,\n    watchers=2500,\n    open_issues=50,\n    language=\"Python\",\n    last_updated=\"2024-01-15T10:00:00Z\",\n    created_at=\"2010-04-06T12:00:00Z\",\n    description=\"The Python micro framework for building web applications.\",\n    main_branch=\"main\"\n)\n</code></pre>"},{"location":"reference/models/#fields","title":"Fields","text":"Field Type Description <code>repo_id</code> int GitHub repository ID <code>name</code> str Repository name <code>full_name</code> str Full name (owner/repo) <code>owner</code> str Repository owner <code>url</code> str GitHub URL <code>clone_url</code> str Git clone URL <code>stars</code> int Star count <code>forks</code> int Fork count <code>watchers</code> int Watcher count <code>open_issues</code> int Open issue count <code>language</code> str Primary language <code>last_updated</code> str Last update timestamp <code>created_at</code> str Creation timestamp <code>description</code> str Repository description <code>main_branch</code> str Default branch name"},{"location":"reference/models/#commit","title":"Commit","text":"<p>Represents a Git commit.</p> <pre><code>from greenmining.models import Commit\n\ncommit = Commit(\n    sha=\"abc123def456789\",\n    message=\"Implement Redis caching for user sessions\",\n    author=\"developer\",\n    date=\"2024-01-15T10:30:00Z\",\n    repository=\"pallets/flask\"\n)\n</code></pre>"},{"location":"reference/models/#fields_1","title":"Fields","text":"Field Type Description <code>sha</code> str Commit SHA hash <code>message</code> str Commit message <code>author</code> str Author username <code>date</code> str Commit timestamp (ISO 8601) <code>repository</code> str Repository full name"},{"location":"reference/models/#analysisresult","title":"AnalysisResult","text":"<p>Represents the analysis result for a commit.</p> <pre><code>from greenmining.models import AnalysisResult\n\nresult = AnalysisResult(\n    commit=commit,\n    green_aware=True,\n    patterns=[\"Cache Static Data\"],\n    confidence=0.85\n)\n</code></pre>"},{"location":"reference/models/#fields_2","title":"Fields","text":"Field Type Description <code>commit</code> Commit The analyzed commit <code>green_aware</code> bool Whether commit is green-aware <code>patterns</code> list[str] Matched pattern names <code>confidence</code> float Detection confidence (0-1)"},{"location":"reference/models/#analysis-output-structures","title":"Analysis Output Structures","text":""},{"location":"reference/models/#commit-analysis-dictionary","title":"Commit Analysis Dictionary","text":"<p>When commits are analyzed, they're returned as dictionaries:</p> <pre><code>{\n    \"sha\": \"abc123def456789\",\n    \"message\": \"Implement Redis caching for user sessions\",\n    \"author\": \"developer\",\n    \"date\": \"2024-01-15T10:30:00Z\",\n    \"repository\": \"pallets/flask\",\n    \"green_aware\": True,\n    \"patterns\": [\"Cache Static Data\"],\n    \"category\": \"caching\",\n    \"keywords_matched\": [\"redis\", \"caching\"],\n    \"confidence\": 0.85,\n\n    # If diff analysis enabled\n    \"diff_patterns\": [\"caching\"],\n    \"files_modified\": 3,\n\n    # If process metrics enabled\n    \"insertions\": 45,\n    \"deletions\": 12,\n    \"dmm_unit_size\": 0.85,\n    \"dmm_unit_complexity\": 0.72,\n    \"dmm_unit_interfacing\": 0.90\n}\n</code></pre>"},{"location":"reference/models/#aggregated-statistics-structure","title":"Aggregated Statistics Structure","text":"<p>Output from <code>DataAggregator.aggregate()</code>:</p> <pre><code>{\n    \"summary\": {\n        \"total_commits\": 5000,\n        \"green_aware_count\": 1250,\n        \"green_aware_percentage\": 25.0,\n        \"total_repos\": 50,\n        \"repos_with_green_commits\": 42,\n        \"analysis_date\": \"2024-01-15T10:00:00Z\"\n    },\n\n    \"pattern_distribution\": {\n        \"Cache Static Data\": 320,\n        \"Use Async Instead of Sync\": 180,\n        \"Compress Transmitted Data\": 150,\n        \"Lazy Loading\": 120,\n        \"Optimize Database Queries\": 95\n    },\n\n    \"category_distribution\": {\n        \"cloud\": 450,\n        \"caching\": 320,\n        \"async\": 210,\n        \"web\": 180,\n        \"database\": 95\n    },\n\n    \"per_repo_stats\": [\n        {\n            \"repository\": \"pallets/flask\",\n            \"total_commits\": 200,\n            \"green_aware_count\": 47,\n            \"green_aware_percentage\": 23.5,\n            \"top_patterns\": [\"Cache Static Data\", \"Lazy Loading\"]\n        }\n    ],\n\n    \"per_language_stats\": {\n        \"Python\": {\n            \"total_commits\": 2000,\n            \"green_aware_count\": 520,\n            \"green_aware_percentage\": 26.0\n        }\n    },\n\n    # If enable_temporal=True\n    \"temporal_analysis\": {\n        \"periods\": [\n            {\n                \"period\": \"2024-Q1\",\n                \"commit_count\": 500,\n                \"green_count\": 125,\n                \"green_awareness_rate\": 0.25\n            }\n        ],\n        \"overall_trend\": {\n            \"direction\": \"increasing\",\n            \"significant\": True\n        }\n    },\n\n    # If enable_stats=True\n    \"statistics\": {\n        \"pattern_correlations\": {\n            \"top_positive_correlations\": [\n                {\n                    \"pattern1\": \"caching\",\n                    \"pattern2\": \"performance\",\n                    \"correlation\": 0.75\n                }\n            ]\n        },\n        \"effect_size\": {\n            \"green_vs_nongreen_patterns\": {\n                \"cohens_d\": 0.65,\n                \"magnitude\": \"medium\"\n            }\n        },\n        \"descriptive\": {\n            \"patterns_per_commit\": {\n                \"mean\": 2.3,\n                \"median\": 2.0,\n                \"std\": 1.1\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"reference/models/#url-analysis-result-structure","title":"URL Analysis Result Structure","text":"<p>Output from <code>LocalRepoAnalyzer.analyze_repository()</code>:</p> <pre><code>{\n    \"repository\": {\n        \"name\": \"flask\",\n        \"url\": \"https://github.com/pallets/flask\",\n        \"owner\": \"pallets\",\n        \"clone_path\": \"/tmp/greenmining_repos/flask\"\n    },\n\n    \"total_commits\": 200,\n    \"green_aware_count\": 47,\n    \"green_aware_percentage\": 23.5,\n\n    \"commits\": [\n        {\n            \"sha\": \"abc123...\",\n            \"message\": \"Optimize template caching\",\n            \"author\": \"developer\",\n            \"date\": \"2024-03-15T10:30:00\",\n            \"green_aware\": True,\n            \"patterns\": [\"Cache Static Data\"],\n\n            # Process metrics\n            \"modified_files\": 3,\n            \"insertions\": 45,\n            \"deletions\": 12,\n            \"files\": [\"app.py\", \"cache.py\", \"config.py\"],\n\n            # DMM metrics\n            \"dmm_unit_size\": 0.85,\n            \"dmm_unit_complexity\": 0.72,\n            \"dmm_unit_interfacing\": 0.90\n        }\n    ],\n\n    \"pattern_distribution\": {\n        \"Cache Static Data\": 15,\n        \"Use Async Instead of Sync\": 12,\n        \"Lazy Loading\": 8\n    },\n\n    \"process_metrics\": {\n        \"change_set\": {\n            \"max\": 25,\n            \"avg\": 5.2\n        },\n        \"code_churn\": {\n            \"added\": 5000,\n            \"removed\": 2000\n        },\n        \"contributors_count\": 45,\n        \"commits_count\": 200\n    }\n}\n</code></pre>"},{"location":"reference/models/#energy-metrics-structures","title":"Energy Metrics Structures","text":""},{"location":"reference/models/#energymetrics","title":"EnergyMetrics","text":"<pre><code>from greenmining.energy.base import EnergyMetrics\n\n@dataclass\nclass EnergyMetrics:\n    energy_joules: float       # Total energy consumed\n    duration_seconds: float    # Measurement duration\n    average_power_watts: float # Average power draw\n    start_time: datetime       # Start timestamp\n    end_time: datetime         # End timestamp\n\n    # CodeCarbon specific\n    energy_kwh: float = 0.0    # Energy in kWh\n    emissions_kg: float = 0.0  # CO2 emissions\n</code></pre>"},{"location":"reference/models/#commitenergyprofile","title":"CommitEnergyProfile","text":"<pre><code>from greenmining.energy.base import CommitEnergyProfile\n\n@dataclass\nclass CommitEnergyProfile:\n    commit_sha: str            # Commit identifier\n    energy_joules: float       # Energy for this commit\n    duration_seconds: float    # Analysis duration\n    patterns_detected: list    # Patterns found\n    files_analyzed: int        # Files in commit\n</code></pre>"},{"location":"reference/models/#pattern-structure","title":"Pattern Structure","text":"<p>GSF patterns are stored as dictionaries:</p> <pre><code>{\n    \"cache_static_data\": {\n        \"name\": \"Cache Static Data\",\n        \"category\": \"cloud\",\n        \"keywords\": [\"cache\", \"caching\", \"static\", \"cdn\", \"redis\", \"memcache\"],\n        \"description\": \"Cache static content to reduce server load and network transfers\",\n        \"sci_impact\": \"Reduces energy by minimizing redundant compute and network operations\"\n    }\n}\n</code></pre>"},{"location":"reference/models/#fields_3","title":"Fields","text":"Field Type Description <code>name</code> str Human-readable pattern name <code>category</code> str Pattern category <code>keywords</code> list[str] Detection keywords <code>description</code> str Pattern description <code>sci_impact</code> str Impact on software carbon intensity"},{"location":"reference/models/#working-with-models","title":"Working with Models","text":""},{"location":"reference/models/#serialization","title":"Serialization","text":"<pre><code>import json\n\n# Convert to JSON\nresult_json = json.dumps(analysis_result, default=str)\n\n# Load from JSON\nwith open(\"results.json\") as f:\n    results = json.load(f)\n</code></pre>"},{"location":"reference/models/#filtering-results","title":"Filtering Results","text":"<pre><code># Get only green-aware commits\ngreen_commits = [c for c in commits if c[\"green_aware\"]]\n\n# Group by pattern\nfrom collections import defaultdict\nby_pattern = defaultdict(list)\nfor commit in green_commits:\n    for pattern in commit[\"patterns\"]:\n        by_pattern[pattern].append(commit)\n\n# Get top patterns\ntop_patterns = sorted(\n    by_pattern.items(), \n    key=lambda x: len(x[1]), \n    reverse=True\n)[:10]\n</code></pre>"},{"location":"reference/models/#aggregating-custom-metrics","title":"Aggregating Custom Metrics","text":"<pre><code>import statistics\n\n# Calculate custom statistics\ngreen_counts = [r[\"green_aware_count\"] for r in per_repo_stats]\navg_green = statistics.mean(green_counts)\nmedian_green = statistics.median(green_counts)\nstd_green = statistics.stdev(green_counts) if len(green_counts) &gt; 1 else 0\n</code></pre>"},{"location":"reference/models/#next-steps","title":"Next Steps","text":"<ul> <li>Python API - Working with models</li> <li>GSF Patterns - Pattern reference</li> <li>Configuration - All options</li> </ul>"},{"location":"reference/patterns/","title":"GSF Patterns Reference","text":"<p>Complete reference for all 122 Green Software Foundation patterns supported by GreenMining.</p>"},{"location":"reference/patterns/#overview","title":"Overview","text":"<p>GreenMining detects patterns from the Green Software Foundation catalog, organized into 15 categories.</p> Statistic Value Total Patterns 122 Categories 15 Keywords 321"},{"location":"reference/patterns/#pattern-categories","title":"Pattern Categories","text":""},{"location":"reference/patterns/#cloud-patterns-40","title":"Cloud Patterns (40+)","text":"<p>Patterns for cloud-native and infrastructure optimization.</p> Pattern Keywords Description Cache Static Data cache, caching, static, cdn, redis, memcache Cache static content to reduce server load Choose Region Closest region, closest, proximity, latency Deploy in regions closest to users Compress Stored Data compress, storage, gzip, zstd Compress data at rest Compress Transmitted Data compress, transmission, gzip, brotli Compress data before network transfer Containerize Workload container, docker, kubernetes, pod Use containers for resource efficiency Delete Unused Storage delete, remove, unused, cleanup Remove unused storage resources Encrypt What Is Necessary encrypt, tls, ssl, crypto Only encrypt data that needs protection Evaluate CPU Architectures cpu, arm, graviton, processor Consider ARM and efficient CPUs Use Service Mesh service mesh, istio, linkerd, envoy Optimize service-to-service communication TLS Termination tls termination, ssl offload Terminate TLS at edge Implement Stateless Design stateless, session, horizontal Design without server-side state Match SLO Requirements slo, sla, service level Don't over-engineer beyond SLO Match VM Utilization vm, instance, size, utilization Right-size VMs to workload Move to Cloud cloud, migrate, migration Leverage cloud efficiency Optimize Average Utilization utilization, optimize, average Increase resource utilization Optimize High Utilization high utilization, maximize Target high utilization levels Optimize Network Traffic network, traffic, optimize Reduce unnecessary network traffic Scale Down Idle Resources scale down, idle, shutdown Reduce resources when idle Scale Infrastructure scaling, autoscaling, scale Scale based on demand Terminate Unused Resources terminate, unused, cleanup Remove unused resources Use Reserved Instances reserved, spot, savings Use cost-efficient instance types Use Serverless serverless, lambda, functions Use serverless for variable workloads Use Spot Instances spot, preemptible, interruption Use spot instances for cost savings"},{"location":"reference/patterns/#web-patterns-15","title":"Web Patterns (15+)","text":"<p>Patterns for web application optimization.</p> Pattern Keywords Description Enable Text Compression gzip, brotli, deflate Compress text responses Lazy Loading lazy, defer, on-demand Load content only when needed Minify CSS/JS minify, minification, uglify Reduce asset file sizes Optimize Images image, webp, avif, srcset Use modern image formats Use CDN cdn, content delivery, edge Serve from edge locations Cache HTTP Responses cache-control, etag, expires Enable browser caching Reduce DOM Size dom, elements, optimize Minimize DOM complexity Use Service Workers service worker, pwa, offline Enable offline caching Preconnect Resources preconnect, preload, prefetch Hint browser to connect early Remove Unused CSS unused, purge, tree-shake Remove dead CSS code Optimize Fonts font, woff2, subset Use efficient font loading Reduce JavaScript javascript, bundle, split Minimize JS payload Use HTTP/2 http2, multiplexing Use modern HTTP protocol Enable Keep-Alive keep-alive, persistent Reuse HTTP connections"},{"location":"reference/patterns/#aiml-patterns-10","title":"AI/ML Patterns (10+)","text":"<p>Patterns for machine learning optimization.</p> Pattern Keywords Description Model Optimization model, optimize, prune Optimize model architecture Quantization quantize, int8, fp16 Reduce model precision Knowledge Distillation distillation, student, teacher Train smaller models Efficient Training training, efficient, epoch Optimize training process Batch Inference batch, inference, throughput Process predictions in batches Model Caching model cache, warm, preload Cache loaded models Feature Selection feature, select, reduce Use fewer features Early Stopping early stop, convergence Stop training when converged Mixed Precision mixed precision, amp Use mixed precision training Gradient Checkpointing checkpoint, gradient, memory Trade compute for memory"},{"location":"reference/patterns/#caching-patterns-8","title":"Caching Patterns (8)","text":"<p>Patterns for caching strategies.</p> Pattern Keywords Description Redis Caching redis, cache, memory Use Redis for caching CDN Caching cdn, edge, cache Cache at CDN edge Database Query Cache query cache, mysql, postgres Cache database queries Application Cache app cache, memory, local In-memory application cache Distributed Cache distributed, memcached Multi-node caching Cache Invalidation invalidate, ttl, expire Proper cache expiration Write-Through Cache write-through, consistency Consistent caching Cache Warming warm, preload, prefetch Pre-populate caches"},{"location":"reference/patterns/#async-patterns-6","title":"Async Patterns (6)","text":"<p>Patterns for asynchronous processing.</p> Pattern Keywords Description Queue Non-Urgent Requests queue, async, defer Queue non-critical work Use Async Instead of Sync async, await, non-blocking Prefer async operations Batch Processing batch, bulk, aggregate Process in batches Event-Driven Architecture event, pub-sub, message Use event-driven design Background Jobs background, worker, job Process in background Stream Processing stream, reactive, flow Use streaming for large data"},{"location":"reference/patterns/#database-patterns-8","title":"Database Patterns (8)","text":"<p>Patterns for database optimization.</p> Pattern Keywords Description Optimize Database Queries query, optimize, explain Improve query performance Use Connection Pooling pool, connection, reuse Reuse database connections Index Optimization index, btree, covering Optimize database indexes Read Replicas replica, read, slave Scale reads with replicas Denormalization denormalize, join, embed Reduce join operations Partition Tables partition, shard, split Split large tables Use NoSQL nosql, document, key-value Use appropriate database type Lazy Loading Relations lazy, eager, n+1 Avoid N+1 query problems"},{"location":"reference/patterns/#network-patterns-6","title":"Network Patterns (6)","text":"<p>Patterns for network optimization.</p> Pattern Keywords Description HTTP Compression gzip, compress, transfer Compress HTTP responses Reduce API Calls batch, aggregate, graphql Minimize API requests Use WebSockets websocket, socket, realtime Use persistent connections Protocol Optimization http3, quic, protocol Use efficient protocols Edge Computing edge, close, proximity Process at the edge Connection Reuse keep-alive, persist, reuse Reuse network connections"},{"location":"reference/patterns/#resource-patterns-5","title":"Resource Patterns (5)","text":"<p>Patterns for resource management.</p> Pattern Keywords Description Memory Optimization memory, heap, gc Optimize memory usage CPU Optimization cpu, thread, parallel Optimize CPU usage I/O Optimization io, disk, buffer Optimize I/O operations Resource Pooling pool, reuse, recycle Pool expensive resources Garbage Collection Tuning gc, tuning, generational Tune GC parameters"},{"location":"reference/patterns/#code-patterns-4","title":"Code Patterns (4)","text":"<p>Patterns for code-level optimization.</p> Pattern Keywords Description Remove Dead Code dead code, unused, remove Eliminate unused code Algorithm Optimization algorithm, complexity, O(n) Use efficient algorithms Loop Optimization loop, iteration, vectorize Optimize loops Avoid Premature Optimization premature, profile, measure Profile before optimizing"},{"location":"reference/patterns/#infrastructure-patterns-4","title":"Infrastructure Patterns (4)","text":"<p>Patterns for infrastructure optimization.</p> Pattern Keywords Description Alpine Containers alpine, minimal, scratch Use minimal base images Infrastructure as Code iac, terraform, ansible Manage infrastructure as code Renewable Energy Regions renewable, green, carbon Use green energy regions Container Optimization container, layer, cache Optimize container builds"},{"location":"reference/patterns/#microservices-patterns-4","title":"Microservices Patterns (4)","text":"<p>Patterns for microservices architecture.</p> Pattern Keywords Description Service Decomposition decompose, microservice, split Right-size services Colocation Strategies colocate, affinity, proximity Place related services together Graceful Shutdown graceful, shutdown, sigterm Handle shutdown properly Service Mesh Optimization mesh, sidecar, istio Optimize service mesh overhead"},{"location":"reference/patterns/#monitoring-patterns-3","title":"Monitoring Patterns (3)","text":"<p>Patterns for observability optimization.</p> Pattern Keywords Description Efficient Logging logging, log level, structured Optimize log volume Metrics Aggregation metrics, aggregate, rollup Aggregate metrics efficiently Trace Sampling sampling, trace, opentelemetry Sample traces appropriately"},{"location":"reference/patterns/#general-patterns-8","title":"General Patterns (8)","text":"<p>General optimization patterns.</p> Pattern Keywords Description Feature Flags feature flag, toggle, switch Use feature flags Incremental Processing incremental, delta, diff Process only changes Precomputation precompute, materialize, cache Precompute expensive results Background Jobs background, async, worker Process in background Rate Limiting rate limit, throttle, backoff Limit request rates Circuit Breaker circuit, breaker, fallback Fail fast with fallbacks Retry with Backoff retry, backoff, exponential Retry with exponential backoff Timeout Configuration timeout, deadline, cancel Set appropriate timeouts"},{"location":"reference/patterns/#accessing-patterns-programmatically","title":"Accessing Patterns Programmatically","text":"<pre><code>from greenmining import GSF_PATTERNS\n\n# Count patterns\nprint(f\"Total patterns: {len(GSF_PATTERNS)}\")  # 122\n\n# Get all categories\ncategories = set(p[\"category\"] for p in GSF_PATTERNS.values())\nprint(f\"Categories: {sorted(categories)}\")\n\n# Find patterns by category\ncloud_patterns = [\n    p for p in GSF_PATTERNS.values() \n    if p[\"category\"] == \"cloud\"\n]\nprint(f\"Cloud patterns: {len(cloud_patterns)}\")\n\n# Get pattern details\npattern = GSF_PATTERNS[\"cache_static_data\"]\nprint(f\"Name: {pattern['name']}\")\nprint(f\"Category: {pattern['category']}\")\nprint(f\"Keywords: {pattern['keywords']}\")\nprint(f\"Description: {pattern['description']}\")\nprint(f\"SCI Impact: {pattern['sci_impact']}\")\n</code></pre>"},{"location":"reference/patterns/#green-keywords","title":"Green Keywords","text":"<p>The 321 green keywords used for detection:</p> <pre><code>from greenmining import GREEN_KEYWORDS\n\n# Categories of keywords\nkeyword_categories = {\n    \"energy\": [\"energy\", \"power\", \"watt\", \"joule\", \"consumption\"],\n    \"carbon\": [\"carbon\", \"emission\", \"co2\", \"greenhouse\", \"footprint\"],\n    \"efficiency\": [\"efficient\", \"efficiency\", \"optimize\", \"reduce\", \"minimize\"],\n    \"sustainability\": [\"sustainable\", \"green\", \"eco\", \"environmental\"],\n    \"performance\": [\"performance\", \"fast\", \"speed\", \"latency\", \"throughput\"],\n    \"resource\": [\"resource\", \"memory\", \"cpu\", \"disk\", \"network\"],\n    \"caching\": [\"cache\", \"cached\", \"caching\", \"redis\", \"memcache\"],\n    \"compression\": [\"compress\", \"gzip\", \"brotli\", \"minify\", \"compact\"],\n}\n\n# Sample keywords\nprint(GREEN_KEYWORDS[:20])\n# ['energy', 'power', 'carbon', 'emission', 'footprint', 'sustainability', ...]\n</code></pre>"},{"location":"reference/patterns/#pattern-detection-example","title":"Pattern Detection Example","text":"<pre><code>from greenmining import is_green_aware, get_pattern_by_keywords\n\n# Test messages\nmessages = [\n    \"Implement Redis caching for user sessions\",\n    \"Enable gzip compression on API responses\",\n    \"Migrate to serverless Lambda functions\",\n    \"Optimize database queries with proper indexing\",\n    \"Add lazy loading for images\",\n    \"Fix typo in documentation\",\n]\n\nfor msg in messages:\n    is_green = is_green_aware(msg)\n    patterns = get_pattern_by_keywords(msg) if is_green else []\n\n    if is_green:\n        print(f\"\ud83c\udf31 {msg}\")\n        print(f\"   Patterns: {patterns}\")\n    else:\n        print(f\"   {msg}\")\n</code></pre> <p>Output:</p> <pre><code>\ud83c\udf31 Implement Redis caching for user sessions\n   Patterns: ['Cache Static Data']\n\ud83c\udf31 Enable gzip compression on API responses\n   Patterns: ['Compress Transmitted Data', 'Enable Text Compression']\n\ud83c\udf31 Migrate to serverless Lambda functions\n   Patterns: ['Use Serverless']\n\ud83c\udf31 Optimize database queries with proper indexing\n   Patterns: ['Optimize Database Queries', 'Index Optimization']\n\ud83c\udf31 Add lazy loading for images\n   Patterns: ['Lazy Loading', 'Optimize Images']\n   Fix typo in documentation\n</code></pre>"},{"location":"reference/patterns/#contributing-patterns","title":"Contributing Patterns","text":"<p>To suggest new patterns or improvements:</p> <ol> <li>Check the GSF Patterns Catalog</li> <li>Open an issue on GitHub</li> <li>Submit a pull request with pattern additions</li> </ol>"},{"location":"reference/patterns/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Options - All configuration parameters</li> <li>Python API - Programmatic usage</li> <li>Data Models - Repository and Commit models</li> </ul>"},{"location":"user-guide/api/","title":"Python API Reference","text":"<p>Use GreenMining programmatically in your Python scripts.</p>"},{"location":"user-guide/api/#quick-import","title":"Quick Import","text":"<pre><code>from greenmining import (\n    GSF_PATTERNS,        # Dict of 122 GSF patterns\n    GREEN_KEYWORDS,      # List of 321 green keywords\n    is_green_aware,      # Check if message is green-aware\n    get_pattern_by_keywords,  # Get matched patterns\n    fetch_repositories,  # Fetch repos from GitHub\n    Config,              # Configuration class\n)\n</code></pre>"},{"location":"user-guide/api/#core-functions","title":"Core Functions","text":""},{"location":"user-guide/api/#is_green_aware","title":"is_green_aware()","text":"<p>Check if a commit message indicates green software awareness.</p> <pre><code>def is_green_aware(commit_message: str) -&gt; bool\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>commit_message</code> str The commit message to analyze <p>Returns: <code>bool</code> - True if message contains green keywords</p> <p>Example:</p> <pre><code>from greenmining import is_green_aware\n\n# Returns True\nis_green_aware(\"Optimize Redis caching for better performance\")\nis_green_aware(\"Enable gzip compression on API responses\")\nis_green_aware(\"Implement async batch processing\")\n\n# Returns False\nis_green_aware(\"Fix typo in README\")\nis_green_aware(\"Update dependencies\")\nis_green_aware(\"Refactor variable names\")\n</code></pre>"},{"location":"user-guide/api/#get_pattern_by_keywords","title":"get_pattern_by_keywords()","text":"<p>Find GSF patterns that match a commit message.</p> <pre><code>def get_pattern_by_keywords(commit_message: str) -&gt; list[str]\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>commit_message</code> str The commit message to analyze <p>Returns: <code>list[str]</code> - List of matched pattern names</p> <p>Example:</p> <pre><code>from greenmining import get_pattern_by_keywords\n\npatterns = get_pattern_by_keywords(\"Implement Redis caching layer\")\nprint(patterns)\n# Output: ['Cache Static Data']\n\npatterns = get_pattern_by_keywords(\"Enable gzip compression for API responses\")\nprint(patterns)\n# Output: ['Compress Transmitted Data', 'Enable Text Compression']\n\npatterns = get_pattern_by_keywords(\"Fix typo\")\nprint(patterns)\n# Output: []\n</code></pre>"},{"location":"user-guide/api/#fetch_repositories","title":"fetch_repositories()","text":"<p>Fetch repositories from GitHub matching search criteria.</p> <pre><code>def fetch_repositories(\n    github_token: str,\n    max_repos: int = 100,\n    min_stars: int = 100,\n    languages: list = None,\n    keywords: str = \"microservices\"\n) -&gt; list\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>github_token</code> str (required) GitHub personal access token <code>max_repos</code> int 100 Maximum repositories to fetch <code>min_stars</code> int 100 Minimum star count <code>languages</code> list None Filter by languages <code>keywords</code> str \"microservices\" Search keywords <p>Returns: <code>list</code> - List of Repository objects</p> <p>Example:</p> <pre><code>from greenmining import fetch_repositories\n\nrepos = fetch_repositories(\n    github_token=\"ghp_xxxx\",\n    max_repos=10,\n    min_stars=500,\n    keywords=\"kubernetes\",\n    languages=[\"Python\", \"Go\"]\n)\n\nfor repo in repos:\n    print(f\"{repo.full_name}: {repo.stars} stars\")\n</code></pre>"},{"location":"user-guide/api/#data-structures","title":"Data Structures","text":""},{"location":"user-guide/api/#gsf_patterns","title":"GSF_PATTERNS","text":"<p>Dictionary containing all 122 Green Software Foundation patterns.</p> <pre><code>from greenmining import GSF_PATTERNS\n\n# Structure\nGSF_PATTERNS = {\n    \"pattern_id\": {\n        \"name\": \"Pattern Name\",\n        \"category\": \"category_name\",\n        \"keywords\": [\"keyword1\", \"keyword2\"],\n        \"description\": \"Pattern description\",\n        \"sci_impact\": \"Impact on software carbon intensity\"\n    },\n    ...\n}\n</code></pre> <p>Example Usage:</p> <pre><code>from greenmining import GSF_PATTERNS\n\n# Get pattern count\nprint(f\"Total patterns: {len(GSF_PATTERNS)}\")  # 122\n\n# Get all categories\ncategories = set(p[\"category\"] for p in GSF_PATTERNS.values())\nprint(f\"Categories: {categories}\")\n# {'cloud', 'web', 'ai', 'caching', 'async', 'database', ...}\n\n# Find patterns by category\ncloud_patterns = [\n    p[\"name\"] for p in GSF_PATTERNS.values() \n    if p[\"category\"] == \"cloud\"\n]\nprint(f\"Cloud patterns: {len(cloud_patterns)}\")  # 40+\n\n# Get pattern details\ncache_pattern = GSF_PATTERNS[\"cache_static_data\"]\nprint(f\"Name: {cache_pattern['name']}\")\nprint(f\"Keywords: {cache_pattern['keywords']}\")\n</code></pre>"},{"location":"user-guide/api/#green_keywords","title":"GREEN_KEYWORDS","text":"<p>List of 321 keywords indicating green software practices.</p> <pre><code>from greenmining import GREEN_KEYWORDS\n\nprint(f\"Total keywords: {len(GREEN_KEYWORDS)}\")  # 321\n\n# Sample keywords\nprint(GREEN_KEYWORDS[:10])\n# ['energy', 'power', 'carbon', 'emission', 'footprint', \n#  'sustainability', 'sustainable', 'green', 'efficient', 'efficiency']\n</code></pre>"},{"location":"user-guide/api/#analyze_repositories","title":"analyze_repositories()","text":"<p>Analyze multiple repositories from URLs with optional parallel processing.</p> <pre><code>def analyze_repositories(\n    urls: list,\n    max_commits: int = 500,\n    parallel_workers: int = 1,\n    output_format: str = \"dict\",\n    energy_tracking: bool = False,\n    energy_backend: str = \"rapl\",\n    method_level_analysis: bool = False,\n    include_source_code: bool = False,\n    ssh_key_path: str = None,\n    github_token: str = None,\n) -&gt; list\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>urls</code> list (required) List of GitHub repository URLs <code>max_commits</code> int 500 Maximum commits per repository <code>parallel_workers</code> int 1 Concurrent analysis workers <code>energy_tracking</code> bool False Enable energy measurement <code>energy_backend</code> str \"rapl\" Energy backend (rapl, codecarbon, cpu_meter, auto) <code>method_level_analysis</code> bool False Include per-method metrics <code>include_source_code</code> bool False Include source code before/after <code>ssh_key_path</code> str None SSH key for private repos <code>github_token</code> str None GitHub token for private HTTPS repos <p>Example:</p> <pre><code>from greenmining import analyze_repositories\n\nresults = analyze_repositories(\n    urls=[\n        \"https://github.com/kubernetes/kubernetes\",\n        \"https://github.com/istio/istio\",\n    ],\n    max_commits=100,\n    parallel_workers=4,\n    energy_tracking=True,\n    energy_backend=\"auto\",\n)\n\nfor result in results:\n    print(f\"{result.name}: {result.green_commit_rate:.1%} green\")\n</code></pre>"},{"location":"user-guide/api/#service-classes","title":"Service Classes","text":""},{"location":"user-guide/api/#dataanalyzer","title":"DataAnalyzer","text":"<p>Analyze commits for green software patterns.</p> <pre><code>from greenmining.services.data_analyzer import DataAnalyzer\n\nanalyzer = DataAnalyzer(\n    enable_diff_analysis=False,  # Analyze code diffs\n    patterns=None,               # Custom patterns (default: GSF_PATTERNS)\n    batch_size=10                # Commits per batch\n)\n</code></pre> <p>Methods:</p> <pre><code># Analyze a single commit\nresult = analyzer.analyze_commit(commit_dict)\n\n# Analyze multiple commits\nresults = analyzer.analyze_commits(commits_list)\n\n# Save results to file\nanalyzer.save_results(results, \"output.json\")\n</code></pre> <p>Example:</p> <pre><code>from greenmining.services.data_analyzer import DataAnalyzer\n\nanalyzer = DataAnalyzer(enable_diff_analysis=True)\n\ncommit = {\n    \"sha\": \"abc123\",\n    \"message\": \"Implement Redis caching for user sessions\",\n    \"author\": \"developer\",\n    \"date\": \"2024-01-15T10:00:00Z\"\n}\n\nresult = analyzer.analyze_commit(commit)\nprint(f\"Green-aware: {result['green_aware']}\")\nprint(f\"Patterns: {result['patterns']}\")\n</code></pre>"},{"location":"user-guide/api/#dataaggregator","title":"DataAggregator","text":"<p>Aggregate analysis results with statistics.</p> <pre><code>from greenmining.services.data_aggregator import DataAggregator\n\naggregator = DataAggregator(\n    config=None,                  # Config object\n    enable_stats=True,            # Statistical analysis\n    enable_temporal=True,         # Temporal trends\n    temporal_granularity=\"quarter\"  # day/week/month/quarter/year\n)\n</code></pre> <p>Methods:</p> <pre><code># Aggregate results\naggregated = aggregator.aggregate(analysis_results, repositories)\n\n# Save to files\naggregator.save_results(aggregated, \"stats.json\", \"stats.csv\", analysis_results)\n\n# Print summary\naggregator.print_summary(aggregated)\n</code></pre> <p>Example:</p> <pre><code>from greenmining.services.data_aggregator import DataAggregator\n\naggregator = DataAggregator(\n    enable_stats=True,\n    enable_temporal=True,\n    temporal_granularity=\"month\"\n)\n\n# Assuming analysis_results and repositories are already populated\naggregated = aggregator.aggregate(analysis_results, repositories)\n\nprint(f\"Total commits: {aggregated['summary']['total_commits']}\")\nprint(f\"Green-aware: {aggregated['summary']['green_aware_percentage']}%\")\n</code></pre>"},{"location":"user-guide/api/#localrepoanalyzer","title":"LocalRepoAnalyzer","text":"<p>Analyze repositories directly from GitHub URLs.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/greenmining_repos\",  # Clone directory\n    max_commits=500,                       # Max commits per repo\n    days_back=730,                         # How far back to analyze\n    skip_merges=True,                      # Skip merge commits\n    compute_process_metrics=True,          # Compute process metrics\n    cleanup_after=True,                    # Delete after analysis\n    ssh_key_path=None,                     # SSH key for private repos\n    github_token=None,                     # GitHub token for private repos\n    energy_tracking=False,                 # Enable energy measurement\n    energy_backend=\"rapl\",                 # Energy backend\n    method_level_analysis=False,           # Per-method metrics\n    include_source_code=False,             # Source code before/after\n    process_metrics=\"standard\",            # \"standard\" or \"full\"\n)\n</code></pre> <p>Methods:</p> <pre><code># Analyze single repository\nresult = analyzer.analyze_repository(\"https://github.com/owner/repo\")\n\n# Analyze multiple repositories (with parallelism)\nresults = analyzer.analyze_repositories(\n    urls=[\"https://github.com/org/repo1\", \"https://github.com/org/repo2\"],\n    parallel_workers=4,\n)\n</code></pre> <p>Example: Basic analysis</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\nresult = analyzer.analyze_repository(\"https://github.com/pallets/flask\")\n\nprint(f\"Repository: {result.name}\")\nprint(f\"Commits analyzed: {result.total_commits}\")\nprint(f\"Green-aware: {result.green_commits} ({result.green_commit_rate:.1%})\")\n\nfor commit in result.commits[:5]:\n    if commit.green_aware:\n        print(f\"  {commit.message[:50]}...\")\n</code></pre> <p>Example: Private repository with energy tracking</p> <pre><code>analyzer = LocalRepoAnalyzer(\n    github_token=\"ghp_xxxx\",\n    energy_tracking=True,\n    energy_backend=\"auto\",\n    method_level_analysis=True,\n)\n\nresult = analyzer.analyze_repository(\"https://github.com/company/private-repo\")\nprint(f\"Energy consumed: {result.energy_metrics['joules']:.2f} J\")\n\nfor commit in result.commits:\n    for method in commit.methods:\n        print(f\"  {method.name}: complexity={method.complexity}\")\n</code></pre> <p>Example: Batch parallel analysis</p> <pre><code>analyzer = LocalRepoAnalyzer(max_commits=100)\nresults = analyzer.analyze_repositories(\n    urls=[\n        \"https://github.com/kubernetes/kubernetes\",\n        \"https://github.com/istio/istio\",\n        \"https://github.com/envoyproxy/envoy\",\n    ],\n    parallel_workers=3,\n)\n\nfor result in results:\n    print(f\"{result.name}: {result.green_commit_rate:.1%} green\")\n</code></pre>"},{"location":"user-guide/api/#reportgenerator","title":"ReportGenerator","text":"<p>Generate Markdown reports from analysis results.</p> <pre><code>from greenmining.services.reports import ReportGenerator\n\ngenerator = ReportGenerator()\n</code></pre> <p>Methods:</p> <pre><code># Generate full report\nreport = generator.generate_report(aggregated_data)\n\n# Save to file\ngenerator.save_report(report, \"report.md\")\n</code></pre>"},{"location":"user-guide/api/#analyzer-classes","title":"Analyzer Classes","text":""},{"location":"user-guide/api/#statisticalanalyzer","title":"StatisticalAnalyzer","text":"<p>Compute statistical metrics on analysis results.</p> <pre><code>from greenmining.analyzers.statistical_analyzer import StatisticalAnalyzer\n\nanalyzer = StatisticalAnalyzer()\n\n# Pattern correlations\ncorrelations = analyzer.analyze_pattern_correlations(analysis_results)\n\n# Effect sizes\neffect_sizes = analyzer.analyze_effect_sizes(analysis_results)\n\n# Descriptive statistics\ndescriptive = analyzer.get_descriptive_statistics(analysis_results)\n</code></pre>"},{"location":"user-guide/api/#temporalanalyzer","title":"TemporalAnalyzer","text":"<p>Analyze patterns over time.</p> <pre><code>from greenmining.analyzers.temporal_analyzer import TemporalAnalyzer\n\nanalyzer = TemporalAnalyzer(granularity=\"quarter\")\n\n# Group commits by period\nperiods = analyzer.group_commits_by_period(commits)\n\n# Analyze trends\ntrends = analyzer.analyze_trends(periods)\n\n# Pattern evolution\nevolution = analyzer.analyze_pattern_evolution(commits)\n</code></pre>"},{"location":"user-guide/api/#qualitativeanalyzer","title":"QualitativeAnalyzer","text":"<p>Generate validation samples for manual review.</p> <pre><code>from greenmining.analyzers.qualitative_analyzer import QualitativeAnalyzer\n\nanalyzer = QualitativeAnalyzer(\n    sample_size=30,\n    stratify_by=\"pattern\"  # pattern/repository/time/random\n)\n\n# Generate samples\nsamples = analyzer.generate_validation_samples(analysis_results)\n\n# Export for review\nanalyzer.export_samples_for_review(samples, \"validation_samples.csv\")\n</code></pre>"},{"location":"user-guide/api/#codediffanalyzer","title":"CodeDiffAnalyzer","text":"<p>Analyze code changes for green patterns.</p> <pre><code>from greenmining.analyzers.code_diff_analyzer import CodeDiffAnalyzer\n\nanalyzer = CodeDiffAnalyzer()\n\n# Check if file is analyzable\nis_code = analyzer.is_code_file(\"app.py\")  # True\n\n# Analyze diff content\npatterns = analyzer.detect_patterns_in_diff(diff_text)\n</code></pre>"},{"location":"user-guide/api/#powerregressiondetector","title":"PowerRegressionDetector","text":"<p>Identify commits that caused power consumption regressions by running a test command at each commit and measuring energy usage.</p> <pre><code>from greenmining.analyzers import PowerRegressionDetector\n\ndetector = PowerRegressionDetector(\n    test_command=\"pytest tests/ -x\",\n    energy_backend=\"rapl\",\n    threshold_percent=5.0,\n    iterations=5,\n    warmup_iterations=1,\n)\n\nregressions = detector.detect(\n    repo_path=\"/path/to/repo\",\n    baseline_commit=\"v1.0.0\",\n    target_commit=\"HEAD\",\n)\n\nfor regression in regressions:\n    print(f\"Commit {regression.sha[:8]}: +{regression.power_increase:.1f}%\")\n    print(f\"  Message: {regression.message}\")\n</code></pre> <p>Constructor Parameters:</p> Parameter Type Default Description <code>test_command</code> str <code>\"pytest tests/ -x\"</code> Shell command to run for measurement <code>energy_backend</code> str <code>\"rapl\"</code> Energy backend (rapl, codecarbon, cpu_meter, auto) <code>threshold_percent</code> float <code>5.0</code> Minimum % increase to flag as regression <code>iterations</code> int <code>5</code> Measurement iterations per commit <code>warmup_iterations</code> int <code>1</code> Warmup runs before measuring <p>PowerRegression Output:</p> Field Type Description <code>sha</code> str Commit SHA <code>message</code> str Commit message <code>author</code> str Commit author <code>date</code> str Commit date <code>power_before</code> float Average power before (watts) <code>power_after</code> float Average power after (watts) <code>power_increase</code> float Percentage increase <code>energy_before</code> float Energy before (joules) <code>energy_after</code> float Energy after (joules) <code>is_regression</code> bool True if above threshold"},{"location":"user-guide/api/#metricspowercorrelator","title":"MetricsPowerCorrelator","text":"<p>Correlate code metrics (complexity, NLOC, churn) with power consumption using Pearson and Spearman correlation coefficients.</p> <pre><code>from greenmining.analyzers import MetricsPowerCorrelator\n\ncorrelator = MetricsPowerCorrelator(significance_level=0.05)\ncorrelator.fit(\n    metrics=[\"complexity\", \"nloc\", \"code_churn\"],\n    metrics_values={\n        \"complexity\": [...],\n        \"nloc\": [...],\n        \"code_churn\": [...],\n    },\n    power_measurements=[...],\n)\n\n# Access results\nfor name, result in correlator.get_results().items():\n    print(f\"{name}: pearson={result.pearson_r:.3f}, spearman={result.spearman_r:.3f}\")\n    print(f\"  Significant: {result.significant}, Strength: {result.strength}\")\n\n# Feature importance ranking\nfor name, importance in correlator.feature_importance.items():\n    print(f\"{name}: {importance:.3f}\")\n</code></pre> <p>Constructor Parameters:</p> Parameter Type Default Description <code>significance_level</code> float <code>0.05</code> P-value threshold for significance <p>CorrelationResult Output:</p> Field Type Description <code>metric_name</code> str Name of the metric <code>pearson_r</code> float Pearson correlation coefficient <code>pearson_p</code> float Pearson p-value <code>spearman_r</code> float Spearman rank correlation <code>spearman_p</code> float Spearman p-value <code>significant</code> bool True if p &lt; significance_level <code>strength</code> str none, weak, moderate, strong"},{"location":"user-guide/api/#versionpoweranalyzer","title":"VersionPowerAnalyzer","text":"<p>Compare energy consumption across software versions by checking out tags/branches and running a test suite at each version.</p> <pre><code>from greenmining.analyzers import VersionPowerAnalyzer\n\nanalyzer = VersionPowerAnalyzer(\n    test_command=\"pytest tests/\",\n    energy_backend=\"rapl\",\n    iterations=10,\n    warmup_iterations=2,\n)\n\nreport = analyzer.analyze_versions(\n    repo_path=\"/path/to/repo\",\n    versions=[\"v1.0\", \"v1.1\", \"v1.2\", \"v2.0\"],\n)\n\nprint(report.summary())\nprint(f\"Trend: {report.trend}\")            # increasing, decreasing, stable\nprint(f\"Most efficient: {report.most_efficient}\")\nprint(f\"Total change: {report.total_change_percent:.1f}%\")\n\nfor v in report.versions:\n    print(f\"  {v.version}: {v.power_watts_avg:.2f}W (std: {v.energy_std:.4f})\")\n</code></pre> <p>Constructor Parameters:</p> Parameter Type Default Description <code>test_command</code> str <code>\"pytest tests/\"</code> Shell command to run per version <code>energy_backend</code> str <code>\"rapl\"</code> Energy backend <code>iterations</code> int <code>5</code> Measurement iterations per version <code>warmup_iterations</code> int <code>1</code> Warmup runs before measuring <p>VersionPowerReport Output:</p> Field Type Description <code>versions</code> list List of VersionPowerProfile objects <code>trend</code> str increasing, decreasing, or stable <code>total_change_percent</code> float % change from first to last version <code>most_efficient</code> str Version tag with lowest power <code>least_efficient</code> str Version tag with highest power <p>VersionPowerProfile Fields:</p> Field Type Description <code>version</code> str Version tag or branch name <code>commit_sha</code> str Resolved commit SHA <code>energy_joules</code> float Average energy per run <code>power_watts_avg</code> float Average power draw <code>duration_seconds</code> float Average test duration <code>iterations</code> int Number of measurement iterations <code>energy_std</code> float Standard deviation across iterations"},{"location":"user-guide/api/#carbonreporter","title":"CarbonReporter","text":"<p>Generate carbon footprint reports from energy measurements. Supports 20+ countries and major cloud providers (AWS, GCP, Azure).</p> <pre><code>from greenmining.energy import CarbonReporter\n\nreporter = CarbonReporter(\n    country_iso=\"USA\",\n    cloud_provider=\"aws\",\n    region=\"us-east-1\",\n)\n\nreport = reporter.generate_report(total_joules=1000.0)\nprint(f\"CO2 emissions: {report.total_emissions_kg:.4f} kg\")\nprint(f\"Equivalent: {report.tree_months:.1f} tree-months\")\nprint(report.summary())\n</code></pre>"},{"location":"user-guide/api/#configuration-class","title":"Configuration Class","text":"<pre><code>from greenmining.config import Config\n\nconfig = Config()\n\n# Access configuration values\nprint(config.MAX_REPOS)           # 100\nprint(config.COMMITS_PER_REPO)    # 1000\nprint(config.SUPPORTED_LANGUAGES) # ['Python', 'Java', ...]\nprint(config.OUTPUT_DIR)          # 'data'\n\n# URL Analysis options\nprint(config.REPOSITORY_URLS)     # []\nprint(config.CLONE_PATH)          # '/tmp/greenmining_repos'\n\n# Energy options\nprint(config.ENERGY_ENABLED)      # False\nprint(config.ENERGY_BACKEND)      # 'rapl'\n\n# Process metric options\nprint(config.PROCESS_METRICS_ENABLED)  # True\nprint(config.DMM_ENABLED)              # True\n</code></pre>"},{"location":"user-guide/api/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Complete GreenMining analysis workflow.\"\"\"\n\nfrom greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom greenmining.services.data_aggregator import DataAggregator\nfrom greenmining.services.reports import ReportGenerator\n\n# 1. Analyze repository\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=200\n)\n\nprint(f\"Analyzed {result['total_commits']} commits\")\nprint(f\"Green-aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\n# 2. Aggregate results\naggregator = DataAggregator(enable_stats=True, enable_temporal=True)\naggregated = aggregator.aggregate(\n    analysis_results=result['commits'],\n    repositories=[result['repository']]\n)\n\n# 3. Generate report\ngenerator = ReportGenerator()\nreport = generator.generate_report(aggregated)\ngenerator.save_report(report, \"flask_analysis.md\")\n\nprint(\"Report saved to flask_analysis.md\")\n</code></pre>"},{"location":"user-guide/api/#next-steps","title":"Next Steps","text":"<ul> <li>URL Analysis - Deep dive into URL-based analysis</li> <li>Energy Measurement - Power profiling with RAPL/CodeCarbon</li> <li>GSF Patterns Reference - All 122 patterns</li> </ul>"},{"location":"user-guide/dashboard/","title":"Web Dashboard","text":"<p>Interactive web dashboard for exploring GreenMining analysis results.</p>"},{"location":"user-guide/dashboard/#overview","title":"Overview","text":"<p>GreenMining includes a Flask-based dashboard that reads analysis data from a directory and presents it through a web UI with REST API endpoints. Install with:</p> <pre><code>pip install greenmining[dashboard]\n</code></pre>"},{"location":"user-guide/dashboard/#quick-start","title":"Quick Start","text":"<pre><code>from greenmining.dashboard import create_app, run_dashboard\n\n# Option 1: Run directly\nrun_dashboard(data_dir=\"./data\", host=\"127.0.0.1\", port=5000)\n\n# Option 2: Get the Flask app (for custom configuration)\napp = create_app(data_dir=\"./data\")\napp.run(host=\"127.0.0.1\", port=5000)\n</code></pre> <p>Then open <code>http://127.0.0.1:5000</code> in your browser.</p>"},{"location":"user-guide/dashboard/#functions","title":"Functions","text":""},{"location":"user-guide/dashboard/#create_app","title":"create_app()","text":"<p>Create a Flask application instance.</p> <pre><code>def create_app(data_dir: str = \"./data\") -&gt; Flask\n</code></pre> Parameter Type Default Description <code>data_dir</code> str <code>\"./data\"</code> Path to directory containing analysis JSON files <p>Returns: Flask application instance.</p> <p>The app looks for these JSON files in <code>data_dir</code>:</p> File Used By <code>repositories.json</code> <code>/api/repositories</code> <code>analysis_results.json</code> <code>/api/analysis</code>, <code>/api/summary</code> <code>aggregated_statistics.json</code> <code>/api/statistics</code> <code>energy_report.json</code> <code>/api/energy</code> <p>Files are loaded on each request, so you can update them while the dashboard is running.</p>"},{"location":"user-guide/dashboard/#run_dashboard","title":"run_dashboard()","text":"<p>Start the dashboard server.</p> <pre><code>def run_dashboard(\n    data_dir: str = \"./data\",\n    host: str = \"127.0.0.1\",\n    port: int = 5000,\n) -&gt; None\n</code></pre> Parameter Type Default Description <code>data_dir</code> str <code>\"./data\"</code> Path to analysis data directory <code>host</code> str <code>\"127.0.0.1\"</code> Host to bind to <code>port</code> int <code>5000</code> Port to bind to"},{"location":"user-guide/dashboard/#rest-api-endpoints","title":"REST API Endpoints","text":"<p>All endpoints return JSON responses.</p>"},{"location":"user-guide/dashboard/#get","title":"GET /","text":"<p>Returns the dashboard HTML page with embedded JavaScript. The page fetches data from the API endpoints below and renders summary cards and a repository table.</p>"},{"location":"user-guide/dashboard/#get-apirepositories","title":"GET /api/repositories","text":"<p>Returns repository data from <code>repositories.json</code>.</p> <p>Response:</p> <pre><code>{\n  \"total_repositories\": 13,\n  \"repositories\": [\n    {\n      \"name\": \"flask\",\n      \"full_name\": \"pallets/flask\",\n      \"language\": \"Python\",\n      \"stars\": 68000,\n      \"description\": \"The Python micro framework...\"\n    }\n  ]\n}\n</code></pre>"},{"location":"user-guide/dashboard/#get-apianalysis","title":"GET /api/analysis","text":"<p>Returns analysis results from <code>analysis_results.json</code>.</p> <p>Response: Full analysis data including per-repository results, commits, patterns matched, and metrics.</p>"},{"location":"user-guide/dashboard/#get-apistatistics","title":"GET /api/statistics","text":"<p>Returns aggregated statistics from <code>aggregated_statistics.json</code>.</p> <p>Response: Statistical summaries including pattern distributions, temporal trends, and correlation data.</p>"},{"location":"user-guide/dashboard/#get-apienergy","title":"GET /api/energy","text":"<p>Returns energy measurement data from <code>energy_report.json</code>.</p> <p>Response: Energy consumption metrics, carbon footprint data, and backend information.</p>"},{"location":"user-guide/dashboard/#get-apisummary","title":"GET /api/summary","text":"<p>Computed on-the-fly from <code>repositories.json</code> and <code>analysis_results.json</code>.</p> <p>Response:</p> <pre><code>{\n  \"repositories\": 13,\n  \"commits_analyzed\": 260,\n  \"green_commits\": 42,\n  \"green_rate\": 16.2\n}\n</code></pre>"},{"location":"user-guide/dashboard/#data-preparation","title":"Data Preparation","text":"<p>The dashboard reads JSON files produced by the analysis pipeline. To populate the data directory, run an analysis first:</p> <pre><code>from greenmining import analyze_repositories\nimport json\n\nresults = analyze_repositories(\n    urls=[\"https://github.com/pallets/flask\"],\n    max_commits=100,\n    energy_tracking=True,\n)\n\n# Save for the dashboard\nwith open(\"data/analysis_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2, default=str)\n</code></pre> <p>Or use the experiment notebook which exports all required files automatically.</p>"},{"location":"user-guide/dashboard/#integration-with-jupyter","title":"Integration with Jupyter","text":"<p>The dashboard app can be created inside a notebook for inspection, but should be run from a terminal for actual use:</p> <pre><code># In a notebook cell (inspect only)\nfrom greenmining.dashboard import create_app\napp = create_app(data_dir=\"./data\")\nprint(\"Dashboard created. Run from terminal:\")\nprint(\"  from greenmining.dashboard import run_dashboard\")\nprint('  run_dashboard(data_dir=\"./data\")')\n</code></pre> <pre><code># From terminal\npython -c \"from greenmining.dashboard import run_dashboard; run_dashboard()\"\n</code></pre>"},{"location":"user-guide/dashboard/#next-steps","title":"Next Steps","text":"<ul> <li>Python API - Full API reference</li> <li>Energy Measurement - Energy backends and carbon reporting</li> <li>Experiment - Full pipeline walkthrough</li> </ul>"},{"location":"user-guide/energy/","title":"Energy Measurement","text":"<p>Measure energy consumption of your analysis workloads with RAPL and CodeCarbon.</p>"},{"location":"user-guide/energy/#overview","title":"Overview","text":"<p>GreenMining includes energy measurement capabilities to profile the power consumption of analysis operations. This is useful for:</p> <ul> <li>Research - Quantify energy cost of mining operations</li> <li>Optimization - Identify energy-intensive analysis steps</li> <li>Reporting - Include energy metrics in analysis reports</li> </ul>"},{"location":"user-guide/energy/#supported-backends","title":"Supported Backends","text":"Backend Platform Features RAPL Linux (Intel/AMD) Direct CPU/DRAM power reading CodeCarbon Cross-platform Emissions tracking, cloud support CPU Energy Meter All platforms Utilization-based estimation Auto All platforms RAPL if available, else CPU Meter"},{"location":"user-guide/energy/#rapl-backend","title":"RAPL Backend","text":"<p>Intel's Running Average Power Limit (RAPL) provides direct power measurements on Linux systems with Intel or AMD processors.</p>"},{"location":"user-guide/energy/#requirements","title":"Requirements","text":"<ul> <li>Linux operating system</li> <li>Intel Core 2nd generation+ or AMD Ryzen</li> <li>Read access to <code>/sys/class/powercap/intel-rapl/</code></li> </ul>"},{"location":"user-guide/energy/#checking-availability","title":"Checking Availability","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\nif meter.is_available():\n    print(\"RAPL is available on this system\")\nelse:\n    print(\"RAPL not available - try running as root\")\n</code></pre>"},{"location":"user-guide/energy/#basic-usage","title":"Basic Usage","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\n\n# Start measurement\nmeter.start()\n\n# Your workload here\nresult = expensive_computation()\n\n# Stop and get metrics\nmetrics = meter.stop()\n\nprint(f\"Energy consumed: {metrics.energy_joules:.2f} J\")\nprint(f\"Duration: {metrics.duration_seconds:.2f} s\")\nprint(f\"Average power: {metrics.average_power_watts:.2f} W\")\n</code></pre>"},{"location":"user-guide/energy/#with-context-manager","title":"With Context Manager","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\n\nwith meter.measure() as measurement:\n    # Your analysis code\n    analyzer.analyze_repository(repo_url)\n\nprint(f\"Analysis consumed {measurement.energy_joules:.2f} J\")\n</code></pre>"},{"location":"user-guide/energy/#permission-setup","title":"Permission Setup","text":"<p>RAPL typically requires root access. To allow non-root users:</p> <pre><code># Grant read access to RAPL files\nsudo chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\nsudo chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/intel-rapl:0:*/energy_uj\n\n# Or create a udev rule for persistent access\necho 'SUBSYSTEM==\"powercap\", ACTION==\"add\", RUN+=\"/bin/chmod a+r /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\"' | sudo tee /etc/udev/rules.d/99-rapl.rules\n</code></pre>"},{"location":"user-guide/energy/#codecarbon-backend","title":"CodeCarbon Backend","text":"<p>CodeCarbon tracks energy consumption and CO2 emissions across platforms.</p>"},{"location":"user-guide/energy/#installation","title":"Installation","text":"<pre><code>pip install codecarbon\n</code></pre>"},{"location":"user-guide/energy/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from greenmining.energy.codecarbon_meter import CodeCarbonMeter\n\nmeter = CodeCarbonMeter(\n    country_iso_code=\"USA\",  # For carbon intensity\n    project_name=\"greenmining-analysis\",\n    tracking_mode=\"process\"  # or \"machine\"\n)\n\n# Start tracking\nmeter.start()\n\n# Your workload\nresult = analyzer.analyze_repository(repo_url)\n\n# Stop and get metrics\nmetrics = meter.stop()\n\nprint(f\"Energy: {metrics.energy_kwh:.6f} kWh\")\nprint(f\"CO2 emissions: {metrics.emissions_kg:.6f} kg\")\nprint(f\"Duration: {metrics.duration_seconds:.2f} s\")\n</code></pre>"},{"location":"user-guide/energy/#configuration-options","title":"Configuration Options","text":"<pre><code>meter = CodeCarbonMeter(\n    country_iso_code=\"FRA\",      # France\n    region=\"ile-de-france\",       # Optional region\n    project_name=\"my-analysis\",\n    output_dir=\"./energy_logs\",   # Where to save logs\n    save_to_file=True,            # Save detailed logs\n    tracking_mode=\"process\"       # process or machine\n)\n</code></pre>"},{"location":"user-guide/energy/#carbon-tracking","title":"Carbon Tracking","text":"<pre><code>from greenmining.energy.codecarbon_meter import CodeCarbonMeter\n\nmeter = CodeCarbonMeter(project_name=\"greenmining\")\n\nmeter.start()\n# ... analysis ...\nmetrics = meter.stop()\n\nprint(f\"Energy: {metrics.energy_joules:.2f} J\")\nprint(f\"Carbon footprint: {metrics.carbon_grams:.4f} g CO2\")\n</code></pre>"},{"location":"user-guide/energy/#cpu-energy-meter","title":"CPU Energy Meter","text":"<p>Cross-platform energy estimation using CPU utilization and TDP modeling.</p>"},{"location":"user-guide/energy/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from greenmining.energy import CPUEnergyMeter\n\nmeter = CPUEnergyMeter()\nmeter.start()\n\n# Your workload\nresult = expensive_computation()\n\nmetrics = meter.stop()\nprint(f\"Estimated energy: {metrics.joules:.2f} J\")\nprint(f\"Average power: {metrics.watts_avg:.2f} W\")\n</code></pre>"},{"location":"user-guide/energy/#custom-tdp","title":"Custom TDP","text":"<pre><code>from greenmining.energy import CPUEnergyMeter\n\n# Specify your CPU's TDP for more accurate estimation\nmeter = CPUEnergyMeter(tdp_watts=45.0)  # e.g., laptop CPU\n</code></pre>"},{"location":"user-guide/energy/#auto-detection","title":"Auto-Detection","text":"<pre><code>from greenmining.energy import get_energy_meter\n\n# Automatically selects RAPL (if available) or CPU Meter\nmeter = get_energy_meter(\"auto\")\nmeter.start()\n# ... workload ...\nmetrics = meter.stop()\n</code></pre>"},{"location":"user-guide/energy/#integrated-energy-tracking","title":"Integrated Energy Tracking","text":"<p>Automatically measure energy during repository analysis.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    energy_tracking=True,\n    energy_backend=\"auto\",  # rapl, codecarbon, cpu_meter, auto\n)\n\nresult = analyzer.analyze_repository(\"https://github.com/pallets/flask\")\nprint(f\"Energy consumed: {result.energy_metrics['joules']:.2f} J\")\nprint(f\"Average power: {result.energy_metrics['watts_avg']:.2f} W\")\nprint(f\"Duration: {result.energy_metrics['duration_seconds']:.2f} s\")\n</code></pre>"},{"location":"user-guide/energy/#carbon-footprint-reporting","title":"Carbon Footprint Reporting","text":"<p>Convert energy measurements to CO2 emissions using regional carbon intensity data.</p>"},{"location":"user-guide/energy/#basic-usage_3","title":"Basic Usage","text":"<pre><code>from greenmining.energy import CarbonReporter\n\nreporter = CarbonReporter(country_iso=\"USA\")\nreport = reporter.generate_report(total_joules=3600.0)\n\nprint(f\"CO2: {report.total_emissions_kg * 1000:.4f} grams\")\nprint(f\"Equivalent to {report.tree_months:.2f} tree-months\")\nprint(f\"Or {report.smartphone_charges:.1f} smartphone charges\")\n</code></pre>"},{"location":"user-guide/energy/#cloud-region-support","title":"Cloud Region Support","text":"<pre><code>from greenmining.energy import CarbonReporter\n\n# Use cloud provider region for accurate carbon intensity\nreporter = CarbonReporter(\n    country_iso=\"USA\",\n    cloud_provider=\"aws\",\n    region=\"eu-north-1\",  # Stockholm - low carbon (renewable energy)\n)\n\nreport = reporter.generate_report(total_joules=1000.0)\nprint(report.summary())\n</code></pre>"},{"location":"user-guide/energy/#supported-regions","title":"Supported Regions","text":"<pre><code>from greenmining.energy import CarbonReporter\n\n# 20+ country profiles\ncountries = CarbonReporter.get_supported_countries()\n\n# AWS, GCP, Azure regions\naws_regions = CarbonReporter.get_supported_cloud_regions(\"aws\")\ngcp_regions = CarbonReporter.get_supported_cloud_regions(\"gcp\")\n</code></pre>"},{"location":"user-guide/energy/#energy-metrics","title":"Energy Metrics","text":""},{"location":"user-guide/energy/#energymetrics-class","title":"EnergyMetrics Class","text":"<pre><code>from greenmining.energy.base import EnergyResult\n\n@dataclass\nclass EnergyResult:\n    energy_joules: float       # Total energy in Joules\n    duration_seconds: float    # Measurement duration\n    average_power_watts: float # Average power draw\n    start_time: datetime       # Measurement start\n    end_time: datetime         # Measurement end\n\n    # CodeCarbon specific\n    energy_kwh: float = 0.0    # Energy in kilowatt-hours\n    emissions_kg: float = 0.0  # CO2 emissions in kg\n</code></pre>"},{"location":"user-guide/energy/#commitenergyprofile","title":"CommitEnergyProfile","text":"<p>Track energy per commit analysis:</p> <pre><code>from greenmining.energy.base import CommitEnergyProfile\n\n@dataclass\nclass CommitEnergyProfile:\n    commit_sha: str\n    energy_joules: float\n    duration_seconds: float\n    patterns_detected: list\n    files_analyzed: int\n</code></pre>"},{"location":"user-guide/energy/#research-applications","title":"Research Applications","text":""},{"location":"user-guide/energy/#measuring-analysis-efficiency","title":"Measuring Analysis Efficiency","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nmeter = RAPLEnergyMeter()\nanalyzer = LocalRepoAnalyzer()\n\nrepos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n]\n\nresults = []\nfor url in repos:\n    meter.start()\n    analysis = analyzer.analyze_repository(url, max_commits=100)\n    energy = meter.stop()\n\n    results.append({\n        \"repo\": url,\n        \"commits\": analysis[\"total_commits\"],\n        \"energy_joules\": energy.energy_joules,\n        \"joules_per_commit\": energy.energy_joules / analysis[\"total_commits\"]\n    })\n\n# Compare efficiency\nfor r in results:\n    print(f\"{r['repo']}: {r['joules_per_commit']:.3f} J/commit\")\n</code></pre>"},{"location":"user-guide/energy/#energy-aware-batch-processing","title":"Energy-Aware Batch Processing","text":"<pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\n\nmeter = RAPLEnergyMeter()\nenergy_budget_joules = 1000  # Set energy budget\n\ntotal_energy = 0\nanalyzed_repos = []\n\nfor repo in repositories:\n    meter.start()\n    result = analyze(repo)\n    metrics = meter.stop()\n\n    total_energy += metrics.energy_joules\n    analyzed_repos.append(repo)\n\n    if total_energy &gt;= energy_budget_joules:\n        print(f\"Energy budget reached after {len(analyzed_repos)} repos\")\n        break\n</code></pre>"},{"location":"user-guide/energy/#comparing-backends","title":"Comparing Backends","text":"Feature RAPL CodeCarbon CPU Meter Accuracy High (hardware) Medium (estimation) Low (model) Platform Linux only Cross-platform All platforms Granularity Microseconds Seconds Seconds CO2 tracking No Yes Via CarbonReporter Cloud support No Yes Via CarbonReporter Setup May need root pip install No setup"},{"location":"user-guide/energy/#recommendation","title":"Recommendation","text":"<ul> <li>Use RAPL for precise measurements on Linux</li> <li>Use CodeCarbon for cross-platform and carbon tracking</li> <li>Use CPU Meter when neither RAPL nor CodeCarbon is available</li> <li>Use auto to let GreenMining pick the best available backend</li> </ul>"},{"location":"user-guide/energy/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/energy/#rapl-not-available","title":"RAPL Not Available","text":"<pre><code># Check if RAPL files exist\nimport os\nrapl_path = \"/sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\"\nprint(f\"RAPL exists: {os.path.exists(rapl_path)}\")\n\n# Check permissions\nif os.path.exists(rapl_path):\n    print(f\"Readable: {os.access(rapl_path, os.R_OK)}\")\n</code></pre>"},{"location":"user-guide/energy/#codecarbon-import-error","title":"CodeCarbon Import Error","text":"<pre><code># Install with all dependencies\npip install codecarbon[viz]\n\n# Or minimal install\npip install codecarbon\n</code></pre>"},{"location":"user-guide/energy/#virtual-machine-limitations","title":"Virtual Machine Limitations","text":"<p>RAPL typically doesn't work in VMs. Use CodeCarbon instead:</p> <pre><code>from greenmining.energy.rapl import RAPLEnergyMeter\nfrom greenmining.energy.codecarbon_meter import CodeCarbonMeter\n\n# Try RAPL first, fall back to CodeCarbon\nrapl = RAPLEnergyMeter()\nif rapl.is_available():\n    meter = rapl\nelse:\n    print(\"RAPL not available, using CodeCarbon\")\n    meter = CodeCarbonMeter()\n</code></pre>"},{"location":"user-guide/energy/#next-steps","title":"Next Steps","text":"<ul> <li>Python API - Full API reference</li> <li>URL Analysis - Analyze repositories by URL</li> <li>Configuration - Energy settings</li> </ul>"},{"location":"user-guide/url-analysis/","title":"URL Analysis","text":"<p>Analyze GitHub repositories directly by URL.</p>"},{"location":"user-guide/url-analysis/#overview","title":"Overview","text":"<p>URL analysis allows you to analyze any GitHub repository without using the GitHub API rate limits. GreenMining clones repositories locally and extracts commit data with full diff information.</p>"},{"location":"user-guide/url-analysis/#benefits","title":"Benefits","text":"<ul> <li>No GitHub API limits - Clone and analyze directly</li> <li>Full commit data - Access diffs, modified files, metrics</li> <li>Process metrics - Code churn, change set size, contributor count</li> <li>DMM metrics - Delta Maintainability Model scores</li> <li>Method-level analysis - Per-function complexity via Lizard</li> <li>Historical analysis - Analyze any date range</li> </ul>"},{"location":"user-guide/url-analysis/#python-api","title":"Python API","text":""},{"location":"user-guide/url-analysis/#localrepoanalyzer","title":"LocalRepoAnalyzer","text":"<p>The main class for URL-based analysis.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/greenmining_repos\",  # Where to clone\n    cleanup_after=True                     # Delete after analysis\n)\n</code></pre>"},{"location":"user-guide/url-analysis/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>clone_path</code> str /tmp/greenmining_repos Directory for cloning <code>cleanup_after</code> bool True Delete cloned repo after analysis"},{"location":"user-guide/url-analysis/#single-repository-analysis","title":"Single Repository Analysis","text":"<p>Analyze a single repository.</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nfrom datetime import datetime\n\nanalyzer = LocalRepoAnalyzer()\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=200,\n    since_date=datetime(2024, 1, 1),\n    to_date=datetime(2024, 12, 31)\n)\n\nprint(f\"Total commits: {result['total_commits']}\")\nprint(f\"Green-aware: {result['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"user-guide/url-analysis/#multiple-repositories","title":"Multiple Repositories","text":"<pre><code>repos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n    \"https://github.com/fastapi/fastapi\"\n]\n\nfor repo_url in repos:\n    result = analyzer.analyze_repository(repo_url, max_commits=100)\n    print(f\"{result['repository']['name']}: {result['green_aware_percentage']:.1f}%\")\n</code></pre>"},{"location":"user-guide/url-analysis/#analyze_repository-parameters","title":"analyze_repository() Parameters","text":"Parameter Type Default Description <code>repo_url</code> str (required) GitHub repository URL <code>max_commits</code> int 1000 Maximum commits to analyze <code>since_date</code> datetime None Start date filter <code>to_date</code> datetime None End date filter"},{"location":"user-guide/url-analysis/#return-value","title":"Return Value","text":"<pre><code>{\n    \"repository\": {\n        \"name\": \"flask\",\n        \"url\": \"https://github.com/pallets/flask\",\n        \"owner\": \"pallets\",\n        \"clone_path\": \"/tmp/greenmining_repos/flask\"\n    },\n    \"total_commits\": 200,\n    \"green_aware_count\": 47,\n    \"green_aware_percentage\": 23.5,\n    \"commits\": [\n        {\n            \"sha\": \"abc123...\",\n            \"message\": \"Optimize template caching\",\n            \"author\": \"developer\",\n            \"date\": \"2024-03-15T10:30:00\",\n            \"green_aware\": True,\n            \"patterns\": [\"Cache Static Data\"],\n            \"modified_files\": 3,\n            \"insertions\": 45,\n            \"deletions\": 12,\n            \"dmm_unit_size\": 0.85,\n            \"dmm_unit_complexity\": 0.72,\n            \"dmm_unit_interfacing\": 0.90\n        },\n        ...\n    ],\n    \"pattern_distribution\": {\n        \"Cache Static Data\": 15,\n        \"Use Async Instead of Sync\": 12,\n        ...\n    },\n    \"process_metrics\": {\n        \"change_set\": {\"max\": 25, \"avg\": 5.2},\n        \"code_churn\": {\"added\": 5000, \"removed\": 2000},\n        \"contributors_count\": 45\n    }\n}\n</code></pre>"},{"location":"user-guide/url-analysis/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Analyze Flask repository for green patterns.\"\"\"\n\nfrom datetime import datetime\nfrom greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n\n# Initialize analyzer\nanalyzer = LocalRepoAnalyzer(\n    clone_path=\"/tmp/flask_analysis\",\n    cleanup_after=True\n)\n\n# Analyze repository\nprint(\"Analyzing Flask repository...\")\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/pallets/flask\",\n    max_commits=100,\n    since_date=datetime(2024, 1, 1)\n)\n\n# Print summary\nprint(f\"\\n{'='*60}\")\nprint(\"ANALYSIS RESULTS\")\nprint(f\"{'='*60}\")\nprint(f\"Repository: {result['repository']['name']}\")\nprint(f\"Total commits: {result['total_commits']}\")\nprint(f\"Green-aware: {result['green_aware_count']} ({result['green_aware_percentage']:.1f}%)\")\n\n# Top patterns\nprint(f\"\\nTop Patterns:\")\nfor pattern, count in sorted(\n    result['pattern_distribution'].items(), \n    key=lambda x: x[1], \n    reverse=True\n)[:5]:\n    print(f\"  {pattern}: {count}\")\n\n# Sample green commits\nprint(f\"\\nSample Green Commits:\")\ngreen_commits = [c for c in result['commits'] if c['green_aware']]\nfor commit in green_commits[:5]:\n    print(f\"  \ud83c\udf31 {commit['message'][:60]}...\")\n    print(f\"     Patterns: {commit['patterns']}\")\n</code></pre> <p>Output:</p> <pre><code>Analyzing Flask repository...\n\n============================================================\nANALYSIS RESULTS\n============================================================\nRepository: flask\nTotal commits: 100\nGreen-aware: 23 (23.0%)\n\nTop Patterns:\n  Cache Static Data: 8\n  Use Async Instead of Sync: 5\n  Lazy Loading: 4\n  Compress Transmitted Data: 3\n  Optimize Database Queries: 3\n\nSample Green Commits:\n  \ud83c\udf31 Implement response caching for static assets...\n     Patterns: ['Cache Static Data']\n  \ud83c\udf31 Add async support for request handling...\n     Patterns: ['Use Async Instead of Sync']\n</code></pre>"},{"location":"user-guide/url-analysis/#supported-url-formats","title":"Supported URL Formats","text":"<pre><code># HTTPS (recommended)\n\"https://github.com/owner/repo\"\n\"https://github.com/owner/repo.git\"\n\n# SSH\n\"git@github.com:owner/repo.git\"\n\n# With branch (coming soon)\n\"https://github.com/owner/repo/tree/branch-name\"\n</code></pre>"},{"location":"user-guide/url-analysis/#commit-metrics","title":"Commit Metrics","text":"<p>GreenMining extracts the following metrics for each commit:</p>"},{"location":"user-guide/url-analysis/#basic-commit-metrics","title":"Basic Commit Metrics","text":"Metric Description <code>modified_files</code> Number of files changed <code>insertions</code> Lines added <code>deletions</code> Lines removed <code>files</code> List of modified file paths"},{"location":"user-guide/url-analysis/#dmm-metrics-delta-maintainability-model","title":"DMM Metrics (Delta Maintainability Model)","text":"<p>Measures how a commit impacts code maintainability on a 0-1 scale (higher is better).</p> Metric Range Description <code>dmm_unit_size</code> 0-1 Unit size maintainability \u2014 proportion of changed code units that remain within acceptable size thresholds <code>dmm_unit_complexity</code> 0-1 Cyclomatic complexity impact \u2014 proportion of changed code units with acceptable complexity <code>dmm_unit_interfacing</code> 0-1 Interface complexity \u2014 proportion of changed code units with manageable parameter counts"},{"location":"user-guide/url-analysis/#process-metrics","title":"Process Metrics","text":"<p>All 8 process metrics tracked per repository:</p> Metric Description <code>change_set</code> Number of files changed per commit (max, avg) <code>code_churn</code> Lines added/removed over time <code>contributors_count</code> Unique contributors in the analysis period <code>commits_count</code> Total commits in the analysis period <code>contributors_experience</code> Average experience of contributors (commits to repo) <code>history_complexity</code> Normalized entropy of file change history <code>hunks_count</code> Number of contiguous changed blocks per file <code>lines_count</code> Total lines of code modified across all commits"},{"location":"user-guide/url-analysis/#method-level-metrics","title":"Method-Level Metrics","text":"<p>When <code>method_level_analysis=True</code>, GreenMining uses Lizard to extract per-function metrics:</p> Metric Description <code>methods_count</code> Number of methods analyzed in a commit <code>total_nloc</code> Total non-comment lines of code <code>total_complexity</code> Sum of cyclomatic complexity across all methods <code>max_complexity</code> Highest single-function complexity <p>Each method entry includes:</p> Field Description <code>name</code> Function/method name <code>nloc</code> Non-comment lines of code <code>complexity</code> Cyclomatic complexity <code>token_count</code> Number of tokens <code>parameters</code> Number of parameters"},{"location":"user-guide/url-analysis/#configuration-options","title":"Configuration Options","text":"<p>Configure URL analysis via environment variables or Config:</p> <pre><code># Environment variables\nexport CLONE_PATH=/custom/path\nexport CLEANUP_AFTER_ANALYSIS=false\nexport PROCESS_METRICS_ENABLED=true\nexport DMM_ENABLED=true\n</code></pre> <pre><code># Python configuration\nfrom greenmining.config import Config\n\nconfig = Config()\nprint(config.CLONE_PATH)               # /tmp/greenmining_repos\nprint(config.CLEANUP_AFTER_ANALYSIS)   # True\nprint(config.PROCESS_METRICS_ENABLED)  # True\nprint(config.DMM_ENABLED)              # True\n</code></pre>"},{"location":"user-guide/url-analysis/#batch-analysis","title":"Batch Analysis","text":"<p>Analyze multiple repositories efficiently:</p> <pre><code>from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\nimport json\n\nrepos = [\n    \"https://github.com/pallets/flask\",\n    \"https://github.com/django/django\",\n    \"https://github.com/fastapi/fastapi\",\n]\n\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\nall_results = []\n\nfor url in repos:\n    print(f\"Analyzing {url}...\")\n    result = analyzer.analyze_repository(url, max_commits=100)\n    all_results.append(result)\n    print(f\"  \u2713 {result['green_aware_count']}/{result['total_commits']} green-aware\")\n\n# Save combined results\nwith open(\"batch_results.json\", \"w\") as f:\n    json.dump(all_results, f, indent=2, default=str)\n\n# Summary\nprint(f\"\\nTotal repositories: {len(all_results)}\")\ntotal_commits = sum(r['total_commits'] for r in all_results)\ntotal_green = sum(r['green_aware_count'] for r in all_results)\nprint(f\"Total commits: {total_commits}\")\nprint(f\"Total green-aware: {total_green} ({total_green/total_commits*100:.1f}%)\")\n</code></pre>"},{"location":"user-guide/url-analysis/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/url-analysis/#clone-failures","title":"Clone Failures","text":"<pre><code># Increase timeout\nanalyzer = LocalRepoAnalyzer(clone_timeout=300)  # 5 minutes\n\n# Use SSH for private repos\nresult = analyzer.analyze_repository(\"git@github.com:org/private-repo.git\")\n</code></pre>"},{"location":"user-guide/url-analysis/#large-repositories","title":"Large Repositories","text":"<pre><code># Limit commits for large repos\nresult = analyzer.analyze_repository(\n    repo_url=\"https://github.com/kubernetes/kubernetes\",\n    max_commits=500  # Limit for faster analysis\n)\n</code></pre>"},{"location":"user-guide/url-analysis/#disk-space","title":"Disk Space","text":"<pre><code># Always cleanup\nanalyzer = LocalRepoAnalyzer(cleanup_after=True)\n\n# Or manual cleanup\nimport shutil\nshutil.rmtree(\"/tmp/greenmining_repos\")\n</code></pre>"},{"location":"user-guide/url-analysis/#next-steps","title":"Next Steps","text":"<ul> <li>Energy Measurement - Measure energy during analysis</li> <li>Python API - Full API reference</li> <li>Configuration Options - All settings</li> </ul>"}]}