{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5cbd4a",
   "metadata": {},
   "source": [
    "# GreenMining Experiment: \n",
    "\n",
    "This notebook demonstrates a complete analysis pipeline using the `greenmining` library.\n",
    "\n",
    "## Experiment Setup\n",
    "- **10 android repositories** found via GraphQL search\n",
    "- **5 manually selected repositories** (FastAPI, UPISAS, Green-lab, Experiment-runner, Requests)\n",
    "- **Total: 15 repositories** — all analyzed with the same pipeline and ALL features enabled\n",
    "- **Commits per repository:** 50\n",
    "- **Min stars:** 1\n",
    "- **Languages:** Top 20 programming languages\n",
    "\n",
    "## Pipeline Structure\n",
    "1. **Data Gathering** — search + URL-based fetching for all 13 repos\n",
    "2. **Unified Analysis** — every feature applied to every repo equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae90321",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install greenmining[energy] --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b5712c",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "Import all GreenMining modules needed for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc5b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import tracemalloc\n",
    "import pandas as pd\n",
    "\n",
    "import greenmining\n",
    "from greenmining import (fetch_repositories,clone_repositories,analyze_repositories,GSF_PATTERNS,GREEN_KEYWORDS,is_green_aware,get_pattern_by_keywords,)\n",
    "from greenmining.analyzers import (StatisticalAnalyzer,TemporalAnalyzer,CodeDiffAnalyzer,MetricsPowerCorrelator)\n",
    "from greenmining.energy import get_energy_meter, CPUEnergyMeter\n",
    "\n",
    "print(f'GreenMining version: {greenmining.__version__}')\n",
    "print(f'GSF Patterns: {len(GSF_PATTERNS)}')\n",
    "print(f'Green Keywords: {len(GREEN_KEYWORDS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211be8b4",
   "metadata": {},
   "source": [
    "## Step 2: Configuration\n",
    "\n",
    "GitHub token and analysis parameters shared across all repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN', 'your_github_token_here')\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN', GITHUB_TOKEN)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "if GITHUB_TOKEN == 'your_github_token_here':\n",
    "    print('WARNING: Set GITHUB_TOKEN to run the search step.')\n",
    "else:\n",
    "    print(f'GitHub token configured ({GITHUB_TOKEN[:8]}...)')\n",
    "\n",
    "# Shared analysis parameters\n",
    "MAX_COMMITS = 50\n",
    "MIN_STARS = 1\n",
    "PARALLEL_WORKERS = 50\n",
    "\n",
    "# Date filters for repository search\n",
    "CREATED_AFTER = '2020-01-01'\n",
    "CREATED_BEFORE = '2025-12-31'\n",
    "PUSHED_AFTER = '2020-01-01'\n",
    "\n",
    "# Date filters for commit analysis\n",
    "COMMIT_DATE_FROM = '2020-01-01'\n",
    "COMMIT_DATE_TO = '2025-12-31'\n",
    "\n",
    "LANGUAGES = [\n",
    "    'Python', 'JavaScript', 'TypeScript', 'Java', 'C++',\n",
    "    'C#', 'Go', 'Rust', 'PHP', 'Ruby',\n",
    "    'Swift', 'Kotlin', 'Scala', 'R', 'MATLAB',\n",
    "    'Dart', 'Lua', 'Perl', 'Haskell', 'Elixir',\n",
    "]\n",
    "\n",
    "print(f'Max commits per repo: {MAX_COMMITS}')\n",
    "print(f'Min stars: {MIN_STARS}')\n",
    "print(f'Languages: {len(LANGUAGES)}')\n",
    "print(f'Repo created: {CREATED_AFTER} to {CREATED_BEFORE}')\n",
    "print(f'Repo pushed after: {PUSHED_AFTER}')\n",
    "print(f'Commit date range: {COMMIT_DATE_FROM} to {COMMIT_DATE_TO}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e76e4",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Search Repositories\n",
    "\n",
    "Use the GraphQL API to find 10 android repositories matching our criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d52cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_repos = fetch_repositories(\n",
    "    github_token=GITHUB_TOKEN,\n",
    "    max_repos=10,\n",
    "    min_stars=MIN_STARS,\n",
    "    languages=LANGUAGES,\n",
    "    keywords='android',\n",
    "    created_after=CREATED_AFTER,\n",
    "    created_before=CREATED_BEFORE,\n",
    "    pushed_after=PUSHED_AFTER,\n",
    ")\n",
    "\n",
    "print(f'Found {len(search_repos)} android repositories:')\n",
    "for i, repo in enumerate(search_repos, 1):\n",
    "    print(f'  {i:2d}. {repo.full_name} ({repo.stars} stars, {repo.language})')\n",
    "\n",
    "search_urls = [repo.url for repo in search_repos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e7d73",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Repositories\n",
    "\n",
    "Combine the 10 search results with 3 manually selected repositories, then run the\n",
    "full analysis pipeline on all of them at once with every feature enabled:\n",
    "- GSF pattern detection (124 patterns, 332 keywords)\n",
    "- Process metrics (DMM size, complexity, interfacing)\n",
    "- Method-level analysis (per-function complexity metrics)\n",
    "- Source code capture (before/after for each modified file)\n",
    "- Energy measurement (CPU-based tracking during analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 manually selected repositories\n",
    "manual_urls = [\n",
    "    'https://github.com/S2-group/green-lab',\n",
    "    'https://github.com/psf/requests',\n",
    "    'https://github.com/S2-group/UPISAS',\n",
    "    'https://github.com/tiangolo/fastapi',\n",
    "    'https://github.com/S2-group/experiment-runner',\n",
    "]\n",
    "\n",
    "# Combine all URLs\n",
    "all_urls = search_urls + manual_urls\n",
    "print(f'Total repositories: {len(all_urls)}')\n",
    "print(f'  Search results: {len(search_urls)}')\n",
    "print(f'  Manual selection: {len(manual_urls)}')\n",
    "print(f'  Commit date range: {COMMIT_DATE_FROM} to {COMMIT_DATE_TO}')\n",
    "print()\n",
    "\n",
    "# Analyze ALL repositories with ALL features\n",
    "raw_results = analyze_repositories(\n",
    "    urls=all_urls,\n",
    "    max_commits=MAX_COMMITS,\n",
    "    parallel_workers=PARALLEL_WORKERS,\n",
    "    output_format='dict',\n",
    "    energy_tracking=True,\n",
    "    energy_backend='auto',\n",
    "    method_level_analysis=True,\n",
    "    include_source_code=True,\n",
    "    github_token=GITHUB_TOKEN,\n",
    "    since_date=COMMIT_DATE_FROM,\n",
    "    to_date=COMMIT_DATE_TO,\n",
    ")\n",
    "\n",
    "# Convert dataclass results to dicts\n",
    "results = [r.to_dict() for r in raw_results]\n",
    "\n",
    "print(f'\\nAnalysis complete: {len(results)} repositories')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcf7968",
   "metadata": {},
   "source": [
    "## Step 5: Results Overview\n",
    "\n",
    "Summary of the analysis across all repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d11144",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_commits = sum(r['total_commits'] for r in results)\n",
    "total_green = sum(r['green_commits'] for r in results)\n",
    "overall_rate = total_green / total_commits if total_commits > 0 else 0\n",
    "\n",
    "print('=' * 70)\n",
    "print('ANALYSIS SUMMARY')\n",
    "print('=' * 70)\n",
    "print(f'Repositories analyzed: {len(results)}')\n",
    "print(f'Total commits: {total_commits}')\n",
    "print(f'Green-aware commits: {total_green}')\n",
    "print(f'Overall green rate: {overall_rate:.1%}')\n",
    "print()\n",
    "print(f'{\"Repository\":<40} {\"Commits\":<10} {\"Green\":<10} {\"Rate\":<10}')\n",
    "print('-' * 70)\n",
    "for r in results:\n",
    "    rate = r['green_commit_rate'] if r['total_commits'] > 0 else 0\n",
    "    print(f'{r[\"name\"]:<40} {r[\"total_commits\"]:<10} {r[\"green_commits\"]:<10} {rate:.1%}')\n",
    "\n",
    "# Build commit list for use in all later analysis steps\n",
    "all_commits = []\n",
    "for r in results:\n",
    "    for c in r.get('commits', []):\n",
    "        c['repository'] = r['name']\n",
    "        all_commits.append(c)\n",
    "\n",
    "print(f'\\nAll Commits: {len(all_commits)} commits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b3b23",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: GSF Pattern Analysis\n",
    "\n",
    "Examine the Green Software Foundation patterns detected across all repositories.\n",
    "GreenMining detects 124 patterns across 15 categories with 332 keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern frequency across all commits\n",
    "pattern_counts = {}\n",
    "for commit in all_commits:\n",
    "    for pattern in commit.get('gsf_patterns_matched', []):\n",
    "        pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1\n",
    "\n",
    "sorted_patterns = sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f'Unique patterns detected: {len(sorted_patterns)}')\n",
    "print(f'\\nTop 20 GSF Patterns:')\n",
    "print(f'{\"Pattern\":<45} {\"Count\":<8} {\"% of Commits\":<12}')\n",
    "print('-' * 65)\n",
    "for pattern, count in sorted_patterns[:20]:\n",
    "    pct = count / len(all_commits) * 100 if all_commits else 0\n",
    "    print(f'{pattern:<45} {count:<8} {pct:.1f}%')\n",
    "\n",
    "# Pattern categories\n",
    "categories = set()\n",
    "for p in GSF_PATTERNS.values():\n",
    "    categories.add(p.get('category', 'Unknown'))\n",
    "print(f'\\nGSF Categories ({len(categories)}):')\n",
    "for cat in sorted(categories):\n",
    "    count = sum(1 for p in GSF_PATTERNS.values() if p.get('category') == cat)\n",
    "    print(f'  {cat}: {count} patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920baf5",
   "metadata": {},
   "source": [
    "## Step 7: Process Metrics\n",
    "\n",
    "Examine the process metrics collected during analysis: DMM (Delta Maintainability Model)\n",
    "scores for size, complexity, and interfacing, plus structural complexity metrics\n",
    "and method-level analysis via Lizard integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d55c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Process Metrics Summary')\n",
    "print('=' * 70)\n",
    "\n",
    "metrics_keys = [\n",
    "    'dmm_unit_size', 'dmm_unit_complexity', 'dmm_unit_interfacing',\n",
    "    'total_nloc', 'total_complexity', 'max_complexity',\n",
    "    'methods_count', 'insertions', 'deletions',\n",
    "]\n",
    "\n",
    "metrics_data = {k: [] for k in metrics_keys}\n",
    "for commit in all_commits:\n",
    "    for key in metrics_keys:\n",
    "        val = commit.get(key)\n",
    "        if val is not None:\n",
    "            metrics_data[key].append(val)\n",
    "\n",
    "print(f'{\"Metric\":<25} {\"Avg\":>10} {\"Min\":>10} {\"Max\":>10} {\"N\":>6}')\n",
    "print('-' * 65)\n",
    "for metric, values in metrics_data.items():\n",
    "    if values:\n",
    "        avg = sum(values) / len(values)\n",
    "        print(f'{metric:<25} {avg:>10.2f} {min(values):>10.2f} {max(values):>10.2f} {len(values):>6}')\n",
    "\n",
    "# Method-level analysis\n",
    "total_methods = sum(len(c.get('methods', [])) for c in all_commits)\n",
    "print(f'\\nMethod-Level Analysis:')\n",
    "print(f'  Total methods analyzed: {total_methods}')\n",
    "\n",
    "for commit in all_commits:\n",
    "    methods = commit.get('methods', [])\n",
    "    if methods:\n",
    "        print(f'  Sample from {commit.get(\"repository\")} ({commit[\"commit_hash\"][:8]}):')\n",
    "        for m in methods[:3]:\n",
    "            print(f'    {m.get(\"name\", \"N/A\")}: nloc={m.get(\"nloc\", 0)}, '\n",
    "                  f'complexity={m.get(\"complexity\", 0)}')\n",
    "        break\n",
    "\n",
    "# Source code changes\n",
    "total_src = sum(len(c.get('source_changes', [])) for c in all_commits)\n",
    "print(f'\\nSource code changes captured: {total_src}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327855e",
   "metadata": {},
   "source": [
    "## Step 8: Statistical Analysis\n",
    "\n",
    "Apply statistical methods to the combined dataset: pattern correlations,\n",
    "temporal trend significance, and effect sizes between green and non-green commits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_analyzer = StatisticalAnalyzer()\n",
    "\n",
    "commits_df = pd.DataFrame(all_commits)\n",
    "\n",
    "# Drop non-numeric pattern columns that conflict with the pattern_ prefix filter\n",
    "commits_df = commits_df.drop(columns=['pattern_details', 'pattern_count'], errors='ignore')\n",
    "\n",
    "# Convert date strings to tz-naive datetime for pandas resample compatibility\n",
    "if 'date' in commits_df.columns:\n",
    "    commits_df['date'] = pd.to_datetime(commits_df['date'], utc=True, errors='coerce')\n",
    "    commits_df['date'] = commits_df['date'].dt.tz_localize(None)\n",
    "\n",
    "# Add binary indicator columns for each pattern\n",
    "all_pattern_names = list(pattern_counts.keys())\n",
    "for pattern in all_pattern_names:\n",
    "    commits_df[f'pattern_{pattern}'] = commits_df['gsf_patterns_matched'].apply(\n",
    "        lambda x, p=pattern: 1 if p in (x or []) else 0\n",
    "    )\n",
    "\n",
    "# Pattern correlations\n",
    "if len(all_pattern_names) >= 2:\n",
    "    corr = stat_analyzer.analyze_pattern_correlations(commits_df)\n",
    "    sig_pairs = corr.get('significant_pairs', [])\n",
    "    print(f'Pattern Correlation Analysis:')\n",
    "    print(f'  Significant pairs: {len(sig_pairs)}')\n",
    "    for pair in sig_pairs[:10]:\n",
    "        print(f'    {pair}')\n",
    "else:\n",
    "    print(f'Found {len(all_pattern_names)} pattern(s) - need >= 2 for correlation')\n",
    "\n",
    "# Temporal trend\n",
    "if 'date' in commits_df.columns and 'green_aware' in commits_df.columns:\n",
    "    if 'commit_hash' not in commits_df.columns:\n",
    "        commits_df['commit_hash'] = commits_df.index.astype(str)\n",
    "    trend_results = stat_analyzer.temporal_trend_analysis(commits_df)\n",
    "    trend = trend_results.get('trend', {})\n",
    "    print(f'\\nTemporal Trend:')\n",
    "    print(f'  Direction: {trend.get(\"direction\", \"N/A\")}')\n",
    "    print(f'  Significant: {trend.get(\"significant\", \"N/A\")}')\n",
    "    print(f'  Correlation: {trend.get(\"correlation\", \"N/A\")}')\n",
    "\n",
    "# Effect size: green vs non-green complexity\n",
    "green_cx = commits_df[commits_df['green_aware'] == True]['total_complexity'].dropna().tolist()\n",
    "non_green_cx = commits_df[commits_df['green_aware'] == False]['total_complexity'].dropna().tolist()\n",
    "\n",
    "if green_cx and non_green_cx:\n",
    "    effect = stat_analyzer.effect_size_analysis(green_cx, non_green_cx)\n",
    "    print(f'\\nEffect Size (Green vs Non-Green Complexity):')\n",
    "    print(f'  Cohen\\'s d: {effect[\"cohens_d\"]:.3f} ({effect[\"magnitude\"]})')\n",
    "    print(f'  Mean difference: {effect[\"mean_difference\"]:.2f}')\n",
    "    print(f'  Significant: {effect[\"significant\"]}')\n",
    "else:\n",
    "    print('\\nInsufficient data for effect size analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e64e5c",
   "metadata": {},
   "source": [
    "## Step 9: Temporal Analysis\n",
    "\n",
    "Analyze how green software practices evolve over time across all repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00245811",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal = TemporalAnalyzer(granularity='quarter')\n",
    "\n",
    "# Convert to analyzer's expected format\n",
    "analysis_results_fmt = []\n",
    "for c in all_commits:\n",
    "    analysis_results_fmt.append({\n",
    "        'commit_sha': c.get('commit_hash', ''),\n",
    "        'is_green_aware': c.get('green_aware', False),\n",
    "        'patterns_detected': c.get('gsf_patterns_matched', []),\n",
    "        'detection_method': 'gsf_keyword',\n",
    "    })\n",
    "\n",
    "temporal_results = temporal.analyze_trends(all_commits, analysis_results_fmt)\n",
    "\n",
    "periods = temporal_results.get('periods', [])\n",
    "print(f'Temporal Analysis ({len(periods)} periods):')\n",
    "print(f'{\"Period\":<20} {\"Commits\":<10} {\"Green\":<10} {\"Rate\":<10} {\"Patterns\":<10}')\n",
    "print('-' * 60)\n",
    "for p in periods:\n",
    "    rate = p.get('green_awareness_rate', 0)\n",
    "    print(f'{p.get(\"period\", \"N/A\"):<20} {p.get(\"commit_count\", 0):<10} '\n",
    "          f'{p.get(\"green_commit_count\", 0):<10} {rate:.1%}      '\n",
    "          f'{p.get(\"unique_patterns\", 0)}')\n",
    "\n",
    "summary = temporal_results.get('summary', {})\n",
    "print(f'\\nTrend: {summary.get(\"overall_direction\", \"N/A\")}')\n",
    "print(f'Peak period: {summary.get(\"peak_period\", \"N/A\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269011c",
   "metadata": {},
   "source": [
    "## Step 10: Code Diff Pattern Signatures\n",
    "\n",
    "The CodeDiffAnalyzer detects green patterns directly in code changes.\n",
    "It is integrated into the analysis pipeline automatically. Here we inspect\n",
    "the pattern signatures it looks for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19562348",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_analyzer = CodeDiffAnalyzer()\n",
    "\n",
    "print(f'Code Diff Pattern Signatures: {len(diff_analyzer.PATTERN_SIGNATURES)} types')\n",
    "print('=' * 60)\n",
    "for name, data in diff_analyzer.PATTERN_SIGNATURES.items():\n",
    "    print(f'  {name}:')\n",
    "    if isinstance(data, dict):\n",
    "        for key, val in list(data.items())[:2]:\n",
    "            if isinstance(val, list):\n",
    "                print(f'    {key}: {val[:3]}...')\n",
    "            else:\n",
    "                print(f'    {key}: {val}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5886de9",
   "metadata": {},
   "source": [
    "## Step 11: Energy Measurement\n",
    "\n",
    "GreenMining provides multiple energy measurement backends:\n",
    "- **RAPL** — Linux kernel hardware counters (Intel/AMD, most accurate)\n",
    "- **CPU Meter** — universal (estimates from CPU utilization and TDP)\n",
    "- **tracemalloc** — Python standard library for memory usage profiling\n",
    "- **CodeCarbon** — cross-platform CO2 emissions tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a670021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check available energy backends\n",
    "print('Available Energy Backends:')\n",
    "for backend in ['rapl', 'codecarbon', 'cpu_meter']:\n",
    "    try:\n",
    "        m = get_energy_meter(backend)\n",
    "        print(f'  {backend}: available ({type(m).__name__})')\n",
    "    except Exception as e:\n",
    "        print(f'  {backend}: not available ({e})')\n",
    "\n",
    "def sample_workload():\n",
    "    return sum(i ** 2 for i in range(1_000_000))\n",
    "\n",
    "# 2. RAPL measurement (Linux Intel/AMD)\n",
    "print('\\n--- RAPL Energy Meter ---')\n",
    "try:\n",
    "    from greenmining.energy import RAPLEnergyMeter\n",
    "    rapl = RAPLEnergyMeter()\n",
    "    if rapl.is_available():\n",
    "        result, energy = rapl.measure(sample_workload)\n",
    "        print(f'  Energy: {energy.joules:.4f} J')\n",
    "        print(f'  Power avg: {energy.watts_avg:.2f} W')\n",
    "        print(f'  Duration: {energy.duration_seconds:.3f} s')\n",
    "    else:\n",
    "        print('  RAPL not available (requires Linux with Intel/AMD CPU)')\n",
    "except Exception as e:\n",
    "    print(f'  RAPL error: {e}')\n",
    "\n",
    "# 3. CPU Meter measurement (universal)\n",
    "print('\\n--- CPU Energy Meter ---')\n",
    "meter = CPUEnergyMeter()\n",
    "result, energy = meter.measure(sample_workload)\n",
    "print(f'  Energy: {energy.joules:.4f} J')\n",
    "print(f'  Power avg: {energy.watts_avg:.2f} W')\n",
    "print(f'  Duration: {energy.duration_seconds:.3f} s')\n",
    "print(f'  Backend: {energy.backend}')\n",
    "\n",
    "# 4. CodeCarbon CO2 tracking\n",
    "print('\\n--- CodeCarbon CO2 Tracking ---')\n",
    "try:\n",
    "    from codecarbon import EmissionsTracker\n",
    "    tracker = EmissionsTracker(log_level='error', save_to_file=False)\n",
    "    tracker.start()\n",
    "    _ = sample_workload()\n",
    "    emissions = tracker.stop()\n",
    "    print(f'  CO2 emissions: {emissions:.8f} kg')\n",
    "    print(f'  Equivalent: {emissions * 1e6:.4f} mg CO2')\n",
    "except ImportError:\n",
    "    print('  CodeCarbon not installed (pip install codecarbon)')\n",
    "except Exception as e:\n",
    "    print(f'  CodeCarbon error: {e}')\n",
    "\n",
    "# 5. tracemalloc memory profiling\n",
    "print('\\n--- tracemalloc Memory Profiling ---')\n",
    "tracemalloc.start()\n",
    "_ = sample_workload()\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "print(f'  Current memory: {current / 1024:.1f} KB')\n",
    "print(f'  Peak memory: {peak / 1024:.1f} KB')\n",
    "\n",
    "# 6. Show energy from the repository analysis pipeline\n",
    "print('\\n--- Analysis Energy (from repository pipeline) ---')\n",
    "for r in results:\n",
    "    e = r.get('energy_metrics')\n",
    "    if e and e.get('joules', 0) > 0:\n",
    "        print(f'  {r[\"name\"]}:')\n",
    "        print(f'    Total: {e.get(\"joules\", 0):.4f} J')\n",
    "        print(f'    Avg power: {e.get(\"watts_avg\", 0):.2f} W')\n",
    "        break\n",
    "else:\n",
    "    print('  No per-repo energy data collected (backend may not support it)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc02b12",
   "metadata": {},
   "source": [
    "## Step 12: Metrics-to-Power Correlation\n",
    "\n",
    "Analyze correlations between code metrics and energy consumption using\n",
    "Pearson and Spearman coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlator = MetricsPowerCorrelator(significance_level=0.05)\n",
    "\n",
    "metric_names = ['total_complexity', 'total_nloc', 'files_modified', 'insertions', 'deletions']\n",
    "metrics_values = {m: [] for m in metric_names}\n",
    "power_measurements = []\n",
    "\n",
    "for c in all_commits:\n",
    "    has_all = all(c.get(m) is not None for m in metric_names)\n",
    "    energy_val = c.get('energy_watts_avg') or c.get('energy_joules')\n",
    "    if has_all and energy_val:\n",
    "        for m in metric_names:\n",
    "            metrics_values[m].append(float(c[m]))\n",
    "        power_measurements.append(float(energy_val))\n",
    "\n",
    "if len(power_measurements) >= 3:\n",
    "    correlator.fit(metric_names, metrics_values, power_measurements)\n",
    "    summary = correlator.summary()\n",
    "    print(f'Metrics-to-Power Correlation:')\n",
    "    print(f'  Metrics analyzed: {summary[\"total_metrics\"]}')\n",
    "    print(f'  Significant: {summary[\"significant_count\"]}')\n",
    "    print()\n",
    "    for name, result in correlator.get_results().items():\n",
    "        print(f'  {name}:')\n",
    "        print(f'    Pearson r={result.pearson_r:.3f}, Spearman rho={result.spearman_rho:.3f}')\n",
    "        print(f'    Significant: {result.is_significant}')\n",
    "    print(f'\\nFeature Importance:')\n",
    "    for name, imp in correlator.feature_importance.items():\n",
    "        bar = '#' * int(imp * 30)\n",
    "        print(f'  {name:<20} {imp:.3f} {bar}')\n",
    "else:\n",
    "    print(f'Insufficient data ({len(power_measurements)} points, need >= 3)')\n",
    "    print('Enable energy_tracking=True to collect per-commit energy data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b9815",
   "metadata": {},
   "source": [
    "## Step 13: Visualization (matplotlib)\n",
    "\n",
    "Static charts from the analysis data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51effe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Green commit rate per repository\n",
    "repo_names = [r['name'][:20] for r in results]\n",
    "green_rates = [r['green_commit_rate'] for r in results]\n",
    "axes[0, 0].barh(repo_names, green_rates, color='green', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Green Commit Rate')\n",
    "axes[0, 0].set_title('Green Awareness by Repository')\n",
    "axes[0, 0].set_xlim(0, 1)\n",
    "\n",
    "# 2. Top 10 patterns\n",
    "if sorted_patterns:\n",
    "    top = sorted_patterns[:10]\n",
    "    axes[0, 1].barh([p[0][:30] for p in top], [p[1] for p in top], color='teal', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Count')\n",
    "    axes[0, 1].set_title('Top 10 GSF Patterns')\n",
    "\n",
    "# 3. Commits breakdown\n",
    "commit_counts = [r['total_commits'] for r in results]\n",
    "green_counts = [r['green_commits'] for r in results]\n",
    "x = range(len(results))\n",
    "axes[1, 0].bar(x, commit_counts, label='Total', alpha=0.7)\n",
    "axes[1, 0].bar(x, green_counts, label='Green', alpha=0.7)\n",
    "axes[1, 0].set_xticks(list(x))\n",
    "axes[1, 0].set_xticklabels(repo_names, rotation=45, ha='right', fontsize=7)\n",
    "axes[1, 0].set_title('Commit Breakdown')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Complexity distribution\n",
    "cxs = [c.get('total_complexity', 0) for c in all_commits if c.get('total_complexity')]\n",
    "if cxs:\n",
    "    axes[1, 1].hist(cxs, bins=20, color='orange', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Total Complexity')\n",
    "    axes[1, 1].set_title('Complexity Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved to data/analysis_plots.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e39a2",
   "metadata": {},
   "source": [
    "## Step 14: Interactive Visualization (Plotly)\n",
    "\n",
    "Interactive charts for deeper exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Sunburst: Repository -> Green/Non-Green -> Pattern\n",
    "sun_data = []\n",
    "for r in results:\n",
    "    for c in r.get('commits', []):\n",
    "        cat = 'Green' if c.get('green_aware') else 'Non-Green'\n",
    "        patterns = c.get('gsf_patterns_matched', [])\n",
    "        pat = patterns[0] if patterns else 'None'\n",
    "        sun_data.append({\n",
    "            'repository': r['name'][:20], 'category': cat,\n",
    "            'pattern': pat[:30], 'count': 1,\n",
    "        })\n",
    "\n",
    "if sun_data:\n",
    "    df_sun = pd.DataFrame(sun_data)\n",
    "    fig = px.sunburst(df_sun, path=['repository', 'category', 'pattern'],\n",
    "                      values='count', title='Repository Analysis Breakdown')\n",
    "    fig.show()\n",
    "\n",
    "# Scatter: Complexity vs NLOC\n",
    "sc = [{'cx': c['total_complexity'], 'nloc': c['total_nloc'],\n",
    "       'green': 'Green' if c.get('green_aware') else 'Non-Green',\n",
    "       'repo': c.get('repository', '')}\n",
    "      for c in all_commits if c.get('total_complexity') and c.get('total_nloc')]\n",
    "\n",
    "if sc:\n",
    "    fig2 = px.scatter(pd.DataFrame(sc), x='nloc', y='cx', color='green',\n",
    "                      hover_data=['repo'],\n",
    "                      title='Complexity vs Lines of Code')\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc209221",
   "metadata": {},
   "source": [
    "## Step 15: Export Results\n",
    "\n",
    "Export the unified analysis to JSON, CSV, and pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e06f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "with open('data/analysis_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "print('Exported data/analysis_results.json')\n",
    "\n",
    "# CSV (flattened commits)\n",
    "csv_rows = []\n",
    "for r in results:\n",
    "    for c in r.get('commits', []):\n",
    "        csv_rows.append({\n",
    "            'repository': r['name'],\n",
    "            'repo_url': r['url'],\n",
    "            'commit_hash': c.get('commit_hash', ''),\n",
    "            'author': c.get('author', ''),\n",
    "            'date': c.get('date', ''),\n",
    "            'message': str(c.get('message', ''))[:100],\n",
    "            'green_aware': c.get('green_aware', False),\n",
    "            'patterns_matched': ', '.join(c.get('gsf_patterns_matched', [])),\n",
    "            'pattern_count': c.get('pattern_count', 0),\n",
    "            'confidence': c.get('confidence', ''),\n",
    "            'files_modified': len(c.get('files_modified', [])) if isinstance(c.get('files_modified'), list) else c.get('files_modified', 0),\n",
    "            'insertions': c.get('insertions', 0),\n",
    "            'deletions': c.get('deletions', 0),\n",
    "            'dmm_unit_size': c.get('dmm_unit_size'),\n",
    "            'dmm_unit_complexity': c.get('dmm_unit_complexity'),\n",
    "            'dmm_unit_interfacing': c.get('dmm_unit_interfacing'),\n",
    "            'total_nloc': c.get('total_nloc'),\n",
    "            'total_complexity': c.get('total_complexity'),\n",
    "            'methods_count': c.get('methods_count'),\n",
    "            'energy_joules': c.get('energy_joules'),\n",
    "        })\n",
    "\n",
    "df_export = pd.DataFrame(csv_rows)\n",
    "df_export.to_csv('data/analysis_results.csv', index=False)\n",
    "print(f'Exported {len(csv_rows)} commits to data/analysis_results.csv')\n",
    "print(f'\\nDataFrame shape: {df_export.shape}')\n",
    "df_export.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c55aa",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Repositories Analyzed\n",
    "- 10 android repositories (GraphQL search, created 2020-2023, pushed after 2023)\n",
    "- 3 selected repositories (Flask, Requests, FastAPI)\n",
    "- **Total: 13 repositories** through a single unified pipeline\n",
    "- **Commit date range:** 2020-01-01 to 2025-12-31\n",
    "\n",
    "## Features Applied to All Repositories\n",
    "\n",
    "| Feature | Status |\n",
    "|---------|--------|\n",
    "| GSF Pattern Detection (122 patterns, 15 categories) | Applied |\n",
    "| Process Metrics (DMM size, complexity, interfacing) | Applied |\n",
    "| Method-Level Analysis (per-function complexity) | Applied |\n",
    "| Source Code Capture (before/after) | Applied |\n",
    "| Energy Measurement (RAPL + CPU Meter) | Applied |\n",
    "| Memory Profiling (tracemalloc) | Applied |\n",
    "| CO2 Emissions (CodeCarbon) | Applied |\n",
    "| Statistical Analysis (correlations, effect sizes) | Applied |\n",
    "| Temporal Analysis (quarterly trends) | Applied |\n",
    "| Code Diff Pattern Signatures | Applied |\n",
    "| Metrics-to-Power Correlation (Pearson/Spearman) | Applied |\n",
    "| Visualization (matplotlib + plotly) | Applied |\n",
    "| Export (JSON, CSV, DataFrame) | Applied | \n",
    "\n",
    "## Output Files\n",
    "- `data/analysis_results.json` -- Full analysis data\n",
    "- `data/analysis_results.csv` -- Flattened commit-level data\n",
    "- `data/analysis_plots.png` -- Static visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
