{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5cbd4a",
   "metadata": {},
   "source": [
    "# GreenMining Experiment: Unified Repository Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates a complete analysis pipeline using the `greenmining` library.\n",
    "\n",
    "## Experiment Setup\n",
    "- **10 blockchain repositories** found via GraphQL search\n",
    "- **3 manually selected repositories** (Flask, Requests, FastAPI)\n",
    "- **Total: 13 repositories** — all analyzed with the same pipeline and ALL features enabled\n",
    "- **Commits per repository:** 20\n",
    "- **Min stars:** 3\n",
    "- **Languages:** Top 20 programming languages\n",
    "\n",
    "## Pipeline Structure\n",
    "1. **Data Gathering** — search + URL-based fetching for all 13 repos\n",
    "2. **Unified Analysis** — every feature applied to every repo equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae90321",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install greenmining[energy,dashboard] --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b5712c",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "Import all GreenMining modules needed for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bc5b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreenMining version: 1.0.9\n",
      "GSF Patterns: 122\n",
      "Green Keywords: 321\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import greenmining\n",
    "from greenmining import (\n",
    "    fetch_repositories,\n",
    "    analyze_repositories,\n",
    "    GSF_PATTERNS,\n",
    "    GREEN_KEYWORDS,\n",
    "    is_green_aware,\n",
    "    get_pattern_by_keywords,\n",
    ")\n",
    "from greenmining.analyzers import (\n",
    "    StatisticalAnalyzer,\n",
    "    TemporalAnalyzer,\n",
    "    QualitativeAnalyzer,\n",
    "    CodeDiffAnalyzer,\n",
    "    PowerRegressionDetector,\n",
    "    MetricsPowerCorrelator,\n",
    "    VersionPowerAnalyzer,\n",
    ")\n",
    "from greenmining.energy import CarbonReporter, get_energy_meter, CPUEnergyMeter\n",
    "from greenmining.dashboard import create_app\n",
    "\n",
    "print(f'GreenMining version: {greenmining.__version__}')\n",
    "print(f'GSF Patterns: {len(GSF_PATTERNS)}')\n",
    "print(f'Green Keywords: {len(GREEN_KEYWORDS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211be8b4",
   "metadata": {},
   "source": [
    "## Step 2: Configuration\n",
    "\n",
    "GitHub token and analysis parameters shared across all repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b83d693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub token configured (github_p...)\n",
      "Max commits per repo: 20\n",
      "Min stars: 3\n",
      "Languages: 20\n"
     ]
    }
   ],
   "source": [
    "GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN', 'your_github_token_here')\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN', GITHUB_TOKEN)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "if GITHUB_TOKEN == 'your_github_token_here':\n",
    "    print('WARNING: Set GITHUB_TOKEN to run the search step.')\n",
    "else:\n",
    "    print(f'GitHub token configured ({GITHUB_TOKEN[:8]}...)')\n",
    "\n",
    "# Shared analysis parameters\n",
    "MAX_COMMITS = 20\n",
    "MIN_STARS = 3\n",
    "PARALLEL_WORKERS = 2\n",
    "\n",
    "LANGUAGES = [\n",
    "    'Python', 'JavaScript', 'TypeScript', 'Java', 'C++',\n",
    "    'C#', 'Go', 'Rust', 'PHP', 'Ruby',\n",
    "    'Swift', 'Kotlin', 'Scala', 'R', 'MATLAB',\n",
    "    'Dart', 'Lua', 'Perl', 'Haskell', 'Elixir',\n",
    "]\n",
    "\n",
    "print(f'Max commits per repo: {MAX_COMMITS}')\n",
    "print(f'Min stars: {MIN_STARS}')\n",
    "print(f'Languages: {len(LANGUAGES)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e76e4",
   "metadata": {},
   "source": [
    "---\n",
    "# Part B: Data Gathering\n",
    "\n",
    "## Step 3: Search Blockchain Repositories\n",
    "\n",
    "Use the GraphQL API to find 10 blockchain repositories matching our criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d52cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching up to 10 repositories...\n",
      "   Keywords: blockchain\n",
      "   Filters: min_stars=3\n",
      "   Created: 2020-01-01 to any\n",
      "GraphQL Search Query: blockchain stars:>=3 created:>=2020-01-01\n",
      "Rate Limit: 4985/5000 (cost: 1)\n",
      "Fetched 10 repositories using GraphQL\n",
      "Fetched 10 repositories\n",
      "   Saved to: data/repositories.json\n",
      "Found 10 blockchain repositories:\n",
      "   1. calistus-igwilo/nitda-blockchain-scholarship (3083 stars, HTML)\n",
      "   2. smartcontractkit/full-blockchain-solidity-course-js (13959 stars, None)\n",
      "   3. smartcontractkit/full-blockchain-solidity-course-py (11234 stars, None)\n",
      "   4. BytePhoenixCoding/BlockchainTokenSniper (472 stars, None)\n",
      "   5. FuelLabs/fuel-core (57393 stars, Rust)\n",
      "   6. slowmist/Blockchain-dark-forest-selfguard-handbook (6739 stars, None)\n",
      "   7. Eternaldeath/BlockchainHome (990 stars, HTML)\n",
      "   8. paritytech/polkadot-sdk (2670 stars, Rust)\n",
      "   9. aptos-labs/aptos-core (6424 stars, Rust)\n",
      "  10. massalabs/massa (5560 stars, Rust)\n"
     ]
    }
   ],
   "source": [
    "search_repos = fetch_repositories(\n",
    "    github_token=GITHUB_TOKEN,\n",
    "    max_repos=10,\n",
    "    min_stars=MIN_STARS,\n",
    "    languages=LANGUAGES,\n",
    "    keywords='blockchain',\n",
    "    created_after='2020-01-01',\n",
    ")\n",
    "\n",
    "print(f'Found {len(search_repos)} blockchain repositories:')\n",
    "for i, repo in enumerate(search_repos, 1):\n",
    "    print(f'  {i:2d}. {repo.full_name} ({repo.stars} stars, {repo.language})')\n",
    "\n",
    "search_urls = [repo.url for repo in search_repos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e7d73",
   "metadata": {},
   "source": [
    "## Step 4: Analyze All 13 Repositories\n",
    "\n",
    "Combine the 10 search results with 3 manually selected repositories, then run the\n",
    "full analysis pipeline on all of them at once with every feature enabled:\n",
    "- GSF pattern detection (122 patterns, 321 keywords)\n",
    "- Process metrics (DMM size, complexity, interfacing)\n",
    "- Method-level analysis (per-function complexity metrics)\n",
    "- Source code capture (before/after for each modified file)\n",
    "- Energy measurement (CPU-based tracking during analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f8f1fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total repositories: 13\n",
      "  Search results: 10\n",
      "  Manual selection: 3\n",
      "\n",
      "\n",
      " Analyzing 13 repositories with 2 workers\n",
      "\n",
      " Analyzing repository: calistus-igwilo/nitda-blockchain-scholarship\n",
      "\n",
      " Analyzing repository: smartcontractkit/full-blockchain-solidity-course-js   Cloning to: /tmp/greenmining_repos/nitda-blockchain-scholarship\n",
      "\n",
      "   Cloning to: /tmp/greenmining_repos/full-blockchain-solidity-course-js\n",
      "   Warning: Energy measurement start failed: Already measuring energy\n",
      "    Analyzed 2 commits\n",
      "   Computing process metrics...\n",
      "   Cleaning up: /tmp/greenmining_repos/full-blockchain-solidity-course-js\n",
      "\n",
      " Analyzing repository: smartcontractkit/full-blockchain-solidity-course-py   Completed: smartcontractkit/full-blockchain-solidity-course-js\n",
      "\n",
      "   Cloning to: /tmp/greenmining_repos/full-blockchain-solidity-course-py\n",
      "    Analyzed 1 commits\n",
      "   Computing process metrics...\n",
      "   Cleaning up: /tmp/greenmining_repos/full-blockchain-solidity-course-py\n",
      "\n",
      " Analyzing repository: BytePhoenixCoding/BlockchainTokenSniper   Completed: smartcontractkit/full-blockchain-solidity-course-py\n",
      "\n",
      "   Cloning to: /tmp/greenmining_repos/BlockchainTokenSniper\n",
      "    Analyzed 9 commits\n",
      "   Computing process metrics...\n",
      "   Cleaning up: /tmp/greenmining_repos/BlockchainTokenSniper\n",
      "\n",
      " Analyzing repository: FuelLabs/fuel-core   Completed: BytePhoenixCoding/BlockchainTokenSniper\n",
      "\n",
      "   Cloning to: /tmp/greenmining_repos/fuel-core\n",
      "    Analyzed 0 commits\n",
      "   Computing process metrics...\n",
      "   Cleaning up: /tmp/greenmining_repos/nitda-blockchain-scholarship\n",
      "\n",
      " Analyzing repository: slowmist/Blockchain-dark-forest-selfguard-handbook   Completed: calistus-igwilo/nitda-blockchain-scholarship\n",
      "\n",
      "   Cloning to: /tmp/greenmining_repos/Blockchain-dark-forest-selfguard-handbook\n",
      "    Analyzed 20 commits\n",
      "   Computing process metrics...\n",
      "   Cleaning up: /tmp/greenmining_repos/Blockchain-dark-forest-selfguard-handbook\n",
      "\n",
      " Analyzing repository: Eternaldeath/BlockchainHome   Completed: slowmist/Blockchain-dark-forest-selfguard-handbook\n",
      "\n",
      "   Cloning to: /tmp/greenmining_repos/BlockchainHome\n",
      "    Analyzed 9 commits\n",
      "   Computing process metrics...\n",
      "   Cleaning up: /tmp/greenmining_repos/BlockchainHome\n",
      "\n",
      " Analyzing repository: paritytech/polkadot-sdk\n",
      "   Completed: Eternaldeath/BlockchainHome   Cloning to: /tmp/greenmining_repos/polkadot-sdk\n",
      "\n",
      "    Analyzed 20 commits\n",
      "   Computing process metrics...\n",
      "    Analyzed 20 commits\n",
      "   Warning: Energy measurement stop failed: Not currently measuring energy\n",
      "   Computing process metrics...\n",
      "   Cleaning up: /tmp/greenmining_repos/fuel-core\n",
      "\n",
      " Analyzing repository: aptos-labs/aptos-core   Completed: FuelLabs/fuel-core\n",
      "\n",
      "   Cloning to: /tmp/greenmining_repos/aptos-core\n",
      "   Cleaning up: /tmp/greenmining_repos/aptos-core\n",
      "\n",
      " Analyzing repository: massalabs/massa   Error analyzing https://github.com/aptos-labs/aptos-core: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git clone -v -- https://*****:*****@github.com/aptos-labs/aptos-core /tmp/greenmining_repos/aptos-core\n",
      "  stderr: 'Cloning into '/tmp/greenmining_repos/aptos-core'...\n",
      "POST git-upload-pack (175 bytes)\n",
      "POST git-upload-pack (gzip 155267 to 74321 bytes)\n",
      "Updating files:  88% (14891/16822)\n",
      "Updating files:  89% (14972/16822)\n",
      "Updating files:  90% (15140/16822)\n",
      "Updating files:  91% (15309/16822)\n",
      "Updating files:  92% (15477/16822)\n",
      "Updating files:  93% (15645/16822)\n",
      "Updating files:  94% (15813/16822)\n",
      "Updating files:  95% (15981/16822)\n",
      "Updating files:  96% (16150/16822)\n",
      "Updating files:  97% (16318/16822)\n",
      "Updating files:  98% (16486/16822)\n",
      "Updating files:  99% (16654/16822)\n",
      "Updating files: 100% (16822/16822)\n",
      "Updating files: 100% (16822/16822), done.\n",
      "Downloading ecosystem/indexer-grpc/transaction-filter/fixtures/compressed_files_lz4_00008bc1d5adcf862d3967c1410001fb_705101000.pb.lz4 (466 KB)\n",
      "Error downloading object: ecosystem/indexer-grpc/transaction-filter/fixtures/compressed_files_lz4_00008bc1d5adcf862d3967c1410001fb_705101000.pb.lz4 (dedf23a): Smudge error: Error downloading ecosystem/indexer-grpc/transaction-filter/fixtures/compressed_files_lz4_00008bc1d5adcf862d3967c1410001fb_705101000.pb.lz4 (dedf23a08bc7bd9fc9c96dc5d45741fe52c225ccfc2f7c72030240585a531dc3): batch response: Resource not accessible by personal access token\n",
      "\n",
      "Errors logged to '/tmp/greenmining_repos/aptos-core/.git/lfs/logs/20260130T150849.511936043.log'.\n",
      "Use `git lfs logs last` to view the log.\n",
      "error: external filter 'git-lfs filter-process' failed\n",
      "fatal: ecosystem/indexer-grpc/transaction-filter/fixtures/compressed_files_lz4_00008bc1d5adcf862d3967c1410001fb_705101000.pb.lz4: smudge filter lfs failed\n",
      "warning: Clone succeeded, but checkout failed.\n",
      "You can inspect what was checked out with 'git status'\n",
      "and retry with 'git restore --source=HEAD :/'\n",
      "\n",
      "'\n",
      "\n",
      "   Cloning to: /tmp/greenmining_repos/massa\n",
      "   Warning: Energy measurement start failed: Already measuring energy\n",
      "    Analyzed 20 commits\n",
      "   Computing process metrics...\n",
      "   Cleaning up: /tmp/greenmining_repos/massa\n",
      "\n",
      " Analyzing repository: pallets/flask   Completed: massalabs/massa\n",
      "   Cloning to: /tmp/greenmining_repos/flask\n",
      "\n",
      "    Analyzed 20 commits\n",
      "   Computing process metrics...\n",
      "   Cleaning up: /tmp/greenmining_repos/flask\n",
      "\n",
      " Analyzing repository: psf/requests   Completed: pallets/flask\n",
      "\n",
      "   Cloning to: /tmp/greenmining_repos/requests\n",
      "    Analyzed 20 commits\n",
      "   Computing process metrics...\n",
      "   Cleaning up: /tmp/greenmining_repos/requests\n",
      "\n",
      " Analyzing repository: tiangolo/fastapi   Completed: psf/requests\n",
      "\n",
      "   Cloning to: /tmp/greenmining_repos/fastapi\n",
      "    Analyzed 20 commits\n",
      "   Computing process metrics...\n",
      "   Cleaning up: /tmp/greenmining_repos/polkadot-sdk\n",
      "   Completed: paritytech/polkadot-sdk\n",
      "   Cleaning up: /tmp/greenmining_repos/fastapi\n",
      "   Completed: tiangolo/fastapi\n",
      "\n",
      "Analysis complete: 12 repositories\n"
     ]
    }
   ],
   "source": [
    "# 3 manually selected repositories\n",
    "manual_urls = [\n",
    "    'https://github.com/pallets/flask',\n",
    "    'https://github.com/psf/requests',\n",
    "    'https://github.com/tiangolo/fastapi',\n",
    "]\n",
    "\n",
    "# Combine all URLs\n",
    "all_urls = search_urls + manual_urls\n",
    "print(f'Total repositories: {len(all_urls)}')\n",
    "print(f'  Search results: {len(search_urls)}')\n",
    "print(f'  Manual selection: {len(manual_urls)}')\n",
    "print()\n",
    "\n",
    "# Analyze ALL repositories with ALL features\n",
    "results = analyze_repositories(\n",
    "    urls=all_urls,\n",
    "    max_commits=MAX_COMMITS,\n",
    "    parallel_workers=PARALLEL_WORKERS,\n",
    "    output_format='dict',\n",
    "    energy_tracking=True,\n",
    "    energy_backend='auto',\n",
    "    method_level_analysis=True,\n",
    "    include_source_code=True,\n",
    "    github_token=GITHUB_TOKEN,\n",
    ")\n",
    "\n",
    "print(f'\\nAnalysis complete: {len(results)} repositories')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcf7968",
   "metadata": {},
   "source": [
    "## Step 5: Results Overview\n",
    "\n",
    "Summary of the unified analysis across all repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d11144",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RepositoryAnalysis' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m total_commits = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_commits\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m total_green = \u001b[38;5;28msum\u001b[39m(r[\u001b[33m'\u001b[39m\u001b[33mgreen_commits\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results)\n\u001b[32m      3\u001b[39m overall_rate = total_green / total_commits \u001b[38;5;28;01mif\u001b[39;00m total_commits > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m total_commits = \u001b[38;5;28msum\u001b[39m(\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_commits\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results)\n\u001b[32m      2\u001b[39m total_green = \u001b[38;5;28msum\u001b[39m(r[\u001b[33m'\u001b[39m\u001b[33mgreen_commits\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results)\n\u001b[32m      3\u001b[39m overall_rate = total_green / total_commits \u001b[38;5;28;01mif\u001b[39;00m total_commits > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: 'RepositoryAnalysis' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "total_commits = sum(r['total_commits'] for r in results)\n",
    "total_green = sum(r['green_commits'] for r in results)\n",
    "overall_rate = total_green / total_commits if total_commits > 0 else 0\n",
    "\n",
    "print('=' * 70)\n",
    "print('UNIFIED ANALYSIS SUMMARY')\n",
    "print('=' * 70)\n",
    "print(f'Repositories analyzed: {len(results)}')\n",
    "print(f'Total commits: {total_commits}')\n",
    "print(f'Green-aware commits: {total_green}')\n",
    "print(f'Overall green rate: {overall_rate:.1%}')\n",
    "print()\n",
    "print(f'{\"Repository\":<40} {\"Commits\":<10} {\"Green\":<10} {\"Rate\":<10}')\n",
    "print('-' * 70)\n",
    "for r in results:\n",
    "    rate = r['green_commit_rate'] if r['total_commits'] > 0 else 0\n",
    "    print(f'{r[\"name\"]:<40} {r[\"total_commits\"]:<10} {r[\"green_commits\"]:<10} {rate:.1%}')\n",
    "\n",
    "# Build flat commit list for use in all later analysis steps\n",
    "all_commits = []\n",
    "for r in results:\n",
    "    for c in r.get('commits', []):\n",
    "        c['repository'] = r['name']\n",
    "        all_commits.append(c)\n",
    "\n",
    "print(f'\\nFlattened commit pool: {len(all_commits)} commits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b3b23",
   "metadata": {},
   "source": [
    "---\n",
    "# Part C: Unified Analysis\n",
    "\n",
    "Every feature is applied to the combined dataset of all 13 repositories.\n",
    "\n",
    "## Step 6: GSF Pattern Analysis\n",
    "\n",
    "Examine the Green Software Foundation patterns detected across all repositories.\n",
    "GreenMining detects 122 patterns across 15 categories with 321 keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern frequency across all commits\n",
    "pattern_counts = {}\n",
    "for commit in all_commits:\n",
    "    for pattern in commit.get('gsf_patterns_matched', []):\n",
    "        pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1\n",
    "\n",
    "sorted_patterns = sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f'Unique patterns detected: {len(sorted_patterns)}')\n",
    "print(f'\\nTop 20 GSF Patterns:')\n",
    "print(f'{\"Pattern\":<45} {\"Count\":<8} {\"% of Commits\":<12}')\n",
    "print('-' * 65)\n",
    "for pattern, count in sorted_patterns[:20]:\n",
    "    pct = count / len(all_commits) * 100 if all_commits else 0\n",
    "    print(f'{pattern:<45} {count:<8} {pct:.1f}%')\n",
    "\n",
    "# Pattern categories\n",
    "categories = set()\n",
    "for p in GSF_PATTERNS.values():\n",
    "    categories.add(p.get('category', 'Unknown'))\n",
    "print(f'\\nGSF Categories ({len(categories)}):')\n",
    "for cat in sorted(categories):\n",
    "    count = sum(1 for p in GSF_PATTERNS.values() if p.get('category') == cat)\n",
    "    print(f'  {cat}: {count} patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b757e",
   "metadata": {},
   "source": [
    "## Step 7: Green Awareness Detection\n",
    "\n",
    "Demonstrate keyword-based green awareness detection on sample commit messages\n",
    "and the pattern lookup API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedaf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "    'Optimize database queries for energy efficiency',\n",
    "    'Fix typo in README',\n",
    "    'Implement lazy loading for images to reduce bandwidth',\n",
    "    'Add unit tests for login',\n",
    "    'Reduce memory footprint of cache layer',\n",
    "    'Refactor to async I/O for better resource utilization',\n",
    "]\n",
    "\n",
    "print('Green Awareness Detection:')\n",
    "for msg in test_messages:\n",
    "    result = is_green_aware(msg)\n",
    "    print(f'  [{\"GREEN\" if result else \"-----\"}] {msg}')\n",
    "\n",
    "print('\\nPattern Lookup Examples:')\n",
    "for keyword in ['cache', 'lazy loading', 'compression', 'async']:\n",
    "    patterns = get_pattern_by_keywords(keyword)\n",
    "    if patterns:\n",
    "        names = [p['name'] for p in patterns[:3]]\n",
    "        print(f'  \"{keyword}\" -> {names}')\n",
    "    else:\n",
    "        print(f'  \"{keyword}\" -> no matching patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920baf5",
   "metadata": {},
   "source": [
    "## Step 8: Process Metrics\n",
    "\n",
    "Examine the process metrics collected during analysis: DMM (Delta Maintainability Model)\n",
    "scores for size, complexity, and interfacing, plus structural complexity metrics\n",
    "and method-level analysis via Lizard integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d55c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Process Metrics Summary')\n",
    "print('=' * 70)\n",
    "\n",
    "metrics_keys = [\n",
    "    'dmm_unit_size', 'dmm_unit_complexity', 'dmm_unit_interfacing',\n",
    "    'total_nloc', 'total_complexity', 'max_complexity',\n",
    "    'methods_count', 'files_modified', 'insertions', 'deletions',\n",
    "]\n",
    "\n",
    "metrics_data = {k: [] for k in metrics_keys}\n",
    "for commit in all_commits:\n",
    "    for key in metrics_keys:\n",
    "        val = commit.get(key)\n",
    "        if val is not None:\n",
    "            metrics_data[key].append(val)\n",
    "\n",
    "print(f'{\"Metric\":<25} {\"Avg\":>10} {\"Min\":>10} {\"Max\":>10} {\"N\":>6}')\n",
    "print('-' * 65)\n",
    "for metric, values in metrics_data.items():\n",
    "    if values:\n",
    "        avg = sum(values) / len(values)\n",
    "        print(f'{metric:<25} {avg:>10.2f} {min(values):>10.2f} {max(values):>10.2f} {len(values):>6}')\n",
    "\n",
    "# Method-level analysis\n",
    "total_methods = sum(len(c.get('methods', [])) for c in all_commits)\n",
    "print(f'\\nMethod-Level Analysis:')\n",
    "print(f'  Total methods analyzed: {total_methods}')\n",
    "\n",
    "for commit in all_commits:\n",
    "    methods = commit.get('methods', [])\n",
    "    if methods:\n",
    "        print(f'  Sample from {commit.get(\"repository\")} ({commit[\"hash\"][:8]}):')\n",
    "        for m in methods[:3]:\n",
    "            print(f'    {m.get(\"name\", \"N/A\")}: nloc={m.get(\"nloc\", 0)}, '\n",
    "                  f'complexity={m.get(\"complexity\", 0)}')\n",
    "        break\n",
    "\n",
    "# Source code changes\n",
    "total_src = sum(len(c.get('source_changes', [])) for c in all_commits)\n",
    "print(f'\\nSource code changes captured: {total_src}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327855e",
   "metadata": {},
   "source": [
    "## Step 9: Statistical Analysis\n",
    "\n",
    "Apply statistical methods to the combined dataset: pattern correlations,\n",
    "temporal trend significance, and effect sizes between green and non-green commits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_analyzer = StatisticalAnalyzer()\n",
    "\n",
    "commits_df = pd.DataFrame(all_commits)\n",
    "\n",
    "# Add binary indicator columns for each pattern\n",
    "all_pattern_names = list(pattern_counts.keys())\n",
    "for pattern in all_pattern_names:\n",
    "    commits_df[f'pattern_{pattern}'] = commits_df['gsf_patterns_matched'].apply(\n",
    "        lambda x, p=pattern: 1 if p in (x or []) else 0\n",
    "    )\n",
    "\n",
    "# Pattern correlations\n",
    "if len(all_pattern_names) >= 2:\n",
    "    corr = stat_analyzer.analyze_pattern_correlations(commits_df)\n",
    "    sig_pairs = corr.get('significant_pairs', [])\n",
    "    print(f'Pattern Correlation Analysis:')\n",
    "    print(f'  Significant pairs: {len(sig_pairs)}')\n",
    "    for pair in sig_pairs[:10]:\n",
    "        print(f'    {pair}')\n",
    "else:\n",
    "    print(f'Found {len(all_pattern_names)} pattern(s) - need >= 2 for correlation')\n",
    "\n",
    "# Temporal trend\n",
    "if 'date' in commits_df.columns and 'green_aware' in commits_df.columns:\n",
    "    if 'commit_hash' not in commits_df.columns:\n",
    "        commits_df['commit_hash'] = commits_df.get('hash', commits_df.index.astype(str))\n",
    "    trend_results = stat_analyzer.temporal_trend_analysis(commits_df)\n",
    "    trend = trend_results.get('trend', {})\n",
    "    print(f'\\nTemporal Trend:')\n",
    "    print(f'  Direction: {trend.get(\"direction\", \"N/A\")}')\n",
    "    print(f'  Significant: {trend.get(\"significant\", \"N/A\")}')\n",
    "    print(f'  Correlation: {trend.get(\"correlation\", \"N/A\")}')\n",
    "\n",
    "# Effect size: green vs non-green complexity\n",
    "green_cx = commits_df[commits_df['green_aware'] == True]['total_complexity'].dropna().tolist()\n",
    "non_green_cx = commits_df[commits_df['green_aware'] == False]['total_complexity'].dropna().tolist()\n",
    "\n",
    "if green_cx and non_green_cx:\n",
    "    effect = stat_analyzer.effect_size_analysis(green_cx, non_green_cx)\n",
    "    print(f'\\nEffect Size (Green vs Non-Green Complexity):')\n",
    "    print(f'  Cohen\\'s d: {effect[\"cohens_d\"]:.3f} ({effect[\"magnitude\"]})')\n",
    "    print(f'  Mean difference: {effect[\"mean_difference\"]:.2f}')\n",
    "    print(f'  Significant: {effect[\"significant\"]}')\n",
    "else:\n",
    "    print('\\nInsufficient data for effect size analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e64e5c",
   "metadata": {},
   "source": [
    "## Step 10: Temporal Analysis\n",
    "\n",
    "Analyze how green software practices evolve over time across all repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00245811",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal = TemporalAnalyzer(granularity='quarter')\n",
    "\n",
    "# Convert to analyzer's expected format\n",
    "analysis_results_fmt = []\n",
    "for c in all_commits:\n",
    "    analysis_results_fmt.append({\n",
    "        'commit_sha': c.get('hash', ''),\n",
    "        'is_green_aware': c.get('green_aware', False),\n",
    "        'patterns_detected': c.get('gsf_patterns_matched', []),\n",
    "        'detection_method': 'gsf_keyword',\n",
    "    })\n",
    "\n",
    "temporal_results = temporal.analyze_trends(all_commits, analysis_results_fmt)\n",
    "\n",
    "periods = temporal_results.get('periods', [])\n",
    "print(f'Temporal Analysis ({len(periods)} periods):')\n",
    "print(f'{\"Period\":<20} {\"Commits\":<10} {\"Green\":<10} {\"Rate\":<10} {\"Patterns\":<10}')\n",
    "print('-' * 60)\n",
    "for p in periods:\n",
    "    rate = p.get('green_awareness_rate', 0)\n",
    "    print(f'{p.get(\"period\", \"N/A\"):<20} {p.get(\"commit_count\", 0):<10} '\n",
    "          f'{p.get(\"green_commit_count\", 0):<10} {rate:.1%}      '\n",
    "          f'{p.get(\"unique_patterns\", 0)}')\n",
    "\n",
    "summary = temporal_results.get('summary', {})\n",
    "print(f'\\nTrend: {summary.get(\"overall_direction\", \"N/A\")}')\n",
    "print(f'Peak period: {summary.get(\"peak_period\", \"N/A\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269011c",
   "metadata": {},
   "source": [
    "## Step 11: Code Diff Pattern Signatures\n",
    "\n",
    "The CodeDiffAnalyzer detects green patterns directly in code changes.\n",
    "It is integrated into the analysis pipeline automatically. Here we inspect\n",
    "the pattern signatures it looks for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19562348",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_analyzer = CodeDiffAnalyzer()\n",
    "\n",
    "print(f'Code Diff Pattern Signatures: {len(diff_analyzer.PATTERN_SIGNATURES)} types')\n",
    "print('=' * 60)\n",
    "for name, data in diff_analyzer.PATTERN_SIGNATURES.items():\n",
    "    print(f'  {name}:')\n",
    "    if isinstance(data, dict):\n",
    "        for key, val in list(data.items())[:2]:\n",
    "            if isinstance(val, list):\n",
    "                print(f'    {key}: {val[:3]}...')\n",
    "            else:\n",
    "                print(f'    {key}: {val}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5886de9",
   "metadata": {},
   "source": [
    "## Step 12: Energy Measurement\n",
    "\n",
    "GreenMining provides multiple energy measurement backends:\n",
    "- **RAPL** — Linux kernel hardware counters (Intel/AMD, most accurate)\n",
    "- **CodeCarbon** — cross-platform (requires codecarbon package)\n",
    "- **CPU Meter** — universal (estimates from CPU utilization and TDP)\n",
    "- **Auto** — selects the best available backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a670021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available backends\n",
    "print('Available Energy Backends:')\n",
    "for backend in ['rapl', 'codecarbon', 'cpu_meter', 'auto']:\n",
    "    try:\n",
    "        m = get_energy_meter(backend)\n",
    "        print(f'  {backend}: available ({type(m).__name__})')\n",
    "    except Exception as e:\n",
    "        print(f'  {backend}: not available ({e})')\n",
    "\n",
    "# Measure a sample workload\n",
    "meter = CPUEnergyMeter()\n",
    "print(f'\\nCPU Energy Meter: available={meter.is_available()}')\n",
    "\n",
    "def sample_workload():\n",
    "    return sum(i ** 2 for i in range(1_000_000))\n",
    "\n",
    "result, energy = meter.measure(sample_workload)\n",
    "print(f'\\nSample Workload Measurement:')\n",
    "print(f'  Energy: {energy.joules:.4f} J')\n",
    "print(f'  Power avg: {energy.watts_avg:.2f} W')\n",
    "print(f'  Peak: {energy.watts_peak:.2f} W')\n",
    "print(f'  Duration: {energy.duration_seconds:.3f} s')\n",
    "print(f'  Backend: {energy.backend}')\n",
    "\n",
    "# Show energy from the repository analysis\n",
    "for r in results:\n",
    "    e = r.get('energy_metrics')\n",
    "    if e and e.get('joules', 0) > 0:\n",
    "        print(f'\\nAnalysis energy ({r[\"name\"]}):')\n",
    "        print(f'  Total: {e.get(\"joules\", 0):.4f} J')\n",
    "        print(f'  Avg power: {e.get(\"watts_avg\", 0):.2f} W')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af8d9a9",
   "metadata": {},
   "source": [
    "## Step 13: Power Regression Detection\n",
    "\n",
    "Detect commits that introduced energy regressions by measuring power before and after\n",
    "each commit. Requires a local repository with a runnable test suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = PowerRegressionDetector(\n",
    "    test_command=\"python -c 'sum(range(100000))'\",\n",
    "    energy_backend='cpu_meter',\n",
    "    threshold_percent=5.0,\n",
    "    iterations=3,\n",
    "    warmup_iterations=1,\n",
    ")\n",
    "\n",
    "print('PowerRegressionDetector configured:')\n",
    "print(f'  Test command: python -c \"sum(range(100000))\"')\n",
    "print(f'  Backend: cpu_meter')\n",
    "print(f'  Threshold: 5.0%')\n",
    "print(f'  Iterations: 3, Warmup: 1')\n",
    "print()\n",
    "print('Usage on a local repository:')\n",
    "print('  regressions = detector.detect(')\n",
    "print('      repo_path=\"./my-repo\",')\n",
    "print('      baseline_commit=\"HEAD~10\",')\n",
    "print('      target_commit=\"HEAD\",')\n",
    "print('  )')\n",
    "print('  for r in regressions:')\n",
    "print('      print(f\"{r.sha[:8]} | before={r.power_before:.2f}W | '\n",
    "      'after={r.power_after:.2f}W | regression={r.is_regression}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc02b12",
   "metadata": {},
   "source": [
    "## Step 14: Metrics-to-Power Correlation\n",
    "\n",
    "Analyze correlations between code metrics and energy consumption using\n",
    "Pearson and Spearman coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlator = MetricsPowerCorrelator(significance_level=0.05)\n",
    "\n",
    "metric_names = ['total_complexity', 'total_nloc', 'files_modified', 'insertions', 'deletions']\n",
    "metrics_values = {m: [] for m in metric_names}\n",
    "power_measurements = []\n",
    "\n",
    "for c in all_commits:\n",
    "    has_all = all(c.get(m) is not None for m in metric_names)\n",
    "    energy_val = c.get('energy_watts_avg') or c.get('energy_joules')\n",
    "    if has_all and energy_val:\n",
    "        for m in metric_names:\n",
    "            metrics_values[m].append(float(c[m]))\n",
    "        power_measurements.append(float(energy_val))\n",
    "\n",
    "if len(power_measurements) >= 3:\n",
    "    correlator.fit(metric_names, metrics_values, power_measurements)\n",
    "    summary = correlator.summary()\n",
    "    print(f'Metrics-to-Power Correlation:')\n",
    "    print(f'  Metrics analyzed: {summary[\"total_metrics\"]}')\n",
    "    print(f'  Significant: {summary[\"significant_count\"]}')\n",
    "    print()\n",
    "    for name, result in correlator.get_results().items():\n",
    "        print(f'  {name}:')\n",
    "        print(f'    Pearson r={result.pearson_r:.3f}, Spearman rho={result.spearman_rho:.3f}')\n",
    "        print(f'    Significant: {result.is_significant}')\n",
    "    print(f'\\nFeature Importance:')\n",
    "    for name, imp in correlator.feature_importance.items():\n",
    "        bar = '#' * int(imp * 30)\n",
    "        print(f'  {name:<20} {imp:.3f} {bar}')\n",
    "else:\n",
    "    print(f'Insufficient data ({len(power_measurements)} points, need >= 3)')\n",
    "    print('Enable energy_tracking=True to collect per-commit energy data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d24876b",
   "metadata": {},
   "source": [
    "## Step 15: Version Power Analysis\n",
    "\n",
    "Compare energy consumption across different software versions by checking out\n",
    "tags and running a test suite at each version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_analyzer = VersionPowerAnalyzer(\n",
    "    test_command=\"python -c 'sum(range(100000))'\",\n",
    "    energy_backend='cpu_meter',\n",
    "    iterations=5,\n",
    "    warmup_iterations=1,\n",
    ")\n",
    "\n",
    "print('VersionPowerAnalyzer configured:')\n",
    "print(f'  Backend: cpu_meter, Iterations: 5, Warmup: 1')\n",
    "print()\n",
    "print('Usage on a local repository with version tags:')\n",
    "print('  report = version_analyzer.analyze_versions(')\n",
    "print('      repo_path=\"./my-repo\",')\n",
    "print('      versions=[\"v1.0\", \"v2.0\", \"v3.0\"],')\n",
    "print('  )')\n",
    "print('  print(f\"Trend: {report.trend}\")')\n",
    "print('  print(f\"Total change: {report.total_change_percent:.1f}%\")')\n",
    "print('  print(f\"Most efficient: {report.most_efficient}\")')\n",
    "print('  for v in report.versions:')\n",
    "print('      print(f\"{v.version}: {v.power_watts_avg:.2f}W\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b9815",
   "metadata": {},
   "source": [
    "## Step 16: Visualization (matplotlib)\n",
    "\n",
    "Static charts from the unified analysis data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51effe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Green commit rate per repository\n",
    "repo_names = [r['name'][:20] for r in results]\n",
    "green_rates = [r['green_commit_rate'] for r in results]\n",
    "axes[0, 0].barh(repo_names, green_rates, color='green', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Green Commit Rate')\n",
    "axes[0, 0].set_title('Green Awareness by Repository')\n",
    "axes[0, 0].set_xlim(0, 1)\n",
    "\n",
    "# 2. Top 10 patterns\n",
    "if sorted_patterns:\n",
    "    top = sorted_patterns[:10]\n",
    "    axes[0, 1].barh([p[0][:30] for p in top], [p[1] for p in top], color='teal', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Count')\n",
    "    axes[0, 1].set_title('Top 10 GSF Patterns')\n",
    "\n",
    "# 3. Commits breakdown\n",
    "commit_counts = [r['total_commits'] for r in results]\n",
    "green_counts = [r['green_commits'] for r in results]\n",
    "x = range(len(results))\n",
    "axes[1, 0].bar(x, commit_counts, label='Total', alpha=0.7)\n",
    "axes[1, 0].bar(x, green_counts, label='Green', alpha=0.7)\n",
    "axes[1, 0].set_xticks(list(x))\n",
    "axes[1, 0].set_xticklabels(repo_names, rotation=45, ha='right', fontsize=7)\n",
    "axes[1, 0].set_title('Commit Breakdown')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Complexity distribution\n",
    "cxs = [c.get('total_complexity', 0) for c in all_commits if c.get('total_complexity')]\n",
    "if cxs:\n",
    "    axes[1, 1].hist(cxs, bins=20, color='orange', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Total Complexity')\n",
    "    axes[1, 1].set_title('Complexity Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved to data/analysis_plots.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e39a2",
   "metadata": {},
   "source": [
    "## Step 17: Interactive Visualization (Plotly)\n",
    "\n",
    "Interactive charts for deeper exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Sunburst: Repository -> Green/Non-Green -> Pattern\n",
    "sun_data = []\n",
    "for r in results:\n",
    "    for c in r.get('commits', []):\n",
    "        cat = 'Green' if c.get('green_aware') else 'Non-Green'\n",
    "        patterns = c.get('gsf_patterns_matched', [])\n",
    "        pat = patterns[0] if patterns else 'None'\n",
    "        sun_data.append({\n",
    "            'repository': r['name'][:20], 'category': cat,\n",
    "            'pattern': pat[:30], 'count': 1,\n",
    "        })\n",
    "\n",
    "if sun_data:\n",
    "    df_sun = pd.DataFrame(sun_data)\n",
    "    fig = px.sunburst(df_sun, path=['repository', 'category', 'pattern'],\n",
    "                      values='count', title='Repository Analysis Breakdown')\n",
    "    fig.show()\n",
    "\n",
    "# Scatter: Complexity vs NLOC\n",
    "sc = [{'cx': c['total_complexity'], 'nloc': c['total_nloc'],\n",
    "       'green': 'Green' if c.get('green_aware') else 'Non-Green',\n",
    "       'repo': c.get('repository', '')}\n",
    "      for c in all_commits if c.get('total_complexity') and c.get('total_nloc')]\n",
    "\n",
    "if sc:\n",
    "    fig2 = px.scatter(pd.DataFrame(sc), x='nloc', y='cx', color='green',\n",
    "                      hover_data=['repo'],\n",
    "                      title='Complexity vs Lines of Code')\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc209221",
   "metadata": {},
   "source": [
    "## Step 18: Export Results\n",
    "\n",
    "Export the unified analysis to JSON, CSV, and pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e06f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "with open('data/analysis_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "print('Exported data/analysis_results.json')\n",
    "\n",
    "# CSV (flattened commits)\n",
    "csv_rows = []\n",
    "for r in results:\n",
    "    for c in r.get('commits', []):\n",
    "        csv_rows.append({\n",
    "            'repository': r['name'],\n",
    "            'repo_url': r['url'],\n",
    "            'commit_hash': c.get('hash', ''),\n",
    "            'author': c.get('author', ''),\n",
    "            'date': c.get('date', ''),\n",
    "            'message': str(c.get('message', ''))[:100],\n",
    "            'green_aware': c.get('green_aware', False),\n",
    "            'patterns_matched': ', '.join(c.get('gsf_patterns_matched', [])),\n",
    "            'pattern_count': c.get('pattern_count', 0),\n",
    "            'confidence': c.get('confidence', ''),\n",
    "            'files_modified': c.get('files_modified', 0),\n",
    "            'insertions': c.get('insertions', 0),\n",
    "            'deletions': c.get('deletions', 0),\n",
    "            'dmm_unit_size': c.get('dmm_unit_size'),\n",
    "            'dmm_unit_complexity': c.get('dmm_unit_complexity'),\n",
    "            'dmm_unit_interfacing': c.get('dmm_unit_interfacing'),\n",
    "            'total_nloc': c.get('total_nloc'),\n",
    "            'total_complexity': c.get('total_complexity'),\n",
    "            'methods_count': c.get('methods_count'),\n",
    "            'energy_joules': c.get('energy_joules'),\n",
    "        })\n",
    "\n",
    "df_export = pd.DataFrame(csv_rows)\n",
    "df_export.to_csv('data/analysis_results.csv', index=False)\n",
    "print(f'Exported {len(csv_rows)} commits to data/analysis_results.csv')\n",
    "print(f'\\nDataFrame shape: {df_export.shape}')\n",
    "df_export.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bd6e9",
   "metadata": {},
   "source": [
    "## Step 19: Web Dashboard\n",
    "\n",
    "GreenMining includes a Flask-based dashboard for interactive exploration.\n",
    "The dashboard reads analysis data from a directory and exposes REST API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = create_app(data_dir='./data')\n",
    "\n",
    "print('Dashboard created successfully')\n",
    "print()\n",
    "print('API Endpoints:')\n",
    "print('  GET /              - Dashboard UI')\n",
    "print('  GET /api/repositories - Repository data')\n",
    "print('  GET /api/analysis    - Analysis results')\n",
    "print('  GET /api/statistics  - Aggregated statistics')\n",
    "print('  GET /api/energy      - Energy report')\n",
    "print('  GET /api/summary     - Summary metrics')\n",
    "print()\n",
    "print('To launch (in a terminal, not here):')\n",
    "print('  from greenmining.dashboard import run_dashboard')\n",
    "print('  run_dashboard(data_dir=\"./data\", host=\"127.0.0.1\", port=5000)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c55aa",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Repositories Analyzed\n",
    "- 10 blockchain repositories (GraphQL search)\n",
    "- 3 selected repositories (Flask, Requests, FastAPI)\n",
    "- **Total: 13 repositories** through a single unified pipeline\n",
    "\n",
    "## Features Applied to All Repositories\n",
    "\n",
    "| Feature | Status |\n",
    "|---------|--------|\n",
    "| GSF Pattern Detection (122 patterns, 15 categories) | Applied |\n",
    "| Process Metrics (DMM size, complexity, interfacing) | Applied |\n",
    "| Method-Level Analysis (per-function complexity) | Applied |\n",
    "| Source Code Capture (before/after) | Applied |\n",
    "| Energy Measurement (auto-detected backend) | Applied |\n",
    "| Statistical Analysis (correlations, effect sizes) | Applied |\n",
    "| Temporal Analysis (quarterly trends) | Applied |\n",
    "| Code Diff Pattern Signatures | Applied | \n",
    "| Power Regression Detection | Demonstrated |\n",
    "| Metrics-to-Power Correlation (Pearson/Spearman) | Applied |\n",
    "| Version Power Comparison | Demonstrated |\n",
    "| Visualization (matplotlib + plotly) | Applied |\n",
    "| Export (JSON, CSV, DataFrame) | Applied |\n",
    "| Web Dashboard (Flask REST API) | Applied |\n",
    "\n",
    "## Output Files\n",
    "- `data/analysis_results.json` — Full analysis data\n",
    "- `data/analysis_results.csv` — Flattened commit-level data\n",
    "- `data/analysis_plots.png` — Static visualizations\n",
    "- `data/validation_samples.json` — Qualitative validation samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
