{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5cbd4a",
   "metadata": {},
   "source": [
    "# GreenMining Experiment: Unified Repository Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates a complete analysis pipeline using the `greenmining` library.\n",
    "\n",
    "## Experiment Setup\n",
    "- **10 blockchain repositories** found via GraphQL search\n",
    "- **3 manually selected repositories** (Flask, Requests, FastAPI)\n",
    "- **Total: 13 repositories** — all analyzed with the same pipeline and ALL features enabled\n",
    "- **Commits per repository:** 20\n",
    "- **Min stars:** 3\n",
    "- **Languages:** Top 20 programming languages\n",
    "\n",
    "## Pipeline Structure\n",
    "1. **Data Gathering** — search + URL-based fetching for all 13 repos\n",
    "2. **Unified Analysis** — every feature applied to every repo equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae90321",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install greenmining[energy,dashboard] --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b5712c",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "Import all GreenMining modules needed for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bc5b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreenMining version: 1.1.4\n",
      "GSF Patterns: 122\n",
      "Green Keywords: 321\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import tracemalloc\n",
    "import pandas as pd\n",
    "\n",
    "import greenmining\n",
    "from greenmining import (\n",
    "    fetch_repositories,\n",
    "    analyze_repositories,\n",
    "    GSF_PATTERNS,\n",
    "    GREEN_KEYWORDS,\n",
    "    is_green_aware,\n",
    "    get_pattern_by_keywords,\n",
    ")\n",
    "from greenmining.analyzers import (\n",
    "    StatisticalAnalyzer,\n",
    "    TemporalAnalyzer,\n",
    "    CodeDiffAnalyzer,\n",
    "    PowerRegressionDetector,\n",
    "    MetricsPowerCorrelator,\n",
    "    VersionPowerAnalyzer,\n",
    ")\n",
    "from greenmining.energy import get_energy_meter, CPUEnergyMeter\n",
    "from greenmining.dashboard import create_app\n",
    "\n",
    "print(f'GreenMining version: {greenmining.__version__}')\n",
    "print(f'GSF Patterns: {len(GSF_PATTERNS)}')\n",
    "print(f'Green Keywords: {len(GREEN_KEYWORDS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211be8b4",
   "metadata": {},
   "source": [
    "## Step 2: Configuration\n",
    "\n",
    "GitHub token and analysis parameters shared across all repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b83d693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub token configured (github_p...)\n",
      "Max commits per repo: 20\n",
      "Min stars: 3\n",
      "Languages: 20\n",
      "Repo created: 2020-01-01 to 2023-12-31\n",
      "Repo pushed after: 2023-01-01\n",
      "Commit date range: 2023-01-01 to 2025-12-31\n"
     ]
    }
   ],
   "source": [
    "GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN', 'your_github_token_here')\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN', GITHUB_TOKEN)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "if GITHUB_TOKEN == 'your_github_token_here':\n",
    "    print('WARNING: Set GITHUB_TOKEN to run the search step.')\n",
    "else:\n",
    "    print(f'GitHub token configured ({GITHUB_TOKEN[:8]}...)')\n",
    "\n",
    "# Shared analysis parameters\n",
    "MAX_COMMITS = 20\n",
    "MIN_STARS = 3\n",
    "PARALLEL_WORKERS = 2\n",
    "\n",
    "# Date filters for repository search\n",
    "CREATED_AFTER = '2020-01-01'\n",
    "CREATED_BEFORE = '2023-12-31'\n",
    "PUSHED_AFTER = '2023-01-01'\n",
    "\n",
    "# Date filters for commit analysis\n",
    "COMMIT_DATE_FROM = '2023-01-01'\n",
    "COMMIT_DATE_TO = '2025-12-31'\n",
    "\n",
    "LANGUAGES = [\n",
    "    'Python', 'JavaScript', 'TypeScript', 'Java', 'C++',\n",
    "    'C#', 'Go', 'Rust', 'PHP', 'Ruby',\n",
    "    'Swift', 'Kotlin', 'Scala', 'R', 'MATLAB',\n",
    "    'Dart', 'Lua', 'Perl', 'Haskell', 'Elixir',\n",
    "]\n",
    "\n",
    "print(f'Max commits per repo: {MAX_COMMITS}')\n",
    "print(f'Min stars: {MIN_STARS}')\n",
    "print(f'Languages: {len(LANGUAGES)}')\n",
    "print(f'Repo created: {CREATED_AFTER} to {CREATED_BEFORE}')\n",
    "print(f'Repo pushed after: {PUSHED_AFTER}')\n",
    "print(f'Commit date range: {COMMIT_DATE_FROM} to {COMMIT_DATE_TO}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e76e4",
   "metadata": {},
   "source": [
    "---\n",
    "# Part B: Data Gathering\n",
    "\n",
    "## Step 3: Search Blockchain Repositories\n",
    "\n",
    "Use the GraphQL API to find 10 blockchain repositories matching our criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d52cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching up to 10 repositories...\n",
      "   Keywords: blockchain\n",
      "   Filters: min_stars=3\n",
      "   Created: 2020-01-01 to 2023-12-31\n",
      "   Pushed: 2023-01-01 to any\n",
      "GraphQL Search Query: blockchain stars:>=3 created:>=2020-01-01 created:<=2023-12-31 pushed:>=2023-01-01\n",
      "Rate Limit: 4999/5000 (cost: 1)\n",
      "Fetched 10 repositories using GraphQL\n",
      "Fetched 10 repositories\n",
      "   Saved to: data/repositories.json\n",
      "Found 10 blockchain repositories:\n",
      "   1. dvf/blockchain (8053 stars, C#)\n",
      "   2. itheima1/BlockChain (4635 stars, JavaScript)\n",
      "   3. chaozh/awesome-blockchain-cn (19003 stars, JavaScript)\n",
      "   4. anders94/blockchain-demo (5636 stars, Pug)\n",
      "   5. Chia-Network/chia-blockchain (10864 stars, Python)\n",
      "   6. Azure-Samples/blockchain (838 stars, HTML)\n",
      "   7. yeasy/blockchain_guide (7064 stars, Go)\n",
      "   8. calistus-igwilo/nitda-blockchain-scholarship (3083 stars, HTML)\n",
      "   9. smartcontractkit/full-blockchain-solidity-course-js (13958 stars, None)\n",
      "  10. Jeiwan/blockchain_go (4376 stars, Go)\n"
     ]
    }
   ],
   "source": [
    "search_repos = fetch_repositories(\n",
    "    github_token=GITHUB_TOKEN,\n",
    "    max_repos=10,\n",
    "    min_stars=MIN_STARS,\n",
    "    languages=LANGUAGES,\n",
    "    keywords='blockchain',\n",
    "    created_after=CREATED_AFTER,\n",
    "    created_before=CREATED_BEFORE,\n",
    "    pushed_after=PUSHED_AFTER,\n",
    ")\n",
    "\n",
    "print(f'Found {len(search_repos)} blockchain repositories:')\n",
    "for i, repo in enumerate(search_repos, 1):\n",
    "    print(f'  {i:2d}. {repo.full_name} ({repo.stars} stars, {repo.language})')\n",
    "\n",
    "search_urls = [repo.url for repo in search_repos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e7d73",
   "metadata": {},
   "source": [
    "## Step 4: Analyze All 13 Repositories\n",
    "\n",
    "Combine the 10 search results with 3 manually selected repositories, then run the\n",
    "full analysis pipeline on all of them at once with every feature enabled:\n",
    "- GSF pattern detection (122 patterns, 321 keywords)\n",
    "- Process metrics (DMM size, complexity, interfacing)\n",
    "- Method-level analysis (per-function complexity metrics)\n",
    "- Source code capture (before/after for each modified file)\n",
    "- Energy measurement (CPU-based tracking during analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f8f1fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total repositories: 13\n",
      "  Search results: 10\n",
      "  Manual selection: 3\n",
      "  Commit date range: 2023-01-01 to 2025-12-31\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "analyze_repositories() got an unexpected keyword argument 'since_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Analyze ALL repositories with ALL features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m raw_results = \u001b[43manalyze_repositories\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43murls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_commits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_COMMITS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPARALLEL_WORKERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43menergy_tracking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43menergy_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod_level_analysis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_source_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgithub_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGITHUB_TOKEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43msince_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCOMMIT_DATE_FROM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCOMMIT_DATE_TO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Convert dataclass results to dicts for uniform access\u001b[39;00m\n\u001b[32m     32\u001b[39m results = [r.to_dict() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m raw_results]\n",
      "\u001b[31mTypeError\u001b[39m: analyze_repositories() got an unexpected keyword argument 'since_date'"
     ]
    }
   ],
   "source": [
    "# 3 manually selected repositories\n",
    "manual_urls = [\n",
    "    'https://github.com/pallets/flask',\n",
    "    'https://github.com/psf/requests',\n",
    "    'https://github.com/tiangolo/fastapi',\n",
    "]\n",
    "\n",
    "# Combine all URLs\n",
    "all_urls = search_urls + manual_urls\n",
    "print(f'Total repositories: {len(all_urls)}')\n",
    "print(f'  Search results: {len(search_urls)}')\n",
    "print(f'  Manual selection: {len(manual_urls)}')\n",
    "print(f'  Commit date range: {COMMIT_DATE_FROM} to {COMMIT_DATE_TO}')\n",
    "print()\n",
    "\n",
    "# Analyze ALL repositories with ALL features\n",
    "raw_results = analyze_repositories(\n",
    "    urls=all_urls,\n",
    "    max_commits=MAX_COMMITS,\n",
    "    parallel_workers=PARALLEL_WORKERS,\n",
    "    output_format='dict',\n",
    "    energy_tracking=True,\n",
    "    energy_backend='auto',\n",
    "    method_level_analysis=True,\n",
    "    include_source_code=True,\n",
    "    github_token=GITHUB_TOKEN,\n",
    "    since_date=COMMIT_DATE_FROM,\n",
    "    to_date=COMMIT_DATE_TO,\n",
    ")\n",
    "\n",
    "# Convert dataclass results to dicts for uniform access\n",
    "results = [r.to_dict() for r in raw_results]\n",
    "\n",
    "print(f'\\nAnalysis complete: {len(results)} repositories')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcf7968",
   "metadata": {},
   "source": [
    "## Step 5: Results Overview\n",
    "\n",
    "Summary of the unified analysis across all repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d11144",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_commits = sum(r['total_commits'] for r in results)\n",
    "total_green = sum(r['green_commits'] for r in results)\n",
    "overall_rate = total_green / total_commits if total_commits > 0 else 0\n",
    "\n",
    "print('=' * 70)\n",
    "print('UNIFIED ANALYSIS SUMMARY')\n",
    "print('=' * 70)\n",
    "print(f'Repositories analyzed: {len(results)}')\n",
    "print(f'Total commits: {total_commits}')\n",
    "print(f'Green-aware commits: {total_green}')\n",
    "print(f'Overall green rate: {overall_rate:.1%}')\n",
    "print()\n",
    "print(f'{\"Repository\":<40} {\"Commits\":<10} {\"Green\":<10} {\"Rate\":<10}')\n",
    "print('-' * 70)\n",
    "for r in results:\n",
    "    rate = r['green_commit_rate'] if r['total_commits'] > 0 else 0\n",
    "    print(f'{r[\"name\"]:<40} {r[\"total_commits\"]:<10} {r[\"green_commits\"]:<10} {rate:.1%}')\n",
    "\n",
    "# Build flat commit list for use in all later analysis steps\n",
    "all_commits = []\n",
    "for r in results:\n",
    "    for c in r.get('commits', []):\n",
    "        c['repository'] = r['name']\n",
    "        all_commits.append(c)\n",
    "\n",
    "print(f'\\nFlattened commit pool: {len(all_commits)} commits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b3b23",
   "metadata": {},
   "source": [
    "---\n",
    "# Part C: Unified Analysis\n",
    "\n",
    "Every feature is applied to the combined dataset of all 13 repositories.\n",
    "\n",
    "## Step 6: GSF Pattern Analysis\n",
    "\n",
    "Examine the Green Software Foundation patterns detected across all repositories.\n",
    "GreenMining detects 122 patterns across 15 categories with 321 keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern frequency across all commits\n",
    "pattern_counts = {}\n",
    "for commit in all_commits:\n",
    "    for pattern in commit.get('gsf_patterns_matched', []):\n",
    "        pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1\n",
    "\n",
    "sorted_patterns = sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f'Unique patterns detected: {len(sorted_patterns)}')\n",
    "print(f'\\nTop 20 GSF Patterns:')\n",
    "print(f'{\"Pattern\":<45} {\"Count\":<8} {\"% of Commits\":<12}')\n",
    "print('-' * 65)\n",
    "for pattern, count in sorted_patterns[:20]:\n",
    "    pct = count / len(all_commits) * 100 if all_commits else 0\n",
    "    print(f'{pattern:<45} {count:<8} {pct:.1f}%')\n",
    "\n",
    "# Pattern categories\n",
    "categories = set()\n",
    "for p in GSF_PATTERNS.values():\n",
    "    categories.add(p.get('category', 'Unknown'))\n",
    "print(f'\\nGSF Categories ({len(categories)}):')\n",
    "for cat in sorted(categories):\n",
    "    count = sum(1 for p in GSF_PATTERNS.values() if p.get('category') == cat)\n",
    "    print(f'  {cat}: {count} patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b757e",
   "metadata": {},
   "source": [
    "## Step 7: Green Awareness Detection\n",
    "\n",
    "Demonstrate keyword-based green awareness detection on sample commit messages\n",
    "and the pattern lookup API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedaf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "    'Optimize database queries for energy efficiency',\n",
    "    'Fix typo in README',\n",
    "    'Implement lazy loading for images to reduce bandwidth',\n",
    "    'Add unit tests for login',\n",
    "    'Reduce memory footprint of cache layer',\n",
    "    'Refactor to async I/O for better resource utilization',\n",
    "]\n",
    "\n",
    "print('Green Awareness Detection:')\n",
    "for msg in test_messages:\n",
    "    result = is_green_aware(msg)\n",
    "    print(f'  [{\"GREEN\" if result else \"-----\"}] {msg}')\n",
    "\n",
    "print('\\nPattern Lookup Examples:')\n",
    "for keyword in ['cache', 'lazy loading', 'compression', 'async']:\n",
    "    patterns = get_pattern_by_keywords(keyword)\n",
    "    if patterns:\n",
    "        names = [p['name'] for p in patterns[:3]]\n",
    "        print(f'  \"{keyword}\" -> {names}')\n",
    "    else:\n",
    "        print(f'  \"{keyword}\" -> no matching patterns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920baf5",
   "metadata": {},
   "source": [
    "## Step 8: Process Metrics\n",
    "\n",
    "Examine the process metrics collected during analysis: DMM (Delta Maintainability Model)\n",
    "scores for size, complexity, and interfacing, plus structural complexity metrics\n",
    "and method-level analysis via Lizard integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d55c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Process Metrics Summary')\n",
    "print('=' * 70)\n",
    "\n",
    "metrics_keys = [\n",
    "    'dmm_unit_size', 'dmm_unit_complexity', 'dmm_unit_interfacing',\n",
    "    'total_nloc', 'total_complexity', 'max_complexity',\n",
    "    'methods_count', 'insertions', 'deletions',\n",
    "]\n",
    "\n",
    "metrics_data = {k: [] for k in metrics_keys}\n",
    "for commit in all_commits:\n",
    "    for key in metrics_keys:\n",
    "        val = commit.get(key)\n",
    "        if val is not None:\n",
    "            metrics_data[key].append(val)\n",
    "\n",
    "print(f'{\"Metric\":<25} {\"Avg\":>10} {\"Min\":>10} {\"Max\":>10} {\"N\":>6}')\n",
    "print('-' * 65)\n",
    "for metric, values in metrics_data.items():\n",
    "    if values:\n",
    "        avg = sum(values) / len(values)\n",
    "        print(f'{metric:<25} {avg:>10.2f} {min(values):>10.2f} {max(values):>10.2f} {len(values):>6}')\n",
    "\n",
    "# Method-level analysis\n",
    "total_methods = sum(len(c.get('methods', [])) for c in all_commits)\n",
    "print(f'\\nMethod-Level Analysis:')\n",
    "print(f'  Total methods analyzed: {total_methods}')\n",
    "\n",
    "for commit in all_commits:\n",
    "    methods = commit.get('methods', [])\n",
    "    if methods:\n",
    "        print(f'  Sample from {commit.get(\"repository\")} ({commit[\"commit_hash\"][:8]}):')\n",
    "        for m in methods[:3]:\n",
    "            print(f'    {m.get(\"name\", \"N/A\")}: nloc={m.get(\"nloc\", 0)}, '\n",
    "                  f'complexity={m.get(\"complexity\", 0)}')\n",
    "        break\n",
    "\n",
    "# Source code changes\n",
    "total_src = sum(len(c.get('source_changes', [])) for c in all_commits)\n",
    "print(f'\\nSource code changes captured: {total_src}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327855e",
   "metadata": {},
   "source": [
    "## Step 9: Statistical Analysis\n",
    "\n",
    "Apply statistical methods to the combined dataset: pattern correlations,\n",
    "temporal trend significance, and effect sizes between green and non-green commits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_analyzer = StatisticalAnalyzer()\n",
    "\n",
    "commits_df = pd.DataFrame(all_commits)\n",
    "\n",
    "# Add binary indicator columns for each pattern\n",
    "all_pattern_names = list(pattern_counts.keys())\n",
    "for pattern in all_pattern_names:\n",
    "    commits_df[f'pattern_{pattern}'] = commits_df['gsf_patterns_matched'].apply(\n",
    "        lambda x, p=pattern: 1 if p in (x or []) else 0\n",
    "    )\n",
    "\n",
    "# Pattern correlations\n",
    "if len(all_pattern_names) >= 2:\n",
    "    corr = stat_analyzer.analyze_pattern_correlations(commits_df)\n",
    "    sig_pairs = corr.get('significant_pairs', [])\n",
    "    print(f'Pattern Correlation Analysis:')\n",
    "    print(f'  Significant pairs: {len(sig_pairs)}')\n",
    "    for pair in sig_pairs[:10]:\n",
    "        print(f'    {pair}')\n",
    "else:\n",
    "    print(f'Found {len(all_pattern_names)} pattern(s) - need >= 2 for correlation')\n",
    "\n",
    "# Temporal trend\n",
    "if 'date' in commits_df.columns and 'green_aware' in commits_df.columns:\n",
    "    if 'commit_hash' not in commits_df.columns:\n",
    "        commits_df['commit_hash'] = commits_df.index.astype(str)\n",
    "    trend_results = stat_analyzer.temporal_trend_analysis(commits_df)\n",
    "    trend = trend_results.get('trend', {})\n",
    "    print(f'\\nTemporal Trend:')\n",
    "    print(f'  Direction: {trend.get(\"direction\", \"N/A\")}')\n",
    "    print(f'  Significant: {trend.get(\"significant\", \"N/A\")}')\n",
    "    print(f'  Correlation: {trend.get(\"correlation\", \"N/A\")}')\n",
    "\n",
    "# Effect size: green vs non-green complexity\n",
    "green_cx = commits_df[commits_df['green_aware'] == True]['total_complexity'].dropna().tolist()\n",
    "non_green_cx = commits_df[commits_df['green_aware'] == False]['total_complexity'].dropna().tolist()\n",
    "\n",
    "if green_cx and non_green_cx:\n",
    "    effect = stat_analyzer.effect_size_analysis(green_cx, non_green_cx)\n",
    "    print(f'\\nEffect Size (Green vs Non-Green Complexity):')\n",
    "    print(f'  Cohen\\'s d: {effect[\"cohens_d\"]:.3f} ({effect[\"magnitude\"]})')\n",
    "    print(f'  Mean difference: {effect[\"mean_difference\"]:.2f}')\n",
    "    print(f'  Significant: {effect[\"significant\"]}')\n",
    "else:\n",
    "    print('\\nInsufficient data for effect size analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e64e5c",
   "metadata": {},
   "source": [
    "## Step 10: Temporal Analysis\n",
    "\n",
    "Analyze how green software practices evolve over time across all repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00245811",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal = TemporalAnalyzer(granularity='quarter')\n",
    "\n",
    "# Convert to analyzer's expected format\n",
    "analysis_results_fmt = []\n",
    "for c in all_commits:\n",
    "    analysis_results_fmt.append({\n",
    "        'commit_sha': c.get('commit_hash', ''),\n",
    "        'is_green_aware': c.get('green_aware', False),\n",
    "        'patterns_detected': c.get('gsf_patterns_matched', []),\n",
    "        'detection_method': 'gsf_keyword',\n",
    "    })\n",
    "\n",
    "temporal_results = temporal.analyze_trends(all_commits, analysis_results_fmt)\n",
    "\n",
    "periods = temporal_results.get('periods', [])\n",
    "print(f'Temporal Analysis ({len(periods)} periods):')\n",
    "print(f'{\"Period\":<20} {\"Commits\":<10} {\"Green\":<10} {\"Rate\":<10} {\"Patterns\":<10}')\n",
    "print('-' * 60)\n",
    "for p in periods:\n",
    "    rate = p.get('green_awareness_rate', 0)\n",
    "    print(f'{p.get(\"period\", \"N/A\"):<20} {p.get(\"commit_count\", 0):<10} '\n",
    "          f'{p.get(\"green_commit_count\", 0):<10} {rate:.1%}      '\n",
    "          f'{p.get(\"unique_patterns\", 0)}')\n",
    "\n",
    "summary = temporal_results.get('summary', {})\n",
    "print(f'\\nTrend: {summary.get(\"overall_direction\", \"N/A\")}')\n",
    "print(f'Peak period: {summary.get(\"peak_period\", \"N/A\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269011c",
   "metadata": {},
   "source": [
    "## Step 11: Code Diff Pattern Signatures\n",
    "\n",
    "The CodeDiffAnalyzer detects green patterns directly in code changes.\n",
    "It is integrated into the analysis pipeline automatically. Here we inspect\n",
    "the pattern signatures it looks for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19562348",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_analyzer = CodeDiffAnalyzer()\n",
    "\n",
    "print(f'Code Diff Pattern Signatures: {len(diff_analyzer.PATTERN_SIGNATURES)} types')\n",
    "print('=' * 60)\n",
    "for name, data in diff_analyzer.PATTERN_SIGNATURES.items():\n",
    "    print(f'  {name}:')\n",
    "    if isinstance(data, dict):\n",
    "        for key, val in list(data.items())[:2]:\n",
    "            if isinstance(val, list):\n",
    "                print(f'    {key}: {val[:3]}...')\n",
    "            else:\n",
    "                print(f'    {key}: {val}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5886de9",
   "metadata": {},
   "source": [
    "## Step 12: Energy Measurement\n",
    "\n",
    "GreenMining provides multiple energy measurement backends:\n",
    "- **RAPL** — Linux kernel hardware counters (Intel/AMD, most accurate)\n",
    "- **CPU Meter** — universal (estimates from CPU utilization and TDP)\n",
    "- **tracemalloc** — Python standard library for memory usage profiling\n",
    "- **CodeCarbon** — cross-platform CO2 emissions tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a670021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check available energy backends\n",
    "print('Available Energy Backends:')\n",
    "for backend in ['rapl', 'codecarbon', 'cpu_meter', 'auto']:\n",
    "    try:\n",
    "        m = get_energy_meter(backend)\n",
    "        print(f'  {backend}: available ({type(m).__name__})')\n",
    "    except Exception as e:\n",
    "        print(f'  {backend}: not available ({e})')\n",
    "\n",
    "# 2. RAPL measurement (Linux Intel/AMD)\n",
    "print('\\n--- RAPL Energy Meter ---')\n",
    "try:\n",
    "    from greenmining.energy import RAPLEnergyMeter\n",
    "    rapl = RAPLEnergyMeter()\n",
    "    if rapl.is_available():\n",
    "        rapl.start()\n",
    "        _ = sum(i ** 2 for i in range(500_000))\n",
    "        rapl_metrics = rapl.stop()\n",
    "        print(f'  Energy: {rapl_metrics.energy_joules:.4f} J')\n",
    "        print(f'  Power avg: {rapl_metrics.average_power_watts:.2f} W')\n",
    "        print(f'  Duration: {rapl_metrics.duration_seconds:.3f} s')\n",
    "    else:\n",
    "        print('  RAPL not available (requires Linux with Intel/AMD CPU)')\n",
    "except Exception as e:\n",
    "    print(f'  RAPL error: {e}')\n",
    "\n",
    "# 3. CPU Meter measurement (universal)\n",
    "print('\\n--- CPU Energy Meter ---')\n",
    "meter = CPUEnergyMeter()\n",
    "\n",
    "def sample_workload():\n",
    "    return sum(i ** 2 for i in range(1_000_000))\n",
    "\n",
    "result, energy = meter.measure(sample_workload)\n",
    "print(f'  Energy: {energy.joules:.4f} J')\n",
    "print(f'  Power avg: {energy.watts_avg:.2f} W')\n",
    "print(f'  Duration: {energy.duration_seconds:.3f} s')\n",
    "print(f'  Backend: {energy.backend}')\n",
    "\n",
    "# 4. tracemalloc memory profiling\n",
    "print('\\n--- tracemalloc Memory Profiling ---')\n",
    "tracemalloc.start()\n",
    "_ = sample_workload()\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "print(f'  Current memory: {current / 1024:.1f} KB')\n",
    "print(f'  Peak memory: {peak / 1024:.1f} KB')\n",
    "\n",
    "# 5. CodeCarbon CO2 tracking\n",
    "print('\\n--- CodeCarbon CO2 Tracking ---')\n",
    "try:\n",
    "    from codecarbon import EmissionsTracker\n",
    "    tracker = EmissionsTracker(log_level='error', save_to_file=False)\n",
    "    tracker.start()\n",
    "    _ = sample_workload()\n",
    "    emissions = tracker.stop()\n",
    "    print(f'  CO2 emissions: {emissions:.8f} kg')\n",
    "    print(f'  Equivalent: {emissions * 1e6:.4f} mg CO2')\n",
    "except ImportError:\n",
    "    print('  CodeCarbon not installed (pip install codecarbon)')\n",
    "except Exception as e:\n",
    "    print(f'  CodeCarbon error: {e}')\n",
    "\n",
    "# 6. Show energy from the repository analysis\n",
    "print('\\n--- Analysis Energy (from repository pipeline) ---')\n",
    "for r in results:\n",
    "    e = r.get('energy_metrics')\n",
    "    if e and e.get('joules', 0) > 0:\n",
    "        print(f'  {r[\"name\"]}:')\n",
    "        print(f'    Total: {e.get(\"joules\", 0):.4f} J')\n",
    "        print(f'    Avg power: {e.get(\"watts_avg\", 0):.2f} W')\n",
    "        break\n",
    "else:\n",
    "    print('  No per-repo energy data collected (backend may not support it)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af8d9a9",
   "metadata": {},
   "source": [
    "## Step 13: Power Regression Detection\n",
    "\n",
    "Detect commits that introduced energy regressions by measuring power before and after\n",
    "each commit. Requires a local repository with a runnable test suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = PowerRegressionDetector(\n",
    "    test_command=\"python -c 'sum(range(100000))'\",\n",
    "    energy_backend='cpu_meter',\n",
    "    threshold_percent=5.0,\n",
    "    iterations=3,\n",
    "    warmup_iterations=1,\n",
    ")\n",
    "\n",
    "print('PowerRegressionDetector configured:')\n",
    "print(f'  Test command: python -c \"sum(range(100000))\"')\n",
    "print(f'  Backend: cpu_meter')\n",
    "print(f'  Threshold: 5.0%')\n",
    "print(f'  Iterations: 3, Warmup: 1')\n",
    "print()\n",
    "print('Usage on a local repository:')\n",
    "print('  regressions = detector.detect(')\n",
    "print('      repo_path=\"./my-repo\",')\n",
    "print('      baseline_commit=\"HEAD~10\",')\n",
    "print('      target_commit=\"HEAD\",')\n",
    "print('  )')\n",
    "print('  for r in regressions:')\n",
    "print('      print(f\"{r.sha[:8]} | before={r.power_before:.2f}W | '\n",
    "      'after={r.power_after:.2f}W | regression={r.is_regression}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc02b12",
   "metadata": {},
   "source": [
    "## Step 14: Metrics-to-Power Correlation\n",
    "\n",
    "Analyze correlations between code metrics and energy consumption using\n",
    "Pearson and Spearman coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlator = MetricsPowerCorrelator(significance_level=0.05)\n",
    "\n",
    "metric_names = ['total_complexity', 'total_nloc', 'files_modified', 'insertions', 'deletions']\n",
    "metrics_values = {m: [] for m in metric_names}\n",
    "power_measurements = []\n",
    "\n",
    "for c in all_commits:\n",
    "    has_all = all(c.get(m) is not None for m in metric_names)\n",
    "    energy_val = c.get('energy_watts_avg') or c.get('energy_joules')\n",
    "    if has_all and energy_val:\n",
    "        for m in metric_names:\n",
    "            metrics_values[m].append(float(c[m]))\n",
    "        power_measurements.append(float(energy_val))\n",
    "\n",
    "if len(power_measurements) >= 3:\n",
    "    correlator.fit(metric_names, metrics_values, power_measurements)\n",
    "    summary = correlator.summary()\n",
    "    print(f'Metrics-to-Power Correlation:')\n",
    "    print(f'  Metrics analyzed: {summary[\"total_metrics\"]}')\n",
    "    print(f'  Significant: {summary[\"significant_count\"]}')\n",
    "    print()\n",
    "    for name, result in correlator.get_results().items():\n",
    "        print(f'  {name}:')\n",
    "        print(f'    Pearson r={result.pearson_r:.3f}, Spearman rho={result.spearman_rho:.3f}')\n",
    "        print(f'    Significant: {result.is_significant}')\n",
    "    print(f'\\nFeature Importance:')\n",
    "    for name, imp in correlator.feature_importance.items():\n",
    "        bar = '#' * int(imp * 30)\n",
    "        print(f'  {name:<20} {imp:.3f} {bar}')\n",
    "else:\n",
    "    print(f'Insufficient data ({len(power_measurements)} points, need >= 3)')\n",
    "    print('Enable energy_tracking=True to collect per-commit energy data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d24876b",
   "metadata": {},
   "source": [
    "## Step 15: Version Power Analysis\n",
    "\n",
    "Compare energy consumption across different software versions by checking out\n",
    "tags and running a test suite at each version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_analyzer = VersionPowerAnalyzer(\n",
    "    test_command=\"python -c 'sum(range(100000))'\",\n",
    "    energy_backend='cpu_meter',\n",
    "    iterations=5,\n",
    "    warmup_iterations=1,\n",
    ")\n",
    "\n",
    "print('VersionPowerAnalyzer configured:')\n",
    "print(f'  Backend: cpu_meter, Iterations: 5, Warmup: 1')\n",
    "print()\n",
    "print('Usage on a local repository with version tags:')\n",
    "print('  report = version_analyzer.analyze_versions(')\n",
    "print('      repo_path=\"./my-repo\",')\n",
    "print('      versions=[\"v1.0\", \"v2.0\", \"v3.0\"],')\n",
    "print('  )')\n",
    "print('  print(f\"Trend: {report.trend}\")')\n",
    "print('  print(f\"Total change: {report.total_change_percent:.1f}%\")')\n",
    "print('  print(f\"Most efficient: {report.most_efficient}\")')\n",
    "print('  for v in report.versions:')\n",
    "print('      print(f\"{v.version}: {v.power_watts_avg:.2f}W\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b9815",
   "metadata": {},
   "source": [
    "## Step 16: Visualization (matplotlib)\n",
    "\n",
    "Static charts from the unified analysis data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51effe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Green commit rate per repository\n",
    "repo_names = [r['name'][:20] for r in results]\n",
    "green_rates = [r['green_commit_rate'] for r in results]\n",
    "axes[0, 0].barh(repo_names, green_rates, color='green', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Green Commit Rate')\n",
    "axes[0, 0].set_title('Green Awareness by Repository')\n",
    "axes[0, 0].set_xlim(0, 1)\n",
    "\n",
    "# 2. Top 10 patterns\n",
    "if sorted_patterns:\n",
    "    top = sorted_patterns[:10]\n",
    "    axes[0, 1].barh([p[0][:30] for p in top], [p[1] for p in top], color='teal', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Count')\n",
    "    axes[0, 1].set_title('Top 10 GSF Patterns')\n",
    "\n",
    "# 3. Commits breakdown\n",
    "commit_counts = [r['total_commits'] for r in results]\n",
    "green_counts = [r['green_commits'] for r in results]\n",
    "x = range(len(results))\n",
    "axes[1, 0].bar(x, commit_counts, label='Total', alpha=0.7)\n",
    "axes[1, 0].bar(x, green_counts, label='Green', alpha=0.7)\n",
    "axes[1, 0].set_xticks(list(x))\n",
    "axes[1, 0].set_xticklabels(repo_names, rotation=45, ha='right', fontsize=7)\n",
    "axes[1, 0].set_title('Commit Breakdown')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Complexity distribution\n",
    "cxs = [c.get('total_complexity', 0) for c in all_commits if c.get('total_complexity')]\n",
    "if cxs:\n",
    "    axes[1, 1].hist(cxs, bins=20, color='orange', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Total Complexity')\n",
    "    axes[1, 1].set_title('Complexity Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/analysis_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved to data/analysis_plots.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e39a2",
   "metadata": {},
   "source": [
    "## Step 17: Interactive Visualization (Plotly)\n",
    "\n",
    "Interactive charts for deeper exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Sunburst: Repository -> Green/Non-Green -> Pattern\n",
    "sun_data = []\n",
    "for r in results:\n",
    "    for c in r.get('commits', []):\n",
    "        cat = 'Green' if c.get('green_aware') else 'Non-Green'\n",
    "        patterns = c.get('gsf_patterns_matched', [])\n",
    "        pat = patterns[0] if patterns else 'None'\n",
    "        sun_data.append({\n",
    "            'repository': r['name'][:20], 'category': cat,\n",
    "            'pattern': pat[:30], 'count': 1,\n",
    "        })\n",
    "\n",
    "if sun_data:\n",
    "    df_sun = pd.DataFrame(sun_data)\n",
    "    fig = px.sunburst(df_sun, path=['repository', 'category', 'pattern'],\n",
    "                      values='count', title='Repository Analysis Breakdown')\n",
    "    fig.show()\n",
    "\n",
    "# Scatter: Complexity vs NLOC\n",
    "sc = [{'cx': c['total_complexity'], 'nloc': c['total_nloc'],\n",
    "       'green': 'Green' if c.get('green_aware') else 'Non-Green',\n",
    "       'repo': c.get('repository', '')}\n",
    "      for c in all_commits if c.get('total_complexity') and c.get('total_nloc')]\n",
    "\n",
    "if sc:\n",
    "    fig2 = px.scatter(pd.DataFrame(sc), x='nloc', y='cx', color='green',\n",
    "                      hover_data=['repo'],\n",
    "                      title='Complexity vs Lines of Code')\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc209221",
   "metadata": {},
   "source": [
    "## Step 18: Export Results\n",
    "\n",
    "Export the unified analysis to JSON, CSV, and pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e06f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "with open('data/analysis_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "print('Exported data/analysis_results.json')\n",
    "\n",
    "# CSV (flattened commits)\n",
    "csv_rows = []\n",
    "for r in results:\n",
    "    for c in r.get('commits', []):\n",
    "        csv_rows.append({\n",
    "            'repository': r['name'],\n",
    "            'repo_url': r['url'],\n",
    "            'commit_hash': c.get('commit_hash', ''),\n",
    "            'author': c.get('author', ''),\n",
    "            'date': c.get('date', ''),\n",
    "            'message': str(c.get('message', ''))[:100],\n",
    "            'green_aware': c.get('green_aware', False),\n",
    "            'patterns_matched': ', '.join(c.get('gsf_patterns_matched', [])),\n",
    "            'pattern_count': c.get('pattern_count', 0),\n",
    "            'confidence': c.get('confidence', ''),\n",
    "            'files_modified': len(c.get('files_modified', [])) if isinstance(c.get('files_modified'), list) else c.get('files_modified', 0),\n",
    "            'insertions': c.get('insertions', 0),\n",
    "            'deletions': c.get('deletions', 0),\n",
    "            'dmm_unit_size': c.get('dmm_unit_size'),\n",
    "            'dmm_unit_complexity': c.get('dmm_unit_complexity'),\n",
    "            'dmm_unit_interfacing': c.get('dmm_unit_interfacing'),\n",
    "            'total_nloc': c.get('total_nloc'),\n",
    "            'total_complexity': c.get('total_complexity'),\n",
    "            'methods_count': c.get('methods_count'),\n",
    "            'energy_joules': c.get('energy_joules'),\n",
    "        })\n",
    "\n",
    "df_export = pd.DataFrame(csv_rows)\n",
    "df_export.to_csv('data/analysis_results.csv', index=False)\n",
    "print(f'Exported {len(csv_rows)} commits to data/analysis_results.csv')\n",
    "print(f'\\nDataFrame shape: {df_export.shape}')\n",
    "df_export.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bd6e9",
   "metadata": {},
   "source": [
    "## Step 19: Web Dashboard\n",
    "\n",
    "GreenMining includes a Flask-based dashboard for interactive exploration.\n",
    "The dashboard reads analysis data from a directory and exposes REST API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = create_app(data_dir='./data')\n",
    "\n",
    "print('Dashboard created successfully')\n",
    "print()\n",
    "print('API Endpoints:')\n",
    "print('  GET /              - Dashboard UI')\n",
    "print('  GET /api/repositories - Repository data')\n",
    "print('  GET /api/analysis    - Analysis results')\n",
    "print('  GET /api/statistics  - Aggregated statistics')\n",
    "print('  GET /api/energy      - Energy report')\n",
    "print('  GET /api/summary     - Summary metrics')\n",
    "print()\n",
    "print('To launch (in a terminal, not here):')\n",
    "print('  from greenmining.dashboard import run_dashboard')\n",
    "print('  run_dashboard(data_dir=\"./data\", host=\"127.0.0.1\", port=5000)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c55aa",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Repositories Analyzed\n",
    "- 10 blockchain repositories (GraphQL search, created 2020-2023, pushed after 2023)\n",
    "- 3 selected repositories (Flask, Requests, FastAPI)\n",
    "- **Total: 13 repositories** through a single unified pipeline\n",
    "- **Commit date range:** 2023-01-01 to 2025-12-31\n",
    "\n",
    "## Features Applied to All Repositories\n",
    "\n",
    "| Feature | Status |\n",
    "|---------|--------|\n",
    "| GSF Pattern Detection (122 patterns, 15 categories) | Applied |\n",
    "| Process Metrics (DMM size, complexity, interfacing) | Applied |\n",
    "| Method-Level Analysis (per-function complexity) | Applied |\n",
    "| Source Code Capture (before/after) | Applied |\n",
    "| Energy Measurement (RAPL + CPU Meter) | Applied |\n",
    "| Memory Profiling (tracemalloc) | Applied |\n",
    "| CO2 Emissions (CodeCarbon) | Applied |\n",
    "| Statistical Analysis (correlations, effect sizes) | Applied |\n",
    "| Temporal Analysis (quarterly trends) | Applied |\n",
    "| Code Diff Pattern Signatures | Applied |\n",
    "| Power Regression Detection | Demonstrated |\n",
    "| Metrics-to-Power Correlation (Pearson/Spearman) | Applied |\n",
    "| Version Power Comparison | Demonstrated |\n",
    "| Visualization (matplotlib + plotly) | Applied |\n",
    "| Export (JSON, CSV, DataFrame) | Applied |\n",
    "| Web Dashboard (Flask REST API) | Applied |\n",
    "\n",
    "## Output Files\n",
    "- `data/analysis_results.json` — Full analysis data\n",
    "- `data/analysis_results.csv` — Flattened commit-level data\n",
    "- `data/analysis_plots.png` — Static visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
