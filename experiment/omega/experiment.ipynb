{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GreenMining Experiment Notebook\n",
    "\n",
    "A comprehensive experiment demonstrating all features of the `greenmining` library for Mining Software Repositories (MSR) in Green IT research.\n",
    "\n",
    "**Experiment setup:**\n",
    "- **Search experiment**: 10 blockchain repositories fetched via GitHub GraphQL API\n",
    "- **Selected repositories**: 2 handpicked repos (`pallets/flask`, `psf/requests`)\n",
    "- **Single repository**: 1 deep analysis repo (`tiangolo/fastapi`)\n",
    "- **Commits per repo**: 20\n",
    "- **Minimum stars**: 3\n",
    "- **Languages**: Top 20 programming languages\n",
    "- **All features enabled**: Energy tracking, method-level analysis, source code access, carbon reporting, power regression, correlation analysis, version comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Installation\n",
    "\n",
    "Install `greenmining` from PyPI with all optional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install greenmining python-dotenv tqdm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Library Overview\n",
    "\n",
    "Verify the installation and inspect the library version, available pattern count, and module structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greenmining version: 1.0.8\n",
      "Total GSF patterns: 122\n",
      "Green keywords count: 321\n",
      "\n",
      "Public API: ['Config', 'GSF_PATTERNS', 'GREEN_KEYWORDS', 'is_green_aware', 'get_pattern_by_keywords', 'fetch_repositories', 'analyze_repositories', '__version__']\n"
     ]
    }
   ],
   "source": [
    "import greenmining\n",
    "\n",
    "print(f\"greenmining version: {greenmining.__version__}\")\n",
    "print(f\"Total GSF patterns: {len(greenmining.GSF_PATTERNS)}\")\n",
    "print(f\"Green keywords count: {len(greenmining.GREEN_KEYWORDS)}\")\n",
    "print(f\"\\nPublic API: {greenmining.__all__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Pattern Detection\n",
    "\n",
    "The GSF (Green Software Foundation) pattern catalog contains 122 sustainability patterns across 15 categories. Each pattern has associated keywords, SCI impact classification, and descriptive metadata.\n",
    "\n",
    "### 3.1 Explore Pattern Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSF Pattern Categories:\n",
      "----------------------------------------\n",
      "  cloud                 40 patterns\n",
      "  ai                    19 patterns\n",
      "  web                   17 patterns\n",
      "  general                8 patterns\n",
      "  network                6 patterns\n",
      "  database               5 patterns\n",
      "  code                   4 patterns\n",
      "  microservices          4 patterns\n",
      "  infrastructure         4 patterns\n",
      "  data                   3 patterns\n",
      "  async                  3 patterns\n",
      "  monitoring             3 patterns\n",
      "  networking             2 patterns\n",
      "  resource               2 patterns\n",
      "  caching                2 patterns\n",
      "                     ---\n",
      "  TOTAL                122\n"
     ]
    }
   ],
   "source": [
    "from greenmining import GSF_PATTERNS, is_green_aware, get_pattern_by_keywords\n",
    "\n",
    "# Count patterns per category\n",
    "categories = {}\n",
    "for pid, pattern in GSF_PATTERNS.items():\n",
    "    cat = pattern[\"category\"]\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "print(\"GSF Pattern Categories:\")\n",
    "print(\"-\" * 40)\n",
    "for cat, count in sorted(categories.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {cat:20s} {count:3d} patterns\")\n",
    "print(f\"{'':20s} {'---':>3s}\")\n",
    "print(f\"  {'TOTAL':20s} {sum(categories.values()):3d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Green Awareness Detection\n",
    "\n",
    "Test the keyword-based green awareness detection on sample commit messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit Message                                          Green?   Patterns\n",
      "====================================================================================================\n",
      "Optimize Redis caching to reduce energy consumption     True     ['Cache Static Data', 'Optimize Storage Resource Utilization', 'Optimize Average CPU Utilization', 'Reduce Transmitted Data', 'Use Energy Efficient Hardware', 'Keep Request Counts Low']\n",
      "Add auto-scaling to handle peak traffic efficiently     True     ['Optimize Peak CPU Utilization', 'Use Energy Efficient Hardware', 'Efficient Format for Model Training', 'Energy Efficient Models']\n",
      "Fix typo in README                                      False    []\n",
      "Implement lazy loading for images to reduce bandwidth   True     ['Reduce Transmitted Data', 'Scale Infrastructure with User Load', 'Defer Offscreen Images', 'Keep Request Counts Low', 'Properly Sized Images', 'Serve Images in Modern Formats', 'Lazy Loading', 'Pagination & Lazy Loading']\n",
      "Refactor database queries with connection pooling       True     ['Reduce Transmitted Data', 'Connection Pooling']\n",
      "Update CI pipeline configuration                        False    []\n",
      "Switch to gRPC for efficient service communication      True     ['Use Energy Efficient Hardware', 'Efficient Format for Model Training', 'Energy Efficient Models', 'gRPC for Service Communication']\n",
      "Enable model pruning to reduce inference cost           True     ['Reduce Transmitted Data', 'Keep Request Counts Low', 'Compress ML Models for Inference', 'Energy Efficient Models', 'Model Pruning']\n"
     ]
    }
   ],
   "source": [
    "test_messages = [\n",
    "    \"Optimize Redis caching to reduce energy consumption\",\n",
    "    \"Add auto-scaling to handle peak traffic efficiently\",\n",
    "    \"Fix typo in README\",\n",
    "    \"Implement lazy loading for images to reduce bandwidth\",\n",
    "    \"Refactor database queries with connection pooling\",\n",
    "    \"Update CI pipeline configuration\",\n",
    "    \"Switch to gRPC for efficient service communication\",\n",
    "    \"Enable model pruning to reduce inference cost\",\n",
    "]\n",
    "\n",
    "print(f\"{'Commit Message':<55} {'Green?':<8} {'Patterns'}\")\n",
    "print(\"=\" * 100)\n",
    "for msg in test_messages:\n",
    "    green = is_green_aware(msg)\n",
    "    patterns = get_pattern_by_keywords(msg) if green else []\n",
    "    print(f\"{msg:<55} {str(green):<8} {patterns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Inspect Individual Pattern Details\n",
    "\n",
    "Examine the structure of a specific pattern including its keywords, SCI impact, and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: cache_static_data\n",
      "  Name:        Cache Static Data\n",
      "  Category:    cloud\n",
      "  SCI Impact:  Reduces energy by minimizing redundant compute and network operations\n",
      "  Keywords:    ['cache', 'caching', 'static', 'cdn', 'redis']...\n",
      "  Description: Cache static content to reduce server load and network transfers...\n",
      "\n",
      "ID: choose_region_closest\n",
      "  Name:        Choose Region Closest to Users\n",
      "  Category:    cloud\n",
      "  SCI Impact:  Less energy for network transmission, lower latency\n",
      "  Keywords:    ['region', 'closest', 'proximity', 'latency', 'location']...\n",
      "  Description: Deploy in regions closest to users to reduce network distance...\n",
      "\n",
      "ID: compress_stored_data\n",
      "  Name:        Compress Stored Data\n",
      "  Category:    cloud\n",
      "  SCI Impact:  Lower embodied carbon from reduced storage infrastructure\n",
      "  Keywords:    ['compress', 'compression', 'stored', 'storage', 'gzip']...\n",
      "  Description: Compress data at rest to reduce storage footprint...\n",
      "\n",
      "ID: compress_transmitted_data\n",
      "  Name:        Compress Transmitted Data\n",
      "  Category:    cloud\n",
      "  SCI Impact:  Reduces network energy consumption and transfer time\n",
      "  Keywords:    ['compress', 'transmission', 'gzip', 'brotli', 'network']...\n",
      "  Description: Compress data before transmission over network...\n",
      "\n",
      "ID: containerize_workload\n",
      "  Name:        Containerize Your Workload\n",
      "  Category:    cloud\n",
      "  SCI Impact:  Improved resource density and efficiency\n",
      "  Keywords:    ['container', 'docker', 'kubernetes', 'containerize', 'pod']...\n",
      "  Description: Use containers for better resource utilization...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show details for the first 5 patterns\n",
    "for pid in list(GSF_PATTERNS.keys())[:5]:\n",
    "    p = GSF_PATTERNS[pid]\n",
    "    print(f\"ID: {pid}\")\n",
    "    print(f\"  Name:        {p['name']}\")\n",
    "    print(f\"  Category:    {p['category']}\")\n",
    "    print(f\"  SCI Impact:  {p['sci_impact']}\")\n",
    "    print(f\"  Keywords:    {p['keywords'][:5]}...\")\n",
    "    print(f\"  Description: {p['description'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. GitHub Token Configuration\n",
    "\n",
    "A GitHub personal access token is required for fetching repositories via the GraphQL API. Set your token below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub token configured (github_p...)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Option 1: Set directly (replace with your token)\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\", \"your_github_token_here\")\n",
    "\n",
    "# Option 2: Load from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\", GITHUB_TOKEN)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "if GITHUB_TOKEN == \"your_github_token_here\":\n",
    "    print(\"WARNING: Set your GITHUB_TOKEN to run the search experiment.\")\n",
    "    print(\"You can still run URL-based analysis without a token.\")\n",
    "else:\n",
    "    print(f\"GitHub token configured ({GITHUB_TOKEN[:8]}...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Experiment 1: Search 10 Blockchain Repositories\n",
    "\n",
    "Use the `fetch_repositories` function to search GitHub for blockchain-related repositories using the GraphQL API v4. Filtering by:\n",
    "- Keyword: `blockchain`\n",
    "- Minimum stars: 3\n",
    "- Languages: Top 20 programming languages\n",
    "- Date filters: Created after 2020-01-01\n",
    "\n",
    "### 5.1 Fetch Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching up to 10 repositories...\n",
      "   Keywords: blockchain\n",
      "   Filters: min_stars=3\n",
      "   Created: 2020-01-01 to any\n",
      "GraphQL Search Query: blockchain stars:>=3 created:>=2020-01-01\n",
      "Rate Limit: 4998/5000 (cost: 1)\n",
      "Error fetching repositories: Repository.__init__() got an unexpected keyword argument 'languages'. Did you mean 'language'?\n",
      "Fetched 0 repositories using GraphQL\n",
      "Fetched 0 repositories\n",
      "   Saved to: data/repositories.json\n",
      "Fetched 0 blockchain repositories:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from greenmining import fetch_repositories\n",
    "\n",
    "# Top 20 programming languages\n",
    "TOP_20_LANGUAGES = [\n",
    "    \"Python\", \"JavaScript\", \"TypeScript\", \"Java\", \"Go\",\n",
    "    \"Rust\", \"C\", \"C++\", \"C#\", \"Ruby\",\n",
    "    \"PHP\", \"Kotlin\", \"Swift\", \"Scala\", \"R\",\n",
    "    \"Dart\", \"Shell\", \"Lua\", \"Perl\", \"Haskell\",\n",
    "]\n",
    "\n",
    "blockchain_repos = fetch_repositories(\n",
    "    github_token=GITHUB_TOKEN,\n",
    "    max_repos=10,\n",
    "    min_stars=3,\n",
    "    keywords=\"blockchain\",\n",
    "    languages=TOP_20_LANGUAGES,\n",
    "    created_after=\"2020-01-01\",\n",
    ")\n",
    "\n",
    "print(f\"Fetched {len(blockchain_repos)} blockchain repositories:\\n\")\n",
    "for i, repo in enumerate(blockchain_repos, 1):\n",
    "    print(f\"  {i:2d}. {repo.full_name} ({repo.stars} stars, {repo.language})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Extract Commits\n",
    "\n",
    "Use `CommitExtractor` to extract up to 20 commits per repository, skipping merge and bot commits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from greenmining.services.commit_extractor import CommitExtractor\n\nextractor = CommitExtractor(\n    max_commits=20,\n    skip_merges=True,\n    days_back=730,\n    github_token=GITHUB_TOKEN,\n    timeout=60,\n)\n\nall_commits = extractor.extract_from_repositories(blockchain_repos)\n\nprint(f\"\\nTotal commits extracted: {len(all_commits)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Analyze Commits for Green Patterns\n",
    "\n",
    "Use `DataAnalyzer` to scan each commit message for GSF sustainability patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenmining.services.data_analyzer import DataAnalyzer\n",
    "\n",
    "analyzer = DataAnalyzer(\n",
    "    enable_diff_analysis=False,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "analyzed_commits = analyzer.analyze_commits(all_commits)\n",
    "\n",
    "green_count = sum(1 for c in analyzed_commits if c.get(\"green_aware\", False))\n",
    "green_pct = (green_count / len(analyzed_commits) * 100) if analyzed_commits else 0\n",
    "\n",
    "print(f\"Analyzed: {len(analyzed_commits)} commits\")\n",
    "print(f\"Green-aware: {green_count} ({green_pct:.1f}%)\")\n",
    "print(f\"\\nSample green commits:\")\n",
    "for c in analyzed_commits:\n",
    "    if c.get(\"green_aware\"):\n",
    "        print(f\"  - {c.get('message', '')[:70]}\")\n",
    "        print(f\"    Patterns: {c.get('known_pattern', [])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Aggregate Results with Statistical and Temporal Analysis\n",
    "\n",
    "Use `DataAggregator` with statistical correlations and temporal trend analysis enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenmining.services.data_aggregator import DataAggregator\n",
    "\n",
    "aggregator = DataAggregator(\n",
    "    enable_stats=True,\n",
    "    enable_temporal=True,\n",
    "    temporal_granularity=\"quarter\",\n",
    ")\n",
    "\n",
    "aggregated = aggregator.aggregate(analyzed_commits, blockchain_repos)\n",
    "\n",
    "print(\"Aggregated Statistics:\")\n",
    "print(f\"  Total commits:     {aggregated.get('total_commits', 'N/A')}\")\n",
    "print(f\"  Green-aware:       {aggregated.get('green_aware_count', 'N/A')}\")\n",
    "print(f\"  Green rate:        {aggregated.get('green_aware_percentage', 'N/A')}%\")\n",
    "\n",
    "# Show top patterns\n",
    "top_patterns = aggregated.get(\"top_patterns\", [])\n",
    "if top_patterns:\n",
    "    print(f\"\\nTop patterns:\")\n",
    "    for p in top_patterns[:5]:\n",
    "        print(f\"  - {p}\")\n",
    "\n",
    "# Show temporal analysis\n",
    "temporal = aggregated.get(\"temporal_analysis\", {})\n",
    "if temporal:\n",
    "    periods = temporal.get(\"periods\", [])\n",
    "    print(f\"\\nTemporal analysis ({len(periods)} periods):\")\n",
    "    for period in periods[:5]:\n",
    "        print(f\"  {period.get('period')}: {period.get('commit_count')} commits, \"\n",
    "              f\"{period.get('green_awareness_rate', 0):.1%} green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Save Search Results\n",
    "\n",
    "Export analyzed data to JSON and CSV for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"experiment_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save analyzed commits to JSON\n",
    "with open(output_dir / \"blockchain_analyzed.json\", \"w\") as f:\n",
    "    json.dump(analyzed_commits, f, indent=2, default=str)\n",
    "\n",
    "# Save aggregated stats\n",
    "with open(output_dir / \"blockchain_stats.json\", \"w\") as f:\n",
    "    json.dump(aggregated, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to {output_dir.absolute()}/\")\n",
    "print(f\"  blockchain_analyzed.json ({len(analyzed_commits)} commits)\")\n",
    "print(f\"  blockchain_stats.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Experiment 2: URL-Based Analysis of 2 Selected Repositories\n",
    "\n",
    "Analyze two handpicked repositories directly from their GitHub URLs using PyDriller. This approach clones the repository locally and performs deep commit-level analysis including:\n",
    "- GSF pattern matching\n",
    "- Delta Maintainability Model (DMM) metrics\n",
    "- Structural complexity metrics (via Lizard)\n",
    "- Full process metrics (8 PyDriller metrics)\n",
    "\n",
    "**Selected repositories:**\n",
    "- `pallets/flask` - Python web microframework\n",
    "- `psf/requests` - HTTP library for Python\n",
    "\n",
    "### 6.1 Analyze Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n",
    "\n",
    "analyzer = LocalRepoAnalyzer(\n",
    "    max_commits=20,\n",
    "    days_back=730,\n",
    "    skip_merges=True,\n",
    "    compute_process_metrics=True,\n",
    "    cleanup_after=True,\n",
    "    method_level_analysis=True,\n",
    "    include_source_code=True,\n",
    "    process_metrics=\"standard\",\n",
    ")\n",
    "\n",
    "flask_result = analyzer.analyze_repository(\"https://github.com/pallets/flask\")\n",
    "\n",
    "print(f\"Repository: {flask_result.name}\")\n",
    "print(f\"Total commits: {flask_result.total_commits}\")\n",
    "print(f\"Green commits: {flask_result.green_commits}\")\n",
    "print(f\"Green rate: {flask_result.green_commit_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Analyze Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_result = analyzer.analyze_repository(\"https://github.com/psf/requests\")\n",
    "\n",
    "print(f\"Repository: {requests_result.name}\")\n",
    "print(f\"Total commits: {requests_result.total_commits}\")\n",
    "print(f\"Green commits: {requests_result.green_commits}\")\n",
    "print(f\"Green rate: {requests_result.green_commit_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Inspect Commit-Level Details\n",
    "\n",
    "Examine the rich data available for each commit, including DMM metrics, complexity, and green patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed commit analysis for Flask\n",
    "print(\"Flask - Commit Details:\")\n",
    "print(\"=\" * 80)\n",
    "for commit in flask_result.commits[:5]:\n",
    "    d = commit.to_dict()\n",
    "    print(f\"\\nCommit: {d['commit_hash'][:8]}\")\n",
    "    print(f\"  Author:     {d['author']}\")\n",
    "    print(f\"  Date:       {d['date']}\")\n",
    "    print(f\"  Message:    {d['message'][:60]}\")\n",
    "    print(f\"  Green:      {d['green_aware']}\")\n",
    "    print(f\"  Patterns:   {d['gsf_patterns_matched']}\")\n",
    "    print(f\"  Confidence: {d['confidence']}\")\n",
    "    print(f\"  Files:      {len(d['files_modified'])} modified\")\n",
    "    print(f\"  Lines:      +{d['insertions']} / -{d['deletions']}\")\n",
    "    print(f\"  DMM Size:   {d['dmm_unit_size']}\")\n",
    "    print(f\"  DMM Cmplx:  {d['dmm_unit_complexity']}\")\n",
    "    print(f\"  NLOC:       {d['total_nloc']}\")\n",
    "    print(f\"  Complexity: {d['total_complexity']}\")\n",
    "    print(f\"  Methods:    {d['methods_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Inspect Method-Level Analysis\n",
    "\n",
    "Method-level metrics are extracted via Lizard integration, providing per-function complexity data for each modified file in a commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find commits with method-level data\n",
    "print(\"Method-Level Analysis (Flask):\")\n",
    "print(\"=\" * 80)\n",
    "methods_found = 0\n",
    "for commit in flask_result.commits:\n",
    "    if commit.methods:\n",
    "        print(f\"\\nCommit {commit.hash[:8]}: {commit.message[:50]}\")\n",
    "        for method in commit.methods[:5]:\n",
    "            m = method.to_dict()\n",
    "            print(f\"  {m['long_name']}\")\n",
    "            print(f\"    File: {m['filename']}, Lines: {m['start_line']}-{m['end_line']}\")\n",
    "            print(f\"    NLOC: {m['nloc']}, Complexity: {m['complexity']}, \"\n",
    "                  f\"Tokens: {m['token_count']}, Params: {m['parameters']}\")\n",
    "            methods_found += 1\n",
    "        if methods_found >= 10:\n",
    "            break\n",
    "\n",
    "if methods_found == 0:\n",
    "    print(\"No method-level data found in analyzed commits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Inspect Source Code Changes\n",
    "\n",
    "Source code before/after is available for each modified file, enabling refactoring detection and diff analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show source code changes for first commit with modifications\n",
    "print(\"Source Code Changes (Flask):\")\n",
    "print(\"=\" * 80)\n",
    "changes_shown = 0\n",
    "for commit in flask_result.commits:\n",
    "    if commit.source_changes:\n",
    "        print(f\"\\nCommit {commit.hash[:8]}: {commit.message[:50]}\")\n",
    "        for change in commit.source_changes[:3]:\n",
    "            c = change.to_dict()\n",
    "            print(f\"  File: {c['filename']} ({c['change_type']})\")\n",
    "            print(f\"  Lines: +{c['added_lines']} / -{c['deleted_lines']}\")\n",
    "            if c['source_code_before']:\n",
    "                lines = c['source_code_before'].split('\\n')\n",
    "                print(f\"  Before ({len(lines)} lines): {lines[0][:60]}...\")\n",
    "            if c['source_code_after']:\n",
    "                lines = c['source_code_after'].split('\\n')\n",
    "                print(f\"  After  ({len(lines)} lines): {lines[0][:60]}...\")\n",
    "            changes_shown += 1\n",
    "        if changes_shown >= 5:\n",
    "            break\n",
    "\n",
    "if changes_shown == 0:\n",
    "    print(\"No source code changes found in analyzed commits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Inspect Process Metrics\n",
    "\n",
    "PyDriller computes 8 process metrics across the repository: ChangeSet, CodeChurn, CommitsCount, ContributorsCount, ContributorsExperience, HistoryComplexity, HunksCount, and LinesCount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Process Metrics (Flask):\")\n",
    "print(\"=\" * 80)\n",
    "for metric_name, metric_value in flask_result.process_metrics.items():\n",
    "    if isinstance(metric_value, dict):\n",
    "        # Show summary for dict metrics\n",
    "        print(f\"  {metric_name}: {len(metric_value)} entries\")\n",
    "        for k, v in list(metric_value.items())[:3]:\n",
    "            print(f\"    {k}: {v}\")\n",
    "        if len(metric_value) > 3:\n",
    "            print(f\"    ... ({len(metric_value) - 3} more)\")\n",
    "    else:\n",
    "        print(f\"  {metric_name}: {metric_value}\")\n",
    "\n",
    "print(f\"\\nProcess Metrics (Requests):\")\n",
    "print(\"=\" * 80)\n",
    "for metric_name, metric_value in requests_result.process_metrics.items():\n",
    "    if isinstance(metric_value, dict):\n",
    "        print(f\"  {metric_name}: {len(metric_value)} entries\")\n",
    "    else:\n",
    "        print(f\"  {metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Experiment 3: Single Repository Deep Analysis\n",
    "\n",
    "Perform a deep analysis on `tiangolo/fastapi` with integrated energy tracking. This demonstrates energy measurement during repository mining.\n",
    "\n",
    "### 7.1 Analyze with Energy Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n",
    "\n",
    "deep_analyzer = LocalRepoAnalyzer(\n",
    "    max_commits=20,\n",
    "    days_back=730,\n",
    "    skip_merges=True,\n",
    "    compute_process_metrics=True,\n",
    "    cleanup_after=True,\n",
    "    energy_tracking=True,\n",
    "    energy_backend=\"auto\",\n",
    "    method_level_analysis=True,\n",
    "    include_source_code=True,\n",
    "    process_metrics=\"standard\",\n",
    ")\n",
    "\n",
    "fastapi_result = deep_analyzer.analyze_repository(\"https://github.com/tiangolo/fastapi\")\n",
    "\n",
    "print(f\"Repository: {fastapi_result.name}\")\n",
    "print(f\"Total commits: {fastapi_result.total_commits}\")\n",
    "print(f\"Green commits: {fastapi_result.green_commits}\")\n",
    "print(f\"Green rate: {fastapi_result.green_commit_rate:.1%}\")\n",
    "\n",
    "# Show energy metrics if available\n",
    "if fastapi_result.energy_metrics:\n",
    "    em = fastapi_result.energy_metrics\n",
    "    print(f\"\\nEnergy Metrics:\")\n",
    "    print(f\"  Energy consumed: {em.get('joules', 0):.4f} Joules\")\n",
    "    print(f\"  Average power:   {em.get('watts_avg', 0):.4f} Watts\")\n",
    "    print(f\"  Duration:        {em.get('duration_seconds', 0):.2f} seconds\")\n",
    "    print(f\"  Backend:         {em.get('backend', 'N/A')}\")\n",
    "else:\n",
    "    print(\"\\nEnergy tracking: No data (backend may not be available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Compare All Three Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_results = [flask_result, requests_result, fastapi_result]\n",
    "\n",
    "print(f\"{'Repository':<25} {'Commits':>8} {'Green':>6} {'Rate':>8} {'Complexity':>11}\")\n",
    "print(\"=\" * 65)\n",
    "for r in url_results:\n",
    "    avg_complexity = (\n",
    "        sum(c.total_complexity for c in r.commits) / len(r.commits)\n",
    "        if r.commits else 0\n",
    "    )\n",
    "    print(f\"{r.name:<25} {r.total_commits:>8} {r.green_commits:>6} \"\n",
    "          f\"{r.green_commit_rate:>7.1%} {avg_complexity:>11.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Batch Analysis with Parallelism\n",
    "\n",
    "Use the top-level `analyze_repositories()` function to analyze the 2 selected + 1 deep repo in parallel with energy tracking enabled. This is the convenience API that handles cloning, analysis, and cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenmining import analyze_repositories\n",
    "\n",
    "batch_results = analyze_repositories(\n",
    "    urls=[\n",
    "        \"https://github.com/pallets/flask\",\n",
    "        \"https://github.com/psf/requests\",\n",
    "        \"https://github.com/tiangolo/fastapi\",\n",
    "    ],\n",
    "    max_commits=20,\n",
    "    parallel_workers=3,\n",
    "    energy_tracking=True,\n",
    "    energy_backend=\"auto\",\n",
    "    method_level_analysis=True,\n",
    "    include_source_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Batch analysis complete: {len(batch_results)} repositories\\n\")\n",
    "for r in batch_results:\n",
    "    print(f\"{r.name}: {r.total_commits} commits, {r.green_commit_rate:.1%} green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Energy Measurement\n",
    "\n",
    "GreenMining provides three energy measurement backends. The `auto` backend selects the best available: RAPL on Linux (Intel/AMD), then CPU Meter as fallback.\n",
    "\n",
    "### 9.1 Standalone Energy Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenmining.energy import get_energy_meter, CPUEnergyMeter, RAPLEnergyMeter\n",
    "import time\n",
    "\n",
    "# Auto-detect best available backend\n",
    "meter = get_energy_meter(\"auto\")\n",
    "print(f\"Selected backend: {meter.__class__.__name__}\")\n",
    "print(f\"Available: {meter.is_available()}\")\n",
    "\n",
    "# Measure a workload\n",
    "meter.start()\n",
    "\n",
    "# Simulate a workload\n",
    "total = 0\n",
    "for i in range(500_000):\n",
    "    total += i * i\n",
    "\n",
    "result = meter.stop()\n",
    "\n",
    "print(f\"\\nMeasurement Results:\")\n",
    "print(f\"  Energy:   {result.joules:.4f} Joules\")\n",
    "print(f\"  Power:    {result.watts_avg:.4f} Watts (avg)\")\n",
    "print(f\"  Peak:     {result.watts_peak:.4f} Watts (peak)\")\n",
    "print(f\"  Duration: {result.duration_seconds:.4f} seconds\")\n",
    "print(f\"  CPU:      {result.cpu_energy_joules:.4f} Joules\")\n",
    "print(f\"  DRAM:     {result.dram_energy_joules} Joules\")\n",
    "print(f\"  Backend:  {result.backend}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 CPU Energy Meter (Cross-Platform)\n",
    "\n",
    "The CPU Energy Meter works on all platforms by estimating power from CPU utilization and TDP (Thermal Design Power)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_meter = CPUEnergyMeter(\n",
    "    tdp_watts=None,          # Auto-detect TDP\n",
    "    sample_interval=0.5,     # 500ms sampling\n",
    ")\n",
    "\n",
    "print(f\"Platform TDP: {cpu_meter.tdp_watts} W\")\n",
    "print(f\"Available: {cpu_meter.is_available()}\")\n",
    "\n",
    "cpu_meter.start()\n",
    "time.sleep(1)  # 1-second measurement window\n",
    "cpu_result = cpu_meter.stop()\n",
    "\n",
    "print(f\"\\nCPU Energy Meter Results (1s window):\")\n",
    "print(f\"  Energy:   {cpu_result.joules:.4f} Joules\")\n",
    "print(f\"  Power:    {cpu_result.watts_avg:.4f} Watts\")\n",
    "print(f\"  Duration: {cpu_result.duration_seconds:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 RAPL Energy Meter (Linux Intel/AMD)\n",
    "\n",
    "RAPL (Running Average Power Limit) provides hardware-level energy counters on Linux with Intel or AMD processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rapl_meter = RAPLEnergyMeter()\n",
    "print(f\"RAPL available: {rapl_meter.is_available()}\")\n",
    "\n",
    "if rapl_meter.is_available():\n",
    "    rapl_meter.start()\n",
    "    time.sleep(1)\n",
    "    rapl_result = rapl_meter.stop()\n",
    "    print(f\"  Energy:   {rapl_result.joules:.4f} Joules\")\n",
    "    print(f\"  CPU:      {rapl_result.cpu_energy_joules:.4f} Joules\")\n",
    "    print(f\"  DRAM:     {rapl_result.dram_energy_joules:.4f} Joules\")\n",
    "else:\n",
    "    print(\"  RAPL is not available on this system (requires Linux with Intel/AMD and powercap access).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Carbon Footprint Reporting\n",
    "\n",
    "Convert energy measurements to CO2 emissions using regional grid carbon intensity data. Supports 20+ countries and major cloud provider regions (AWS, GCP, Azure).\n",
    "\n",
    "### 10.1 Generate Carbon Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenmining.energy import CarbonReporter, CarbonReport\n",
    "\n",
    "reporter = CarbonReporter(\n",
    "    country_iso=\"USA\",\n",
    "    cloud_provider=\"aws\",\n",
    "    region=\"us-east-1\",\n",
    ")\n",
    "\n",
    "# Generate report from the energy measurement above\n",
    "report = reporter.generate_report(total_joules=cpu_result.joules)\n",
    "\n",
    "print(report.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Compare Carbon Across Regions\n",
    "\n",
    "Compare the same energy consumption across different countries and cloud regions to see the impact of grid carbon intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a workload energy measurement\n",
    "test_joules = 3600.0  # 1 kWh\n",
    "\n",
    "regions = [\n",
    "    (\"Sweden (NOR)\", \"SWE\", None, None),\n",
    "    (\"France (FRA)\", \"FRA\", None, None),\n",
    "    (\"USA (USA)\", \"USA\", None, None),\n",
    "    (\"Germany (DEU)\", \"DEU\", None, None),\n",
    "    (\"Australia (AUS)\", \"AUS\", None, None),\n",
    "    (\"India (IND)\", \"IND\", None, None),\n",
    "    (\"AWS us-west-2\", \"USA\", \"aws\", \"us-west-2\"),\n",
    "    (\"AWS eu-north-1\", \"SWE\", \"aws\", \"eu-north-1\"),\n",
    "    (\"GCP europe-north1\", \"SWE\", \"gcp\", \"europe-north1\"),\n",
    "]\n",
    "\n",
    "print(f\"Carbon comparison for {test_joules:.0f} Joules ({test_joules/3_600_000*1000:.1f} Wh):\")\n",
    "print(f\"{'Region':<25} {'gCO2/kWh':>10} {'Emissions (g)':>15} {'Tree-months':>12}\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "for label, country, cloud, region in regions:\n",
    "    r = CarbonReporter(country_iso=country, cloud_provider=cloud, region=region)\n",
    "    report = r.generate_report(total_joules=test_joules)\n",
    "    d = report.to_dict()\n",
    "    print(f\"{label:<25} {d['carbon_intensity_gco2_kwh']:>10.0f} \"\n",
    "          f\"{d['total_emissions_grams']:>15.4f} {d['equivalents']['tree_months']:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Supported Countries and Cloud Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Supported countries: {CarbonReporter.get_supported_countries()}\")\n",
    "print(f\"\\nAWS regions: {CarbonReporter.get_supported_cloud_regions('aws')}\")\n",
    "print(f\"\\nGCP regions: {CarbonReporter.get_supported_cloud_regions('gcp')}\")\n",
    "print(f\"\\nAzure regions: {CarbonReporter.get_supported_cloud_regions('azure')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Carbon Report from Analysis Results\n",
    "\n",
    "Generate a carbon footprint report from the batch analysis results, combining energy data from multiple repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect analysis results with energy data\n",
    "analysis_dicts = [r.to_dict() for r in batch_results]\n",
    "\n",
    "# Generate combined carbon report\n",
    "combined_reporter = CarbonReporter(country_iso=\"USA\")\n",
    "combined_report = combined_reporter.generate_report(analysis_results=analysis_dicts)\n",
    "\n",
    "print(combined_report.summary())\n",
    "\n",
    "print(\"\\nPer-repository breakdown:\")\n",
    "for entry in combined_report.to_dict().get(\"analysis_results\", []):\n",
    "    print(f\"  {entry['name']}: {entry['energy_joules']:.4f} J, {entry['duration_seconds']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Power Regression Detection\n",
    "\n",
    "Detect commits that caused power consumption regressions by measuring energy at each commit in a range. The detector runs a test command at each commit and compares energy usage against a configurable threshold.\n",
    "\n",
    "This requires a local git repository. We demonstrate the API and configuration here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenmining.analyzers import PowerRegressionDetector, PowerRegression\n",
    "\n",
    "detector = PowerRegressionDetector(\n",
    "    test_command=\"pytest tests/ -x --no-header -q\",\n",
    "    energy_backend=\"auto\",\n",
    "    threshold_percent=5.0,      # Flag regressions above 5%\n",
    "    iterations=3,               # 3 measurement iterations for accuracy\n",
    "    warmup_iterations=1,        # 1 warmup run before measuring\n",
    ")\n",
    "\n",
    "print(\"PowerRegressionDetector configured:\")\n",
    "print(f\"  Test command:       {detector.test_command}\")\n",
    "print(f\"  Energy backend:     {detector.energy_backend}\")\n",
    "print(f\"  Threshold:          {detector.threshold_percent}%\")\n",
    "print(f\"  Iterations:         {detector.iterations}\")\n",
    "print(f\"  Warmup iterations:  {detector.warmup_iterations}\")\n",
    "\n",
    "# To run detection on a local repo:\n",
    "# regressions = detector.detect(\n",
    "#     repo_path=\"/path/to/local/repo\",\n",
    "#     baseline_commit=\"HEAD~10\",\n",
    "#     target_commit=\"HEAD\",\n",
    "#     max_commits=50,\n",
    "# )\n",
    "# for r in regressions:\n",
    "#     print(f\"Regression at {r.sha[:8]}: +{r.power_increase:.1f}%\")\n",
    "\n",
    "print(\"\\nPowerRegression dataclass fields:\")\n",
    "print(f\"  {[f.name for f in PowerRegression.__dataclass_fields__.values()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Metrics-to-Power Correlation\n",
    "\n",
    "Analyze the statistical relationship between code metrics (complexity, NLOC, churn) and power consumption using Pearson and Spearman correlations.\n",
    "\n",
    "### 13.1 Build Correlation from Analysis Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenmining.analyzers import MetricsPowerCorrelator, CorrelationResult\n",
    "\n",
    "# Extract metrics from the FastAPI analysis\n",
    "commits_data = [c.to_dict() for c in fastapi_result.commits if c.total_nloc > 0]\n",
    "\n",
    "if len(commits_data) >= 3:\n",
    "    complexity_values = [c[\"total_complexity\"] for c in commits_data]\n",
    "    nloc_values = [c[\"total_nloc\"] for c in commits_data]\n",
    "    insertions_values = [c[\"insertions\"] for c in commits_data]\n",
    "    deletions_values = [c[\"deletions\"] for c in commits_data]\n",
    "    methods_values = [c[\"methods_count\"] for c in commits_data]\n",
    "\n",
    "    # Use insertions as a proxy for \"power\" in this demo\n",
    "    # (in practice, you would use actual energy measurements)\n",
    "    power_proxy = [c[\"insertions\"] + c[\"deletions\"] for c in commits_data]\n",
    "\n",
    "    correlator = MetricsPowerCorrelator(significance_level=0.05)\n",
    "    correlator.fit(\n",
    "        metrics=[\"complexity\", \"nloc\", \"insertions\", \"deletions\", \"methods_count\"],\n",
    "        metrics_values={\n",
    "            \"complexity\": complexity_values,\n",
    "            \"nloc\": nloc_values,\n",
    "            \"insertions\": insertions_values,\n",
    "            \"deletions\": deletions_values,\n",
    "            \"methods_count\": methods_values,\n",
    "        },\n",
    "        power_measurements=power_proxy,\n",
    "    )\n",
    "\n",
    "    print(\"Pearson correlations (linear):\")\n",
    "    for metric, r in correlator.pearson.items():\n",
    "        print(f\"  {metric:<20s} r = {r:+.4f}\")\n",
    "\n",
    "    print(f\"\\nSpearman correlations (monotonic):\")\n",
    "    for metric, r in correlator.spearman.items():\n",
    "        print(f\"  {metric:<20s} rho = {r:+.4f}\")\n",
    "\n",
    "    print(f\"\\nFeature importance (normalized):\")\n",
    "    for metric, imp in correlator.feature_importance.items():\n",
    "        bar = \"#\" * int(imp * 30)\n",
    "        print(f\"  {metric:<20s} {imp:.3f} {bar}\")\n",
    "else:\n",
    "    print(f\"Insufficient data points ({len(commits_data)}). Need at least 3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2 Inspect Significant Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(commits_data) >= 3:\n",
    "    significant = correlator.get_significant_correlations()\n",
    "    print(f\"Significant correlations (p < 0.05): {len(significant)}\")\n",
    "    for name, result in significant.items():\n",
    "        d = result.to_dict()\n",
    "        print(f\"  {name}: strength={d['strength']}, \"\n",
    "              f\"pearson_r={d['pearson_r']:.4f} (p={d['pearson_p']:.6f}), \"\n",
    "              f\"spearman_r={d['spearman_r']:.4f} (p={d['spearman_p']:.6f})\")\n",
    "\n",
    "    all_results = correlator.get_results()\n",
    "    print(f\"\\nAll correlations: {len(all_results)}\")\n",
    "    for name, result in all_results.items():\n",
    "        d = result.to_dict()\n",
    "        sig = \"*\" if d[\"significant\"] else \" \"\n",
    "        print(f\"  {sig} {name:<20s} strength={d['strength']:<12s} \"\n",
    "              f\"pearson={d['pearson_r']:+.4f} spearman={d['spearman_r']:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Version Power Analysis\n",
    "\n",
    "Compare power consumption across different software versions or tags. The analyzer checks out each version, runs a test command, measures energy, and reports trends.\n",
    "\n",
    "This requires a local git repository with version tags. We demonstrate the API configuration and data structures here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenmining.analyzers import VersionPowerAnalyzer, VersionPowerReport\n",
    "\n",
    "version_analyzer = VersionPowerAnalyzer(\n",
    "    test_command=\"pytest tests/ --no-header -q\",\n",
    "    energy_backend=\"auto\",\n",
    "    iterations=5,\n",
    "    warmup_iterations=1,\n",
    ")\n",
    "\n",
    "print(\"VersionPowerAnalyzer configured:\")\n",
    "print(f\"  Test command:       {version_analyzer.test_command}\")\n",
    "print(f\"  Energy backend:     {version_analyzer.energy_backend}\")\n",
    "print(f\"  Iterations:         {version_analyzer.iterations}\")\n",
    "print(f\"  Warmup:             {version_analyzer.warmup_iterations}\")\n",
    "\n",
    "# To run on a local repo:\n",
    "# report = version_analyzer.analyze_versions(\n",
    "#     repo_path=\"/path/to/local/repo\",\n",
    "#     versions=[\"v1.0\", \"v1.1\", \"v1.2\", \"v2.0\"],\n",
    "# )\n",
    "# print(report.summary())\n",
    "\n",
    "# Demonstrate the report data structure\n",
    "from greenmining.analyzers.version_power_analyzer import VersionPowerProfile\n",
    "\n",
    "demo_report = VersionPowerReport(\n",
    "    versions=[\n",
    "        VersionPowerProfile(version=\"v1.0\", commit_sha=\"abc1234\", energy_joules=10.5, power_watts_avg=8.2, duration_seconds=1.28, iterations=5, energy_std=0.3),\n",
    "        VersionPowerProfile(version=\"v1.1\", commit_sha=\"def5678\", energy_joules=11.2, power_watts_avg=8.5, duration_seconds=1.32, iterations=5, energy_std=0.4),\n",
    "        VersionPowerProfile(version=\"v2.0\", commit_sha=\"ghi9012\", energy_joules=9.8, power_watts_avg=7.9, duration_seconds=1.24, iterations=5, energy_std=0.2),\n",
    "    ],\n",
    "    trend=\"decreasing\",\n",
    "    total_change_percent=-6.67,\n",
    "    most_efficient=\"v2.0\",\n",
    "    least_efficient=\"v1.1\",\n",
    ")\n",
    "\n",
    "print(f\"\\nDemo report structure:\")\n",
    "print(demo_report.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Statistical Analyzer\n",
    "\n",
    "The `StatisticalAnalyzer` computes effect sizes, correlations, and statistical significance tests on the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nfrom greenmining.analyzers import StatisticalAnalyzer\n\nstat_analyzer = StatisticalAnalyzer()\n\n# Prepare data from URL analysis results\nall_analysis_dicts = []\nfor repo_result in url_results:\n    for commit in repo_result.commits:\n        d = commit.to_dict()\n        d[\"repository\"] = repo_result.name\n        all_analysis_dicts.append(d)\n\n# Convert to DataFrame for statistical analysis\ncommits_df = pd.DataFrame(all_analysis_dicts)\n\n# Pattern correlation analysis\nall_patterns = set()\nfor patterns_list in commits_df.get(\"gsf_patterns_matched\", []):\n    if patterns_list:\n        all_patterns.update(patterns_list)\n\nfor pattern in all_patterns:\n    commits_df[f\"pattern_{pattern}\"] = commits_df[\"gsf_patterns_matched\"].apply(\n        lambda x, p=pattern: 1 if p in (x or []) else 0\n    )\n\nif len(all_patterns) >= 2:\n    correlation_results = stat_analyzer.analyze_pattern_correlations(commits_df)\n    print(\"Pattern Correlation Analysis:\")\n    sig_pairs = correlation_results.get(\"significant_pairs\", [])\n    print(f\"  Significant pairs: {len(sig_pairs)}\")\n    for pair in sig_pairs[:10]:\n        print(f\"    {pair}\")\nelse:\n    print(f\"Found {len(all_patterns)} pattern(s) - need >= 2 for correlation analysis\")\n\n# Temporal trend analysis\nif \"date\" in commits_df.columns and \"green_aware\" in commits_df.columns:\n    if \"commit_hash\" not in commits_df.columns:\n        commits_df[\"commit_hash\"] = commits_df.index.astype(str)\n    trend_results = stat_analyzer.temporal_trend_analysis(commits_df)\n    trend = trend_results.get(\"trend\", {})\n    print(f\"\\nTemporal Trend:\")\n    print(f\"  Direction: {trend.get('direction', 'N/A')}\")\n    print(f\"  Significant: {trend.get('significant', 'N/A')}\")\n    print(f\"  Correlation: {trend.get('correlation', 'N/A')}\")\n\n# Effect size: green vs non-green commit complexity\ngreen_complexity = commits_df[commits_df[\"green_aware\"] == True][\"total_complexity\"].dropna().tolist()\nnon_green_complexity = commits_df[commits_df[\"green_aware\"] == False][\"total_complexity\"].dropna().tolist()\n\nif green_complexity and non_green_complexity:\n    effect = stat_analyzer.effect_size_analysis(green_complexity, non_green_complexity)\n    print(f\"\\nEffect Size (Green vs Non-Green Complexity):\")\n    print(f\"  Cohen's d: {effect['cohens_d']:.3f} ({effect['magnitude']})\")\n    print(f\"  Mean difference: {effect['mean_difference']:.2f}\")\n    print(f\"  Significant: {effect['significant']}\")\nelse:\n    print(\"\\nInsufficient data for effect size analysis\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Temporal Analyzer\n",
    "\n",
    "Analyze how green software adoption evolves over time using configurable time granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from greenmining.analyzers import TemporalAnalyzer\n\ntemporal = TemporalAnalyzer(granularity=\"month\")\n\n# Prepare analysis results in the expected format\nanalysis_results_list = []\nfor d in all_analysis_dicts:\n    analysis_results_list.append({\n        \"commit_sha\": d.get(\"commit_hash\", \"\"),\n        \"is_green_aware\": d.get(\"green_aware\", False),\n        \"patterns_detected\": d.get(\"gsf_patterns_matched\", []),\n        \"detection_method\": \"gsf_keyword\",\n    })\n\n# Run temporal analysis\ntemporal_results = temporal.analyze_trends(all_analysis_dicts, analysis_results_list)\n\nprint(\"Temporal Analysis (monthly):\")\nprint(\"=\" * 60)\nperiods = temporal_results.get(\"periods\", [])\nfor period in periods:\n    if isinstance(period, dict):\n        print(f\"  {period.get('period', 'N/A')}: \"\n              f\"{period.get('commit_count', 0)} commits, \"\n              f\"{period.get('green_awareness_rate', 0):.1%} green\")\n\nsummary = temporal_results.get(\"summary\", {})\nif summary:\n    print(f\"\\nTrend Summary:\")\n    print(f\"  Overall direction: {summary.get('overall_direction', 'N/A')}\")\n    print(f\"  Total periods: {summary.get('total_periods', 0)}\")\n    print(f\"  Peak period: {summary.get('peak_period', 'N/A')}\")\n\nevolution = temporal_results.get(\"pattern_evolution\", {})\nif evolution:\n    print(f\"\\nPattern Evolution:\")\n    print(f\"  Emerging: {evolution.get('emerging', [])}\")\n    print(f\"  Stable:   {evolution.get('stable', [])}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 17. Qualitative Analyzer\n",
    "\n",
    "The `QualitativeAnalyzer` selects a stratified sample of commits for manual validation, useful for research validation of automated classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from greenmining.analyzers import QualitativeAnalyzer\n\nqual_analyzer = QualitativeAnalyzer(\n    sample_size=10,\n    stratify_by=\"pattern\",\n)\n\n# Generate stratified validation samples\nsamples = qual_analyzer.generate_validation_samples(\n    commits=all_analysis_dicts,\n    analysis_results=analysis_results_list,\n    include_negatives=True,\n)\n\nprint(\"Qualitative Validation Samples:\")\nprint(\"=\" * 60)\nprint(f\"Generated {len(samples)} samples\\n\")\n\nfor sample in samples[:5]:\n    print(f\"  Commit: {sample.commit_hash[:8]}... | \"\n          f\"Pattern: {sample.pattern} | \"\n          f\"Green: {sample.is_green}\")\n\n# Export for manual review\nqual_analyzer.export_samples_for_review(\"experiment_output/validation_samples.json\")\nprint(f\"\\nSamples exported to experiment_output/validation_samples.json\")\nprint(\"After manual review, import with: qual_analyzer.import_validated_samples('validated.json')\")\nprint(\"Then compute metrics with: qual_analyzer.calculate_metrics()\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 18. Code Diff Analyzer\n",
    "\n",
    "The `CodeDiffAnalyzer` performs 15-type code-level pattern detection by analyzing actual code changes in diffs, providing deeper insight than commit message analysis alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from greenmining.analyzers import CodeDiffAnalyzer\n\ndiff_analyzer = CodeDiffAnalyzer()\n\n# CodeDiffAnalyzer.analyze_commit_diff() requires a PyDriller Commit object.\n# It is automatically integrated into the LocalRepoAnalyzer pipeline.\n# Here we show the 15 pattern signatures it detects in code diffs.\n\nprint(\"CodeDiffAnalyzer - Green Pattern Signatures:\")\nprint(\"=\" * 60)\nprint(f\"Total pattern types: {len(diff_analyzer.PATTERN_SIGNATURES)}\\n\")\nfor pattern_name, pattern_data in diff_analyzer.PATTERN_SIGNATURES.items():\n    print(f\"  {pattern_name}:\")\n    if isinstance(pattern_data, dict):\n        for key, value in list(pattern_data.items())[:2]:\n            if isinstance(value, list):\n                print(f\"    {key}: {value[:3]}...\")\n            else:\n                print(f\"    {key}: {value}\")\n    print()\n\nprint(\"Note: To analyze actual diffs, use LocalRepoAnalyzer which calls\")\nprint(\"analyze_commit_diff() on each PyDriller Commit object automatically.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 19. Report Generation\n",
    "\n",
    "Generate a Markdown analysis report from the aggregated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from greenmining.services.reports import ReportGenerator\n\nreport_gen = ReportGenerator()\n\n# Prepare data in expected format\nrepos_data = {\n    \"metadata\": {\"total_repos\": len(url_results)},\n    \"repositories\": [r.to_dict() for r in url_results],\n}\n\nanalysis_data = {\n    \"metadata\": {\"total_commits\": len(all_analysis_dicts)},\n    \"commits\": all_analysis_dicts,\n}\n\naggregated_data = aggregated if \"aggregated\" in dir() else {\n    \"total_commits\": len(all_analysis_dicts),\n    \"green_aware_count\": sum(1 for d in all_analysis_dicts if d.get(\"green_aware\")),\n}\n\nreport_content = report_gen.generate_report(\n    aggregated_data=aggregated_data,\n    analysis_data=analysis_data,\n    repos_data=repos_data,\n)\n\n# Save report\nreport_path = output_dir / \"experiment_report.md\"\nwith open(report_path, \"w\") as f:\n    f.write(report_content)\n\n# Show first 30 lines\nprint(f\"Report saved to {report_path}\")\nprint(\"\\nReport preview:\")\nprint(\"=\" * 60)\nfor line in report_content.split(\"\\n\")[:30]:\n    print(line)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 20. Export Results\n",
    "\n",
    "Export all analysis results to multiple formats for further research use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Export URL analysis results\n",
    "url_export = []\n",
    "for r in url_results:\n",
    "    url_export.append(r.to_dict())\n",
    "\n",
    "with open(output_dir / \"url_analysis_results.json\", \"w\") as f:\n",
    "    json.dump(url_export, f, indent=2, default=str)\n",
    "\n",
    "# Export batch results\n",
    "batch_export = [r.to_dict() for r in batch_results]\n",
    "with open(output_dir / \"batch_analysis_results.json\", \"w\") as f:\n",
    "    json.dump(batch_export, f, indent=2, default=str)\n",
    "\n",
    "# Export carbon report\n",
    "with open(output_dir / \"carbon_report.json\", \"w\") as f:\n",
    "    json.dump(combined_report.to_dict(), f, indent=2, default=str)\n",
    "\n",
    "# Summary\n",
    "print(\"Exported files:\")\n",
    "for p in sorted(output_dir.glob(\"*\")):\n",
    "    size = p.stat().st_size\n",
    "    print(f\"  {p.name:<35} {size:>8,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 21. Private Repository Support\n",
    "\n",
    "GreenMining supports analyzing private repositories via two authentication methods. This cell demonstrates the configuration (not executed without actual credentials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from greenmining.services.local_repo_analyzer import LocalRepoAnalyzer\n",
    "\n",
    "# Method 1: HTTPS with GitHub token\n",
    "# The token is injected into the clone URL for authentication.\n",
    "https_analyzer = LocalRepoAnalyzer(\n",
    "    github_token=\"ghp_your_token_here\",  # Replace with actual token\n",
    "    max_commits=20,\n",
    ")\n",
    "print(\"HTTPS authentication configured (token-based)\")\n",
    "print(f\"  Token set: {https_analyzer.github_token is not None}\")\n",
    "\n",
    "# Method 2: SSH with private key\n",
    "# Sets GIT_SSH_COMMAND to use the specified key.\n",
    "ssh_analyzer = LocalRepoAnalyzer(\n",
    "    ssh_key_path=\"~/.ssh/id_rsa\",  # Replace with actual key path\n",
    "    max_commits=20,\n",
    ")\n",
    "print(f\"\\nSSH authentication configured (key-based)\")\n",
    "print(f\"  Key path: {ssh_analyzer.ssh_key_path}\")\n",
    "\n",
    "# Usage (not executed):\n",
    "# result = https_analyzer.analyze_repository(\"https://github.com/company/private-repo\")\n",
    "# result = ssh_analyzer.analyze_repository(\"git@github.com:company/private-repo.git\")\n",
    "print(\"\\nTo analyze a private repo, call analyze_repository() with the repo URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 22. Web Dashboard\n",
    "\n",
    "GreenMining includes a Flask-based web dashboard for interactive visualization of analysis results. The dashboard provides REST API endpoints and an HTML interface.\n",
    "\n",
    "**Requires:** `pip install greenmining[dashboard]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from greenmining.dashboard import create_app, run_dashboard\n",
    "\n",
    "    print(\"Dashboard module available.\")\n",
    "    print(f\"\\nAPI endpoints:\")\n",
    "    print(f\"  GET /                  - Dashboard UI\")\n",
    "    print(f\"  GET /api/repositories  - List analyzed repositories\")\n",
    "    print(f\"  GET /api/analysis      - Full analysis results\")\n",
    "    print(f\"  GET /api/statistics    - Aggregated statistics\")\n",
    "    print(f\"  GET /api/energy        - Energy measurement data\")\n",
    "    print(f\"  GET /api/summary       - Summary overview\")\n",
    "    print(f\"\\nTo launch the dashboard:\")\n",
    "    print(f\"  run_dashboard(data_dir='./experiment_output', host='127.0.0.1', port=5000)\")\n",
    "    print(f\"\\nNote: Do not run in a notebook cell (blocks execution). Use a separate terminal.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Dashboard not available. Install with: pip install greenmining[dashboard]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 23. Experiment Summary\n",
    "\n",
    "Final summary of all experiments and features demonstrated in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GreenMining Experiment Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Library version: {greenmining.__version__}\")\n",
    "print(f\"GSF patterns:    {len(greenmining.GSF_PATTERNS)}\")\n",
    "print(f\"Green keywords:  {len(greenmining.GREEN_KEYWORDS)}\")\n",
    "print()\n",
    "\n",
    "print(\"Experiments:\")\n",
    "print(f\"  1. Search: {len(blockchain_repos)} blockchain repos, \"\n",
    "      f\"{len(analyzed_commits)} commits analyzed\")\n",
    "print(f\"  2. URL analysis: Flask, Requests (2 repos, method-level + source code)\")\n",
    "print(f\"  3. Deep analysis: FastAPI (energy tracking enabled)\")\n",
    "print(f\"  4. Batch: 3 repos analyzed in parallel with energy tracking\")\n",
    "print()\n",
    "\n",
    "print(\"Features demonstrated:\")\n",
    "features = [\n",
    "    \"GSF pattern detection (122 patterns, 15 categories)\",\n",
    "    \"Green awareness keyword matching (321 keywords)\",\n",
    "    \"GitHub GraphQL API repository search\",\n",
    "    \"Date, star, and language filters\",\n",
    "    \"Commit extraction (merge/bot filtering)\",\n",
    "    \"Commit analysis with pattern classification\",\n",
    "    \"Data aggregation with temporal trends\",\n",
    "    \"URL-based repository analysis (PyDriller)\",\n",
    "    \"Batch analysis with parallel workers\",\n",
    "    \"Private repository support (HTTPS + SSH)\",\n",
    "    \"Energy measurement (RAPL, CPU Meter, Auto)\",\n",
    "    \"Carbon footprint reporting (20+ countries, cloud regions)\",\n",
    "    \"Power regression detection\",\n",
    "    \"Metrics-to-power correlation (Pearson/Spearman)\",\n",
    "    \"Version power analysis\",\n",
    "    \"Method-level analysis (Lizard)\",\n",
    "    \"Source code before/after access\",\n",
    "    \"Process metrics (8 PyDriller metrics)\",\n",
    "    \"Statistical analysis\",\n",
    "    \"Temporal analysis\",\n",
    "    \"Qualitative analysis (stratified sampling)\",\n",
    "    \"Code diff analysis (15 pattern types)\",\n",
    "    \"Report generation (Markdown)\",\n",
    "    \"Multi-format export (JSON, CSV)\",\n",
    "    \"Web dashboard (Flask)\",\n",
    "]\n",
    "for i, f in enumerate(features, 1):\n",
    "    print(f\"  {i:2d}. {f}\")\n",
    "\n",
    "print(f\"\\nOutput directory: {output_dir.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}