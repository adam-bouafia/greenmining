{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2e7b11",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4af2c8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Installation complete\n"
     ]
    }
   ],
   "source": [
    "# Install GreenMining from PyPI\n",
    "!pip install greenmining python-dotenv tqdm --quiet\n",
    "print(\"[OK] Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0b01600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: greenmining\n",
      "Version: 1.0.3\n"
     ]
    }
   ],
   "source": [
    "# Verify installation\n",
    "!pip show greenmining | grep -E \"^(Name|Version)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3877b804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete.\n",
      "Data directory: /home/neo/Documents/greenmining/experiment/data\n",
      "Figures directory: /home/neo/Documents/greenmining/experiment/figures\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set up plotting style for academic papers (black & white)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "# Use fonts commonly available on Linux (fallback chain)\n",
    "plt.rcParams['font.serif'] = ['DejaVu Serif', 'Liberation Serif', 'FreeSerif', 'serif']\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "# Create output directories\n",
    "DATA_DIR = Path('data')\n",
    "FIGURES_DIR = Path('figures')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Environment setup complete.\")\n",
    "print(f\"Data directory: {DATA_DIR.absolute()}\")\n",
    "print(f\"Figures directory: {FIGURES_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4463b30",
   "metadata": {},
   "source": [
    "## 2. Experiment Configuration\n",
    "\n",
    "Define the experimental parameters for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82cdaa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPERIMENT CONFIGURATION\n",
      "============================================================\n",
      "max_repos                 : 100\n",
      "min_stars                 : 100\n",
      "languages                 : 10 items\n",
      "search_keyword_groups     : 8 items\n",
      "commits_per_repo          : 500\n",
      "date_from                 : 2023-01-01\n",
      "date_to                   : 2025-01-01\n",
      "enable_nlp                : True\n",
      "enable_ml_features        : True\n",
      "enable_enhanced_stats     : True\n",
      "enable_temporal           : True\n",
      "temporal_granularity      : quarter\n",
      "pattern_source            : GSF Catalog\n",
      "pattern_count             : 122\n",
      "category_count            : 15\n",
      "============================================================\n",
      "\n",
      "Total Languages: 10\n",
      "Total Search Keyword Groups: 8\n",
      "\n",
      "-- ESTIMATED SCALE:\n",
      "   Max commits: 50,000\n",
      "   Est. runtime: ~200-300 minutes\n",
      "\n",
      "Note: Tip: For faster testing, reduce max_repos to 10-20 first\n"
     ]
    }
   ],
   "source": [
    "# Experiment Configuration\n",
    "CONFIG = {\n",
    "    # Repository Selection\n",
    "    \"max_repos\": 100,           # Reduced for practical runtime (~1-2 hours total)\n",
    "    \"min_stars\": 100,          # Higher threshold = more active/maintained repos\n",
    "    \"languages\": [ \n",
    "        \"Python\",\n",
    "        \"Java\", \n",
    "        \"Go\",\n",
    "        \"JavaScript\",\n",
    "        \"TypeScript\", \n",
    "        \"C#\",\n",
    "        \"Rust\",\n",
    "        \"Kotlin\",\n",
    "        \"Ruby\",\n",
    "        \"C++\",\n",
    "    ],\n",
    "    # Note: Keywords are now defined as OR-groups in the fetch cell\n",
    "    # to work properly with GitHub's search API\n",
    "    \"search_keyword_groups\": [\n",
    "        \"microservices OR kubernetes OR docker\",\n",
    "        \"cloud-native OR serverless OR containerization\",\n",
    "        \"energy-efficient OR green-software OR carbon-aware\",\n",
    "        \"sustainable OR eco-friendly\",\n",
    "        \"performance-optimization OR resource-optimization\",\n",
    "        \"memory-efficient OR cpu-efficient\",\n",
    "        \"event-driven OR distributed-systems\",\n",
    "        \"scalable OR high-performance\",\n",
    "    ],\n",
    "    \n",
    "    # Commit Extraction - PRACTICAL VALUES\n",
    "    # 100 commits × 50 repos = 5,000 commits (manageable in ~1-2 hours)\n",
    "    \"commits_per_repo\": 500,\n",
    "    \"date_from\": \"2023-01-01\",\n",
    "    \"date_to\": \"2025-01-01\",\n",
    "    \n",
    "    # Analysis Features\n",
    "    \"enable_nlp\": True,\n",
    "    \"enable_ml_features\": True,\n",
    "    \"enable_enhanced_stats\": True,\n",
    "    \"enable_temporal\": True,\n",
    "    \"temporal_granularity\": \"quarter\",\n",
    "    \n",
    "    # Pattern Database\n",
    "    \"pattern_source\": \"GSF Catalog\",\n",
    "    \"pattern_count\": 122,\n",
    "    \"category_count\": 15\n",
    "}\n",
    "\n",
    "# Calculate estimated runtime\n",
    "estimated_commits = CONFIG[\"max_repos\"] * CONFIG[\"commits_per_repo\"]\n",
    "estimated_time_mins = (CONFIG[\"max_repos\"] * 1.5) + (estimated_commits * 0.001)  # ~1.5 min/repo clone + analysis\n",
    "\n",
    "# Display configuration as table\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"{key:25} : {len(value)} items\")\n",
    "    else:\n",
    "        print(f\"{key:25} : {value}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal Languages: {len(CONFIG['languages'])}\")\n",
    "print(f\"Total Search Keyword Groups: {len(CONFIG['search_keyword_groups'])}\")\n",
    "print(f\"\\n-- ESTIMATED SCALE:\")\n",
    "print(f\"   Max commits: {estimated_commits:,}\")\n",
    "print(f\"   Est. runtime: ~{estimated_time_mins:.0f}-{estimated_time_mins*1.5:.0f} minutes\")\n",
    "print(\"\\nNote: Tip: For faster testing, reduce max_repos to 10-20 first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11bd4902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] GitHub token configured\n"
     ]
    }
   ],
   "source": [
    "# Set GitHub Token (required for API access)\n",
    "# Option 1: Set environment variable\n",
    "# os.environ['GITHUB_TOKEN'] = 'your_github_token_here'\n",
    "\n",
    "# Option 2: Load from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('./.env')\n",
    "\n",
    "# Verify token is set\n",
    "if os.getenv('GITHUB_TOKEN'):\n",
    "    print(\"[OK] GitHub token configured\")\n",
    "else:\n",
    "    print(\"WARNING: GitHub token not set. Set GITHUB_TOKEN environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2323c3",
   "metadata": {},
   "source": [
    "## 3. Pipeline Execution\n",
    "\n",
    "Execute the GreenMining pipeline in 5 stages:\n",
    "1. **FETCH** - Discover repositories from GitHub\n",
    "2. **EXTRACT** - Extract commit messages using PyDriller\n",
    "3. **ANALYZE** - Detect green patterns in commits\n",
    "4. **AGGREGATE** - Compute statistics and metrics\n",
    "5. **REPORT** - Generate analysis report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad1fcc",
   "metadata": {},
   "source": [
    "### 3.1 Stage 1: Fetch Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "545f79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:14: DeprecationWarning: Argument login_or_token is deprecated, please use auth=github.Auth.Token(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MULTI-KEYWORD REPOSITORY SEARCH\n",
      "============================================================\n",
      "Keyword groups: 8\n",
      "Target repos: 100\n",
      "Min stars: 100\n",
      "Languages: ['Python', 'Java', 'Go', 'JavaScript', 'TypeScript', 'C#', 'Rust', 'Kotlin', 'Ruby', 'C++']\n",
      "============================================================\n",
      "\n",
      "[1/8] Searching: microservices OR kubernetes OR docker\n",
      "    Found: 1000 repositories\n",
      "    Added: 17 new unique repos (total: 17)\n",
      "\n",
      "[2/8] Searching: cloud-native OR serverless OR containerization\n",
      "    Found: 1000 repositories\n",
      "    Added: 17 new unique repos (total: 34)\n",
      "\n",
      "[3/8] Searching: energy-efficient OR green-software OR carbon-aware\n",
      "    Found: 32 repositories\n",
      "    Added: 17 new unique repos (total: 51)\n",
      "\n",
      "[4/8] Searching: sustainable OR eco-friendly\n",
      "    Found: 62 repositories\n",
      "    Added: 17 new unique repos (total: 68)\n",
      "\n",
      "[5/8] Searching: performance-optimization OR resource-optimization\n",
      "    Found: 217 repositories\n",
      "    Added: 17 new unique repos (total: 85)\n",
      "\n",
      "[6/8] Searching: memory-efficient OR cpu-efficient\n",
      "    Found: 172 repositories\n",
      "    Added: 17 new unique repos (total: 102)\n",
      "\n",
      "[OK] Reached target of 100 repositories\n",
      "\n",
      "============================================================\n",
      "[OK] TOTAL: 100 unique repositories fetched\n",
      "[OK] Saved to: data/repositories.json\n",
      "\n",
      "Language Distribution:\n",
      "  Python: 29\n",
      "  Go: 21\n",
      "  JavaScript: 12\n",
      "  TypeScript: 10\n",
      "  Java: 8\n",
      "  C++: 8\n",
      "  Rust: 7\n",
      "  Ruby: 3\n",
      "  C#: 2\n",
      "============================================================\n",
      "CPU times: user 92.1 ms, sys: 15.7 ms, total: 108 ms\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Stage 1: Fetch repositories using MULTI-KEYWORD search strategy\n",
    "# \n",
    "# GitHub's search API doesn't handle comma-separated keywords well.\n",
    "# Solution: Run multiple searches (one per keyword group) and merge unique results.\n",
    "# This ensures comprehensive coverage for software engineering mining research.\n",
    "\n",
    "from greenmining.config import Config\n",
    "from greenmining.controllers.repository_controller import RepositoryController\n",
    "from github import Github\n",
    "import time\n",
    "\n",
    "# Initialize\n",
    "gm_config = Config()\n",
    "github_client = Github(os.getenv('GITHUB_TOKEN'))\n",
    "\n",
    "# Define keyword groups for comprehensive SE mining\n",
    "# Group by related topics to avoid duplicates within groups\n",
    "KEYWORD_GROUPS = [\n",
    "    # Cloud & Infrastructure (most common in SE)\n",
    "    \"microservices OR kubernetes OR docker\",\n",
    "    \"cloud-native OR serverless OR containerization\",\n",
    "    # Green Software & Sustainability  \n",
    "    \"energy-efficient OR green-software OR carbon-aware\",\n",
    "    \"sustainable OR eco-friendly\",\n",
    "    # Performance & Optimization\n",
    "    \"performance-optimization OR resource-optimization\",\n",
    "    \"memory-efficient OR cpu-efficient\",\n",
    "    # Architecture Patterns\n",
    "    \"event-driven OR distributed-systems\",\n",
    "    \"scalable OR high-performance\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTI-KEYWORD REPOSITORY SEARCH\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Keyword groups: {len(KEYWORD_GROUPS)}\")\n",
    "print(f\"Target repos: {CONFIG['max_repos']}\")\n",
    "print(f\"Min stars: {CONFIG['min_stars']}\")\n",
    "print(f\"Languages: {CONFIG['languages']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Collect unique repositories across all keyword searches\n",
    "all_repos = {}  # Use dict to deduplicate by full_name\n",
    "repos_per_keyword = CONFIG['max_repos'] // len(KEYWORD_GROUPS) + 5  # Extra buffer\n",
    "\n",
    "for i, keyword_query in enumerate(KEYWORD_GROUPS, 1):\n",
    "    print(f\"\\n[{i}/{len(KEYWORD_GROUPS)}] Searching: {keyword_query}\")\n",
    "    \n",
    "    # Build GitHub search query\n",
    "    query_parts = [\n",
    "        keyword_query,\n",
    "        f\"stars:>={CONFIG['min_stars']}\"\n",
    "    ]\n",
    "    query = \" \".join(query_parts)\n",
    "    \n",
    "    try:\n",
    "        results = github_client.search_repositories(query=query, sort=\"stars\", order=\"desc\")\n",
    "        found = results.totalCount\n",
    "        print(f\"    Found: {found} repositories\")\n",
    "        \n",
    "        # Collect repos from this search\n",
    "        added = 0\n",
    "        for repo in results:\n",
    "            if added >= repos_per_keyword:\n",
    "                break\n",
    "            \n",
    "            # Check language filter\n",
    "            if repo.language and repo.language in CONFIG['languages']:\n",
    "                full_name = repo.full_name\n",
    "                if full_name not in all_repos:\n",
    "                    all_repos[full_name] = {\n",
    "                        'id': repo.id,\n",
    "                        'name': repo.name,\n",
    "                        'owner': repo.owner.login,\n",
    "                        'full_name': full_name,\n",
    "                        'description': repo.description,\n",
    "                        'url': repo.html_url,\n",
    "                        'clone_url': repo.clone_url,\n",
    "                        'language': repo.language,\n",
    "                        'stars': repo.stargazers_count,\n",
    "                        'forks': repo.forks_count,\n",
    "                        'created_at': str(repo.created_at),\n",
    "                        'updated_at': str(repo.updated_at),\n",
    "                        'search_keyword': keyword_query.split(' OR ')[0]  # Primary keyword\n",
    "                    }\n",
    "                    added += 1\n",
    "        \n",
    "        print(f\"    Added: {added} new unique repos (total: {len(all_repos)})\")\n",
    "        \n",
    "        # Rate limiting - GitHub allows 30 search requests/min\n",
    "        time.sleep(2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Stop if we have enough\n",
    "    if len(all_repos) >= CONFIG['max_repos']:\n",
    "        print(f\"\\n[OK] Reached target of {CONFIG['max_repos']} repositories\")\n",
    "        break\n",
    "\n",
    "# Convert to list and limit to max_repos\n",
    "repositories = list(all_repos.values())[:CONFIG['max_repos']]\n",
    "\n",
    "# Save to our data directory\n",
    "with open(DATA_DIR / 'repositories.json', 'w') as f:\n",
    "    json.dump(repositories, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"[OK] TOTAL: {len(repositories)} unique repositories fetched\")\n",
    "print(f\"[OK] Saved to: {DATA_DIR / 'repositories.json'}\")\n",
    "\n",
    "# Show language distribution\n",
    "lang_dist = {}\n",
    "for r in repositories:\n",
    "    lang = r.get('language', 'Unknown')\n",
    "    lang_dist[lang] = lang_dist.get(lang, 0) + 1\n",
    "\n",
    "print(\"\\nLanguage Distribution:\")\n",
    "for lang, count in sorted(lang_dist.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  {lang}: {count}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec811dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 repositories\n",
      "\n",
      "Sample repositories:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "owner",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "stars",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "language",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f2d573cd-e7a4-4f80-9332-7e8f362a6504",
       "rows": [
        [
         "0",
         "kubernetes",
         "kubernetes",
         "119680",
         "Go",
         "https://github.com/kubernetes/kubernetes"
        ],
        [
         "1",
         "gin",
         "gin-gonic",
         "87626",
         "Go",
         "https://github.com/gin-gonic/gin"
        ],
        [
         "2",
         "mall",
         "macrozheng",
         "82635",
         "Java",
         "https://github.com/macrozheng/mall"
        ],
        [
         "3",
         "uptime-kuma",
         "louislam",
         "80931",
         "JavaScript",
         "https://github.com/louislam/uptime-kuma"
        ],
        [
         "4",
         "devops-exercises",
         "bregman-arie",
         "80525",
         "Python",
         "https://github.com/bregman-arie/devops-exercises"
        ],
        [
         "5",
         "nest",
         "nestjs",
         "74164",
         "TypeScript",
         "https://github.com/nestjs/nest"
        ],
        [
         "6",
         "Stirling-PDF",
         "Stirling-Tools",
         "72811",
         "TypeScript",
         "https://github.com/Stirling-Tools/Stirling-PDF"
        ],
        [
         "7",
         "moby",
         "moby",
         "71318",
         "Go",
         "https://github.com/moby/moby"
        ],
        [
         "8",
         "traefik",
         "traefik",
         "60923",
         "Go",
         "https://github.com/traefik/traefik"
        ],
        [
         "9",
         "minio",
         "minio",
         "59622",
         "Go",
         "https://github.com/minio/minio"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>owner</th>\n",
       "      <th>stars</th>\n",
       "      <th>language</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kubernetes</td>\n",
       "      <td>kubernetes</td>\n",
       "      <td>119680</td>\n",
       "      <td>Go</td>\n",
       "      <td>https://github.com/kubernetes/kubernetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gin</td>\n",
       "      <td>gin-gonic</td>\n",
       "      <td>87626</td>\n",
       "      <td>Go</td>\n",
       "      <td>https://github.com/gin-gonic/gin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mall</td>\n",
       "      <td>macrozheng</td>\n",
       "      <td>82635</td>\n",
       "      <td>Java</td>\n",
       "      <td>https://github.com/macrozheng/mall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uptime-kuma</td>\n",
       "      <td>louislam</td>\n",
       "      <td>80931</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>https://github.com/louislam/uptime-kuma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>devops-exercises</td>\n",
       "      <td>bregman-arie</td>\n",
       "      <td>80525</td>\n",
       "      <td>Python</td>\n",
       "      <td>https://github.com/bregman-arie/devops-exercises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nest</td>\n",
       "      <td>nestjs</td>\n",
       "      <td>74164</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>https://github.com/nestjs/nest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stirling-PDF</td>\n",
       "      <td>Stirling-Tools</td>\n",
       "      <td>72811</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>https://github.com/Stirling-Tools/Stirling-PDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>moby</td>\n",
       "      <td>moby</td>\n",
       "      <td>71318</td>\n",
       "      <td>Go</td>\n",
       "      <td>https://github.com/moby/moby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>traefik</td>\n",
       "      <td>traefik</td>\n",
       "      <td>60923</td>\n",
       "      <td>Go</td>\n",
       "      <td>https://github.com/traefik/traefik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>minio</td>\n",
       "      <td>minio</td>\n",
       "      <td>59622</td>\n",
       "      <td>Go</td>\n",
       "      <td>https://github.com/minio/minio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name           owner   stars    language  \\\n",
       "0        kubernetes      kubernetes  119680          Go   \n",
       "1               gin       gin-gonic   87626          Go   \n",
       "2              mall      macrozheng   82635        Java   \n",
       "3       uptime-kuma        louislam   80931  JavaScript   \n",
       "4  devops-exercises    bregman-arie   80525      Python   \n",
       "5              nest          nestjs   74164  TypeScript   \n",
       "6      Stirling-PDF  Stirling-Tools   72811  TypeScript   \n",
       "7              moby            moby   71318          Go   \n",
       "8           traefik         traefik   60923          Go   \n",
       "9             minio           minio   59622          Go   \n",
       "\n",
       "                                                url  \n",
       "0          https://github.com/kubernetes/kubernetes  \n",
       "1                  https://github.com/gin-gonic/gin  \n",
       "2                https://github.com/macrozheng/mall  \n",
       "3           https://github.com/louislam/uptime-kuma  \n",
       "4  https://github.com/bregman-arie/devops-exercises  \n",
       "5                    https://github.com/nestjs/nest  \n",
       "6    https://github.com/Stirling-Tools/Stirling-PDF  \n",
       "7                      https://github.com/moby/moby  \n",
       "8                https://github.com/traefik/traefik  \n",
       "9                    https://github.com/minio/minio  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display fetched repositories\n",
    "with open(DATA_DIR / 'repositories.json', 'r') as f:\n",
    "    repositories = json.load(f)\n",
    "\n",
    "print(f\"Fetched {len(repositories)} repositories\")\n",
    "print(\"\\nSample repositories:\")\n",
    "repos_df = pd.DataFrame(repositories[:10])[['name', 'owner', 'stars', 'language', 'url']]\n",
    "display(repos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7a8f9",
   "metadata": {},
   "source": [
    "### 3.2 Stage 2: Extract Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15559fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting commits from 100 repositories...\n",
      "Settings: max_commits=500, skip_merges=True, days_back=1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing repositories:  11%|█         | 11/100 [1:05:32<9:00:24, 364.32s/repo, commits=5500, failed=0]Request GET /repos/wagoodman/dive/commits/50d776e84592b01d65732c9f17db4d9f30a115e7 failed with 403: Forbidden\n",
      "Setting next backoff to 206.710302s\n",
      "Request GET /repos/wagoodman/dive/commits/d2c661eaf7e321aa992396552f54dc8c3db358a8 failed with 403: Forbidden\n",
      "Setting next backoff to 200.536644s\n",
      "Processing repositories:  21%|██        | 21/100 [2:07:13<7:57:21, 362.55s/repo, commits=10402, failed=0]Request GET /repos/ClickHouse/ClickHouse/commits/f81ccc56a808a0245695977a4718bf6ba32db436 failed with 403: Forbidden\n",
      "Setting next backoff to 190.499281s\n",
      "Request GET /repos/ClickHouse/ClickHouse/commits/54392bc2a8583600210e7a6d6033b792435f5138 failed with 403: Forbidden\n",
      "Setting next backoff to 100.755894s\n",
      "Processing repositories:  30%|███       | 30/100 [3:06:29<7:26:04, 382.36s/repo, commits=14902, failed=0]Request GET /repos/envoyproxy/envoy/commits/7644b22d2dfc522105e0017de465ca8ddfa6bff2 failed with 403: Forbidden\n",
      "Setting next backoff to 31.500337s\n",
      "Processing repositories:  43%|████▎     | 43/100 [4:05:34<3:40:14, 231.84s/repo, commits=19527, failed=0]Request GET /repos/genai-impact/ecologits/commits/5b8a32b30df23a98de820c9750e81b8a99ba25be failed with 403: Forbidden\n",
      "Setting next backoff to 11.219332s\n",
      "Processing repositories:  55%|█████▌    | 55/100 [4:56:29<3:57:01, 316.03s/repo, commits=23754, failed=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting commits from chaoss/augur: Repository extraction timeout\n",
      "Attempt 1/3 failed: Repository extraction timeout\n",
      "Retrying in 5.0 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing repositories:  56%|█████▌    | 56/100 [5:04:33<4:28:43, 366.43s/repo, commits=24254, failed=0]Request GET /repos/pogopaule/awesome-sustainability-jobs/commits/88c6431cf634a81cbdc21648ebf21b583f1fd8ba failed with 403: Forbidden\n",
      "Setting next backoff to 173.179069s\n",
      "Processing repositories:  71%|███████   | 71/100 [6:08:40<2:14:22, 278.03s/repo, commits=29195, failed=0]Request GET /repos/fastruby/fast-ruby/commits/8d9e752f5cdfa644d94da5c3b66b5ddbe60e98cf failed with 403: Forbidden\n",
      "Setting next backoff to 97.911763s\n",
      "Request GET /repos/fastruby/fast-ruby/commits/d6e1ac0ef58931479e5c89478992a78c0271acd9 failed with 403: Forbidden\n",
      "Setting next backoff to 17.900002s\n",
      "Processing repositories:  85%|████████▌ | 85/100 [7:06:42<57:15, 229.03s/repo, commits=34005, failed=0]  Request GET /repos/vllm-project/vllm/commits/1d9e9ae8a4498782de0dd51627ab1fddac4692ef failed with 403: Forbidden\n",
      "Setting next backoff to 251.509298s\n",
      "Request GET /repos/vllm-project/vllm/commits/b7036c87a13bd94fabf9e46436d3c1e67688f729 failed with 403: Forbidden\n",
      "Setting next backoff to 136.778069s\n",
      "Processing repositories:  96%|█████████▌| 96/100 [8:03:48<17:34, 263.57s/repo, commits=38614, failed=0]  Request GET /repos/opensolon/solon/commits/79fe4b468ec39534f91bc1d10a0e5d2d9c2f2ccc failed with 403: Forbidden\n",
      "Setting next backoff to 298.781847s\n",
      "Processing repositories: 100%|██████████| 100/100 [8:21:10<00:00, 300.70s/repo, commits=39664, failed=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Extracted 39664 commits from 100 repositories\n",
      "[OK] Saved to: data/commits.json\n",
      "CPU times: user 1min 16s, sys: 3.56 s, total: 1min 19s\n",
      "Wall time: 8h 21min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Stage 2: Extract commits using Python API\n",
    "from greenmining.services.commit_extractor import CommitExtractor\n",
    "\n",
    "# Calculate days back from date range\n",
    "date_from = datetime.strptime(CONFIG[\"date_from\"], \"%Y-%m-%d\")\n",
    "date_to = datetime.strptime(CONFIG[\"date_to\"], \"%Y-%m-%d\")\n",
    "days_back = (datetime.now() - date_from).days\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = CommitExtractor(\n",
    "    max_commits=CONFIG[\"commits_per_repo\"],\n",
    "    skip_merges=True,\n",
    "    days_back=days_back,\n",
    "    github_token=os.getenv('GITHUB_TOKEN'),\n",
    "    timeout=120  # 2 minutes per repo\n",
    ")\n",
    "\n",
    "# Extract commits from all repositories\n",
    "commits = extractor.extract_from_repositories(repositories)\n",
    "\n",
    "# Save commits\n",
    "with open(DATA_DIR / 'commits.json', 'w') as f:\n",
    "    json.dump(commits, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n[OK] Extracted {len(commits)} commits from {len(repositories)} repositories\")\n",
    "print(f\"[OK] Saved to: {DATA_DIR / 'commits.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d0faff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 39664 commits\n",
      "\n",
      "Sample commit messages:\n",
      "  1. Move dummy testing to subpackage\n",
      "\n",
      "Change-Id: I52863cf256fc52b863c182932eb9520f36...\n",
      "  2. run codegen...\n",
      "  3. feat: skip validation for types that are Lists in GetTargets function...\n",
      "  4. Enable nomaps rule for Kube API Linter (#134852)\n",
      "\n",
      "* tested how many errors\n",
      "\n",
      "* ad...\n",
      "  5. leasecandidate: Improve goroutine management\n",
      "\n",
      "Make sure all goroutines are termi...\n"
     ]
    }
   ],
   "source": [
    "# Load and summarize extracted commits\n",
    "with open(DATA_DIR / 'commits.json', 'r') as f:\n",
    "    commits = json.load(f)\n",
    "\n",
    "print(f\"Extracted {len(commits)} commits\")\n",
    "print(\"\\nSample commit messages:\")\n",
    "for i, commit in enumerate(commits[:5]):\n",
    "    msg = commit.get('message', '')[:80]\n",
    "    print(f\"  {i+1}. {msg}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70e084",
   "metadata": {},
   "source": [
    "### 3.3 Stage 3: Analyze Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e0f93cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP analysis enabled (morphological variants + synonyms)\n",
      "ML feature extraction enabled\n",
      "\n",
      "Analyzing 39664 commits for green practices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing commits: 100%|██████████| 39664/39664 [00:28<00:00, 1368.18commit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Analyzed 39664 commits\n",
      "[OK] Green-aware commits: 0 (0.00%)\n",
      "[OK] Saved to: data/analysis_results.json\n",
      "CPU times: user 30.3 s, sys: 126 ms, total: 30.4 s\n",
      "Wall time: 30.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Stage 3: Analyze commits for green patterns using Python API\n",
    "from greenmining.services.data_analyzer import DataAnalyzer\n",
    "\n",
    "# Initialize analyzer with configured options\n",
    "analyzer = DataAnalyzer(\n",
    "    enable_nlp=CONFIG[\"enable_nlp\"],\n",
    "    enable_ml_features=CONFIG[\"enable_ml_features\"]\n",
    ")\n",
    "\n",
    "# Analyze all commits\n",
    "analysis_results = analyzer.analyze_commits(commits)\n",
    "\n",
    "# Save results\n",
    "with open(DATA_DIR / 'analysis_results.json', 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2, default=str)\n",
    "\n",
    "# Summary\n",
    "green_commits = [r for r in analysis_results if r.get('is_green_aware', False)]\n",
    "print(f\"\\n[OK] Analyzed {len(analysis_results)} commits\")\n",
    "print(f\"[OK] Green-aware commits: {len(green_commits)} ({len(green_commits)/len(analysis_results)*100:.2f}%)\")\n",
    "print(f\"[OK] Saved to: {DATA_DIR / 'analysis_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a720538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total commits analyzed: 39664\n",
      "Green-aware commits: 0\n",
      "Green awareness rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Load analysis results\n",
    "with open(DATA_DIR / 'analysis_results.json', 'r') as f:\n",
    "    analysis_results = json.load(f)\n",
    "\n",
    "# Count green-aware commits\n",
    "green_commits = [r for r in analysis_results if r.get('is_green_aware', False)]\n",
    "print(f\"Total commits analyzed: {len(analysis_results)}\")\n",
    "print(f\"Green-aware commits: {len(green_commits)}\")\n",
    "print(f\"Green awareness rate: {len(green_commits)/len(analysis_results)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62cdc4",
   "metadata": {},
   "source": [
    "### 3.4 Stage 4: Aggregate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6329ba00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced statistical analysis enabled\n",
      "Temporal analysis enabled (granularity: quarter)\n",
      "\n",
      "Aggregating analysis results...\n",
      "[DONE] Enhanced statistical analysis complete\n",
      "[DONE] Temporal trend analysis complete\n",
      "\n",
      "[OK] Aggregation complete\n",
      "[OK] Saved to: data/aggregated_statistics.json\n",
      "\n",
      "Available data sections:\n",
      "  - summary\n",
      "  - known_patterns\n",
      "  - emergent_patterns\n",
      "  - per_repo_stats\n",
      "  - per_language_stats\n",
      "  - enhanced_statistics\n",
      "  - temporal_analysis\n",
      "CPU times: user 769 ms, sys: 3.82 ms, total: 773 ms\n",
      "Wall time: 773 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Stage 4: Aggregate statistics using Python API\n",
    "from greenmining.services.data_aggregator import DataAggregator\n",
    "\n",
    "# Initialize aggregator with configured options\n",
    "aggregator = DataAggregator(\n",
    "    enable_enhanced_stats=CONFIG[\"enable_enhanced_stats\"],\n",
    "    enable_temporal=CONFIG[\"enable_temporal\"],\n",
    "    temporal_granularity=CONFIG[\"temporal_granularity\"]\n",
    ")\n",
    "\n",
    "# Aggregate results\n",
    "stats = aggregator.aggregate(analysis_results, repositories)\n",
    "\n",
    "# Save aggregated statistics\n",
    "with open(DATA_DIR / 'aggregated_statistics.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n[OK] Aggregation complete\")\n",
    "print(f\"[OK] Saved to: {DATA_DIR / 'aggregated_statistics.json'}\")\n",
    "print(\"\\nAvailable data sections:\")\n",
    "for key in stats.keys():\n",
    "    print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27afc552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation complete. Available data sections:\n",
      "  - summary\n",
      "  - known_patterns\n",
      "  - emergent_patterns\n",
      "  - per_repo_stats\n",
      "  - per_language_stats\n",
      "  - enhanced_statistics\n",
      "  - temporal_analysis\n"
     ]
    }
   ],
   "source": [
    "# Load aggregated statistics\n",
    "with open(DATA_DIR / 'aggregated_statistics.json', 'r') as f:\n",
    "    stats = json.load(f)\n",
    "\n",
    "print(\"Aggregation complete. Available data sections:\")\n",
    "for key in stats.keys():\n",
    "    print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5721050",
   "metadata": {},
   "source": [
    "### 3.5 Stage 5: Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69ed4471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Report generated: data/green_software_analysis_report.md\n",
      "\n",
      "Report preview (first 500 chars):\n",
      "\n",
      "# Mining Software Repositories for Green Microservices\n",
      "## Comprehensive Analysis Report\n",
      "\n",
      "**Report Generated:** 2026-01-07 21:29:05\n",
      "**Analysis Type:** Keyword and Heuristic-Based Pattern Detection\n",
      "\n",
      "---\n",
      "\n",
      "### Executive Summary\n",
      "\n",
      "This report presents findings from analyzing **39,664 commits** across **100 microservice-based repositories** to identify green software engineering practices.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "- **46.0%** of commits (18,253) explicitly mention energy efficiency, performance optimization...\n"
     ]
    }
   ],
   "source": [
    "# Stage 5: Generate report using Python API\n",
    "from greenmining.services.reports import ReportGenerator\n",
    "\n",
    "# Initialize report generator\n",
    "report_gen = ReportGenerator()\n",
    "\n",
    "# Wrap data in expected format with metadata\n",
    "# The ReportGenerator expects dict with 'metadata' keys\n",
    "repos_data_wrapped = {\n",
    "    \"metadata\": {\n",
    "        \"languages\": CONFIG[\"languages\"],\n",
    "        \"search_keywords\": [kw.split(\" OR \")[0] for kw in CONFIG[\"search_keyword_groups\"]],\n",
    "        \"min_stars\": CONFIG[\"min_stars\"],\n",
    "        \"total_repos\": len(repositories),\n",
    "    },\n",
    "    \"repositories\": repositories\n",
    "}\n",
    "\n",
    "analysis_data_wrapped = {\n",
    "    \"metadata\": {\n",
    "        \"total_commits\": len(analysis_results),\n",
    "        \"date_from\": CONFIG[\"date_from\"],\n",
    "        \"date_to\": CONFIG[\"date_to\"],\n",
    "    },\n",
    "    \"results\": analysis_results\n",
    "}\n",
    "\n",
    "# Generate markdown report\n",
    "report_content = report_gen.generate_report(\n",
    "    aggregated_data=stats,\n",
    "    analysis_data=analysis_data_wrapped,\n",
    "    repos_data=repos_data_wrapped\n",
    ")\n",
    "\n",
    "# Save report\n",
    "report_path = DATA_DIR / 'green_software_analysis_report.md'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"[OK] Report generated: {report_path}\")\n",
    "print(f\"\\nReport preview (first 500 chars):\\n\")\n",
    "print(report_content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0202a6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SUMMARY STATISTICS\n",
      "============================================================\n",
      "Total Commits Analyzed:      39,664\n",
      "Green-Aware Commits:         18,253\n",
      "Green Awareness Rate:        46.02%\n",
      "Total Repositories:          100\n",
      "Repos with Green Commits:    98\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Display Summary Statistics\n",
    "summary = stats.get('summary', {})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Commits Analyzed:      {summary.get('total_commits', 'N/A'):,}\")\n",
    "print(f\"Green-Aware Commits:         {summary.get('green_aware_count', 'N/A'):,}\")\n",
    "print(f\"Green Awareness Rate:        {summary.get('green_aware_percentage', 'N/A'):.2f}%\")\n",
    "print(f\"Total Repositories:          {summary.get('total_repos', 'N/A')}\")\n",
    "print(f\"Repos with Green Commits:    {summary.get('repos_with_green_commits', 'N/A')}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
